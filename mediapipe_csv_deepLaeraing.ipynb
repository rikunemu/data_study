{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mediapipe_csv_deepLaeraing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOItDHA5PPAUtEelESwKy/V"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQzvztFP513g",
        "outputId": "9c0b2825-809e-4190-f7a1-c0a361711ef6"
      },
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import Series,DataFrame\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import os\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import utils as np_utils\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYz-88dFlO4e"
      },
      "source": [
        "fp='/content/drive/MyDrive/data分析/Mediapipe/csvtraintotal.csv'\n",
        "#files=os.listdir(fp)\n",
        "df=pd.read_csv(fp)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "OnuDyefiX3F1",
        "outputId": "ea547191-eb57-4de1-d25d-bfbdaa91bf9c"
      },
      "source": [
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.470799</td>\n",
              "      <td>0.682127</td>\n",
              "      <td>-0.080930</td>\n",
              "      <td>0.339680</td>\n",
              "      <td>0.500239</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.296106</td>\n",
              "      <td>0.526307</td>\n",
              "      <td>0.013302</td>\n",
              "      <td>0.361964</td>\n",
              "      <td>0.573378</td>\n",
              "      <td>-0.044539</td>\n",
              "      <td>0.191371</td>\n",
              "      <td>0.194533</td>\n",
              "      <td>0.097439</td>\n",
              "      <td>0.212881</td>\n",
              "      <td>0.238277</td>\n",
              "      <td>0.052467</td>\n",
              "      <td>0.235472</td>\n",
              "      <td>0.285108</td>\n",
              "      <td>0.014390</td>\n",
              "      <td>0.385465</td>\n",
              "      <td>0.781734</td>\n",
              "      <td>-0.007592</td>\n",
              "      <td>0.357043</td>\n",
              "      <td>0.274428</td>\n",
              "      <td>-0.037246</td>\n",
              "      <td>0.340158</td>\n",
              "      <td>0.202853</td>\n",
              "      <td>-0.015436</td>\n",
              "      <td>0.321931</td>\n",
              "      <td>0.130519</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>0.412151</td>\n",
              "      <td>0.113719</td>\n",
              "      <td>-0.008327</td>\n",
              "      <td>0.253681</td>\n",
              "      <td>0.442930</td>\n",
              "      <td>0.056188</td>\n",
              "      <td>0.192371</td>\n",
              "      <td>...</td>\n",
              "      <td>0.737290</td>\n",
              "      <td>-0.025478</td>\n",
              "      <td>0.434136</td>\n",
              "      <td>0.314108</td>\n",
              "      <td>-0.042840</td>\n",
              "      <td>0.398939</td>\n",
              "      <td>0.746655</td>\n",
              "      <td>-0.030840</td>\n",
              "      <td>0.393578</td>\n",
              "      <td>0.757806</td>\n",
              "      <td>-0.026364</td>\n",
              "      <td>0.351389</td>\n",
              "      <td>0.677704</td>\n",
              "      <td>-0.021752</td>\n",
              "      <td>0.173446</td>\n",
              "      <td>0.579080</td>\n",
              "      <td>0.323007</td>\n",
              "      <td>0.459225</td>\n",
              "      <td>0.599125</td>\n",
              "      <td>-0.099350</td>\n",
              "      <td>0.399182</td>\n",
              "      <td>0.731124</td>\n",
              "      <td>-0.006322</td>\n",
              "      <td>0.391899</td>\n",
              "      <td>0.733549</td>\n",
              "      <td>-0.009079</td>\n",
              "      <td>0.424361</td>\n",
              "      <td>0.611631</td>\n",
              "      <td>-0.070455</td>\n",
              "      <td>0.380985</td>\n",
              "      <td>0.606789</td>\n",
              "      <td>-0.037339</td>\n",
              "      <td>0.419343</td>\n",
              "      <td>0.605716</td>\n",
              "      <td>-0.069694</td>\n",
              "      <td>0.428503</td>\n",
              "      <td>0.269363</td>\n",
              "      <td>-0.045183</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.469567</td>\n",
              "      <td>0.799870</td>\n",
              "      <td>-0.017644</td>\n",
              "      <td>0.348688</td>\n",
              "      <td>0.513593</td>\n",
              "      <td>-0.045884</td>\n",
              "      <td>0.290292</td>\n",
              "      <td>0.530457</td>\n",
              "      <td>-0.030681</td>\n",
              "      <td>0.352485</td>\n",
              "      <td>0.633756</td>\n",
              "      <td>-0.053042</td>\n",
              "      <td>0.209036</td>\n",
              "      <td>0.094351</td>\n",
              "      <td>-0.139540</td>\n",
              "      <td>0.239477</td>\n",
              "      <td>0.182606</td>\n",
              "      <td>-0.157115</td>\n",
              "      <td>0.265887</td>\n",
              "      <td>0.273181</td>\n",
              "      <td>-0.170042</td>\n",
              "      <td>0.327009</td>\n",
              "      <td>0.827892</td>\n",
              "      <td>0.100199</td>\n",
              "      <td>0.425728</td>\n",
              "      <td>0.307515</td>\n",
              "      <td>-0.205852</td>\n",
              "      <td>0.416682</td>\n",
              "      <td>0.194925</td>\n",
              "      <td>-0.227324</td>\n",
              "      <td>0.406638</td>\n",
              "      <td>0.083548</td>\n",
              "      <td>-0.242068</td>\n",
              "      <td>0.529422</td>\n",
              "      <td>0.090397</td>\n",
              "      <td>-0.250223</td>\n",
              "      <td>0.250477</td>\n",
              "      <td>0.403172</td>\n",
              "      <td>-0.039577</td>\n",
              "      <td>0.157762</td>\n",
              "      <td>...</td>\n",
              "      <td>0.798798</td>\n",
              "      <td>0.062630</td>\n",
              "      <td>0.508284</td>\n",
              "      <td>0.366295</td>\n",
              "      <td>-0.177562</td>\n",
              "      <td>0.354303</td>\n",
              "      <td>0.805612</td>\n",
              "      <td>0.062696</td>\n",
              "      <td>0.345936</td>\n",
              "      <td>0.813267</td>\n",
              "      <td>0.070353</td>\n",
              "      <td>0.316791</td>\n",
              "      <td>0.738285</td>\n",
              "      <td>0.024206</td>\n",
              "      <td>0.072487</td>\n",
              "      <td>0.447892</td>\n",
              "      <td>0.333080</td>\n",
              "      <td>0.477093</td>\n",
              "      <td>0.723211</td>\n",
              "      <td>-0.088660</td>\n",
              "      <td>0.350487</td>\n",
              "      <td>0.789399</td>\n",
              "      <td>0.078221</td>\n",
              "      <td>0.343767</td>\n",
              "      <td>0.790928</td>\n",
              "      <td>0.078075</td>\n",
              "      <td>0.422094</td>\n",
              "      <td>0.713270</td>\n",
              "      <td>-0.049938</td>\n",
              "      <td>0.363159</td>\n",
              "      <td>0.677821</td>\n",
              "      <td>-0.022846</td>\n",
              "      <td>0.415244</td>\n",
              "      <td>0.704734</td>\n",
              "      <td>-0.056130</td>\n",
              "      <td>0.513777</td>\n",
              "      <td>0.319253</td>\n",
              "      <td>-0.206077</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.717369</td>\n",
              "      <td>-0.049849</td>\n",
              "      <td>0.492522</td>\n",
              "      <td>0.549239</td>\n",
              "      <td>-0.050151</td>\n",
              "      <td>0.450243</td>\n",
              "      <td>0.587217</td>\n",
              "      <td>-0.045329</td>\n",
              "      <td>0.539467</td>\n",
              "      <td>0.635224</td>\n",
              "      <td>-0.070256</td>\n",
              "      <td>0.223377</td>\n",
              "      <td>0.272958</td>\n",
              "      <td>-0.083552</td>\n",
              "      <td>0.283708</td>\n",
              "      <td>0.326970</td>\n",
              "      <td>-0.109453</td>\n",
              "      <td>0.340850</td>\n",
              "      <td>0.387407</td>\n",
              "      <td>-0.131149</td>\n",
              "      <td>0.603942</td>\n",
              "      <td>0.857271</td>\n",
              "      <td>0.056888</td>\n",
              "      <td>0.490310</td>\n",
              "      <td>0.351051</td>\n",
              "      <td>-0.153234</td>\n",
              "      <td>0.437256</td>\n",
              "      <td>0.259501</td>\n",
              "      <td>-0.155545</td>\n",
              "      <td>0.385495</td>\n",
              "      <td>0.177831</td>\n",
              "      <td>-0.154630</td>\n",
              "      <td>0.488445</td>\n",
              "      <td>0.137941</td>\n",
              "      <td>-0.152786</td>\n",
              "      <td>0.367887</td>\n",
              "      <td>0.503332</td>\n",
              "      <td>-0.038149</td>\n",
              "      <td>0.294357</td>\n",
              "      <td>...</td>\n",
              "      <td>0.822683</td>\n",
              "      <td>0.029002</td>\n",
              "      <td>0.578697</td>\n",
              "      <td>0.367118</td>\n",
              "      <td>-0.129393</td>\n",
              "      <td>0.622796</td>\n",
              "      <td>0.834589</td>\n",
              "      <td>0.028758</td>\n",
              "      <td>0.616266</td>\n",
              "      <td>0.845286</td>\n",
              "      <td>0.035142</td>\n",
              "      <td>0.550873</td>\n",
              "      <td>0.747057</td>\n",
              "      <td>-0.012852</td>\n",
              "      <td>0.228344</td>\n",
              "      <td>0.659144</td>\n",
              "      <td>0.268810</td>\n",
              "      <td>0.688327</td>\n",
              "      <td>0.643454</td>\n",
              "      <td>-0.101639</td>\n",
              "      <td>0.612709</td>\n",
              "      <td>0.811374</td>\n",
              "      <td>0.041834</td>\n",
              "      <td>0.604680</td>\n",
              "      <td>0.816566</td>\n",
              "      <td>0.040922</td>\n",
              "      <td>0.633793</td>\n",
              "      <td>0.664174</td>\n",
              "      <td>-0.071061</td>\n",
              "      <td>0.566127</td>\n",
              "      <td>0.666339</td>\n",
              "      <td>-0.046193</td>\n",
              "      <td>0.624728</td>\n",
              "      <td>0.659804</td>\n",
              "      <td>-0.075197</td>\n",
              "      <td>0.565757</td>\n",
              "      <td>0.328598</td>\n",
              "      <td>-0.146556</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.483527</td>\n",
              "      <td>0.809326</td>\n",
              "      <td>-0.011760</td>\n",
              "      <td>0.344644</td>\n",
              "      <td>0.581638</td>\n",
              "      <td>-0.033273</td>\n",
              "      <td>0.295061</td>\n",
              "      <td>0.597420</td>\n",
              "      <td>-0.013657</td>\n",
              "      <td>0.360514</td>\n",
              "      <td>0.680113</td>\n",
              "      <td>-0.041067</td>\n",
              "      <td>0.189817</td>\n",
              "      <td>0.220668</td>\n",
              "      <td>-0.127457</td>\n",
              "      <td>0.215817</td>\n",
              "      <td>0.290792</td>\n",
              "      <td>-0.141683</td>\n",
              "      <td>0.238612</td>\n",
              "      <td>0.363065</td>\n",
              "      <td>-0.150872</td>\n",
              "      <td>0.378712</td>\n",
              "      <td>0.844660</td>\n",
              "      <td>0.111848</td>\n",
              "      <td>0.382644</td>\n",
              "      <td>0.402066</td>\n",
              "      <td>-0.192089</td>\n",
              "      <td>0.370748</td>\n",
              "      <td>0.305048</td>\n",
              "      <td>-0.214974</td>\n",
              "      <td>0.355163</td>\n",
              "      <td>0.208554</td>\n",
              "      <td>-0.232295</td>\n",
              "      <td>0.464029</td>\n",
              "      <td>0.211647</td>\n",
              "      <td>-0.244948</td>\n",
              "      <td>0.247434</td>\n",
              "      <td>0.488584</td>\n",
              "      <td>-0.026676</td>\n",
              "      <td>0.175191</td>\n",
              "      <td>...</td>\n",
              "      <td>0.820019</td>\n",
              "      <td>0.071907</td>\n",
              "      <td>0.465131</td>\n",
              "      <td>0.452157</td>\n",
              "      <td>-0.168719</td>\n",
              "      <td>0.398679</td>\n",
              "      <td>0.827414</td>\n",
              "      <td>0.072785</td>\n",
              "      <td>0.392145</td>\n",
              "      <td>0.834791</td>\n",
              "      <td>0.081655</td>\n",
              "      <td>0.346620</td>\n",
              "      <td>0.765851</td>\n",
              "      <td>0.036317</td>\n",
              "      <td>0.139569</td>\n",
              "      <td>0.514248</td>\n",
              "      <td>0.319508</td>\n",
              "      <td>0.475415</td>\n",
              "      <td>0.749521</td>\n",
              "      <td>-0.076082</td>\n",
              "      <td>0.396229</td>\n",
              "      <td>0.811944</td>\n",
              "      <td>0.086949</td>\n",
              "      <td>0.389692</td>\n",
              "      <td>0.813754</td>\n",
              "      <td>0.087736</td>\n",
              "      <td>0.431490</td>\n",
              "      <td>0.742306</td>\n",
              "      <td>-0.039092</td>\n",
              "      <td>0.377746</td>\n",
              "      <td>0.715237</td>\n",
              "      <td>-0.014063</td>\n",
              "      <td>0.424486</td>\n",
              "      <td>0.735751</td>\n",
              "      <td>-0.044687</td>\n",
              "      <td>0.464442</td>\n",
              "      <td>0.411362</td>\n",
              "      <td>-0.195280</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.061688</td>\n",
              "      <td>0.692855</td>\n",
              "      <td>-0.050553</td>\n",
              "      <td>0.088094</td>\n",
              "      <td>0.485313</td>\n",
              "      <td>0.096132</td>\n",
              "      <td>0.066970</td>\n",
              "      <td>0.506532</td>\n",
              "      <td>0.140697</td>\n",
              "      <td>0.043108</td>\n",
              "      <td>0.566985</td>\n",
              "      <td>0.046351</td>\n",
              "      <td>0.085716</td>\n",
              "      <td>0.166476</td>\n",
              "      <td>0.243169</td>\n",
              "      <td>0.064643</td>\n",
              "      <td>0.223619</td>\n",
              "      <td>0.188716</td>\n",
              "      <td>0.040649</td>\n",
              "      <td>0.289942</td>\n",
              "      <td>0.143320</td>\n",
              "      <td>0.082358</td>\n",
              "      <td>0.776436</td>\n",
              "      <td>0.096337</td>\n",
              "      <td>0.084009</td>\n",
              "      <td>0.307915</td>\n",
              "      <td>0.002832</td>\n",
              "      <td>0.094099</td>\n",
              "      <td>0.206533</td>\n",
              "      <td>0.022408</td>\n",
              "      <td>0.109919</td>\n",
              "      <td>0.122637</td>\n",
              "      <td>0.042296</td>\n",
              "      <td>0.163256</td>\n",
              "      <td>0.115645</td>\n",
              "      <td>-0.043075</td>\n",
              "      <td>0.080999</td>\n",
              "      <td>0.401563</td>\n",
              "      <td>0.211497</td>\n",
              "      <td>0.079665</td>\n",
              "      <td>...</td>\n",
              "      <td>0.740983</td>\n",
              "      <td>0.055285</td>\n",
              "      <td>0.131059</td>\n",
              "      <td>0.346348</td>\n",
              "      <td>-0.051665</td>\n",
              "      <td>0.071792</td>\n",
              "      <td>0.746798</td>\n",
              "      <td>0.055567</td>\n",
              "      <td>0.069141</td>\n",
              "      <td>0.755517</td>\n",
              "      <td>0.065676</td>\n",
              "      <td>0.047076</td>\n",
              "      <td>0.671389</td>\n",
              "      <td>0.082513</td>\n",
              "      <td>0.229300</td>\n",
              "      <td>0.530411</td>\n",
              "      <td>0.530587</td>\n",
              "      <td>0.052978</td>\n",
              "      <td>0.619554</td>\n",
              "      <td>-0.066292</td>\n",
              "      <td>0.085567</td>\n",
              "      <td>0.738580</td>\n",
              "      <td>0.082343</td>\n",
              "      <td>0.080121</td>\n",
              "      <td>0.739309</td>\n",
              "      <td>0.081224</td>\n",
              "      <td>0.058816</td>\n",
              "      <td>0.624059</td>\n",
              "      <td>-0.017864</td>\n",
              "      <td>0.052244</td>\n",
              "      <td>0.608127</td>\n",
              "      <td>0.034917</td>\n",
              "      <td>0.055178</td>\n",
              "      <td>0.616471</td>\n",
              "      <td>-0.019056</td>\n",
              "      <td>0.126254</td>\n",
              "      <td>0.306380</td>\n",
              "      <td>-0.058228</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82190</th>\n",
              "      <td>0.436367</td>\n",
              "      <td>0.787778</td>\n",
              "      <td>-0.124726</td>\n",
              "      <td>0.343724</td>\n",
              "      <td>0.529838</td>\n",
              "      <td>0.029624</td>\n",
              "      <td>0.286873</td>\n",
              "      <td>0.550876</td>\n",
              "      <td>0.056680</td>\n",
              "      <td>0.348470</td>\n",
              "      <td>0.626252</td>\n",
              "      <td>-0.042911</td>\n",
              "      <td>0.254712</td>\n",
              "      <td>0.095861</td>\n",
              "      <td>0.198664</td>\n",
              "      <td>0.255640</td>\n",
              "      <td>0.142529</td>\n",
              "      <td>0.130252</td>\n",
              "      <td>0.261927</td>\n",
              "      <td>0.191391</td>\n",
              "      <td>0.074780</td>\n",
              "      <td>0.339070</td>\n",
              "      <td>0.897009</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.408799</td>\n",
              "      <td>0.218067</td>\n",
              "      <td>-0.025457</td>\n",
              "      <td>0.410505</td>\n",
              "      <td>0.142572</td>\n",
              "      <td>0.004624</td>\n",
              "      <td>0.414198</td>\n",
              "      <td>0.054057</td>\n",
              "      <td>0.037996</td>\n",
              "      <td>0.532120</td>\n",
              "      <td>0.060675</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.257418</td>\n",
              "      <td>0.427196</td>\n",
              "      <td>0.127661</td>\n",
              "      <td>0.195598</td>\n",
              "      <td>...</td>\n",
              "      <td>0.861018</td>\n",
              "      <td>-0.032097</td>\n",
              "      <td>0.503545</td>\n",
              "      <td>0.298082</td>\n",
              "      <td>-0.048313</td>\n",
              "      <td>0.353288</td>\n",
              "      <td>0.871084</td>\n",
              "      <td>-0.038138</td>\n",
              "      <td>0.344398</td>\n",
              "      <td>0.881407</td>\n",
              "      <td>-0.030064</td>\n",
              "      <td>0.322134</td>\n",
              "      <td>0.758520</td>\n",
              "      <td>-0.018979</td>\n",
              "      <td>0.228365</td>\n",
              "      <td>0.563227</td>\n",
              "      <td>0.524367</td>\n",
              "      <td>0.437460</td>\n",
              "      <td>0.668506</td>\n",
              "      <td>-0.135968</td>\n",
              "      <td>0.368955</td>\n",
              "      <td>0.853075</td>\n",
              "      <td>-0.006710</td>\n",
              "      <td>0.358095</td>\n",
              "      <td>0.856069</td>\n",
              "      <td>-0.010641</td>\n",
              "      <td>0.405361</td>\n",
              "      <td>0.682727</td>\n",
              "      <td>-0.090688</td>\n",
              "      <td>0.365983</td>\n",
              "      <td>0.674993</td>\n",
              "      <td>-0.039105</td>\n",
              "      <td>0.401111</td>\n",
              "      <td>0.674640</td>\n",
              "      <td>-0.088754</td>\n",
              "      <td>0.507884</td>\n",
              "      <td>0.236569</td>\n",
              "      <td>-0.051585</td>\n",
              "      <td>8108</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82191</th>\n",
              "      <td>0.456891</td>\n",
              "      <td>0.704518</td>\n",
              "      <td>-0.093469</td>\n",
              "      <td>0.350804</td>\n",
              "      <td>0.496862</td>\n",
              "      <td>-0.003796</td>\n",
              "      <td>0.301899</td>\n",
              "      <td>0.518511</td>\n",
              "      <td>0.008652</td>\n",
              "      <td>0.366491</td>\n",
              "      <td>0.575456</td>\n",
              "      <td>-0.050161</td>\n",
              "      <td>0.239688</td>\n",
              "      <td>0.157823</td>\n",
              "      <td>0.090260</td>\n",
              "      <td>0.254965</td>\n",
              "      <td>0.204339</td>\n",
              "      <td>0.041235</td>\n",
              "      <td>0.271476</td>\n",
              "      <td>0.255241</td>\n",
              "      <td>0.002160</td>\n",
              "      <td>0.358471</td>\n",
              "      <td>0.805676</td>\n",
              "      <td>-0.005432</td>\n",
              "      <td>0.407089</td>\n",
              "      <td>0.273282</td>\n",
              "      <td>-0.053026</td>\n",
              "      <td>0.402834</td>\n",
              "      <td>0.196553</td>\n",
              "      <td>-0.033259</td>\n",
              "      <td>0.397798</td>\n",
              "      <td>0.120587</td>\n",
              "      <td>-0.008418</td>\n",
              "      <td>0.499167</td>\n",
              "      <td>0.121729</td>\n",
              "      <td>-0.022568</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>0.416111</td>\n",
              "      <td>0.052676</td>\n",
              "      <td>0.193656</td>\n",
              "      <td>...</td>\n",
              "      <td>0.775459</td>\n",
              "      <td>-0.030029</td>\n",
              "      <td>0.482762</td>\n",
              "      <td>0.327106</td>\n",
              "      <td>-0.056223</td>\n",
              "      <td>0.382838</td>\n",
              "      <td>0.783833</td>\n",
              "      <td>-0.034135</td>\n",
              "      <td>0.374178</td>\n",
              "      <td>0.792643</td>\n",
              "      <td>-0.028977</td>\n",
              "      <td>0.349132</td>\n",
              "      <td>0.689029</td>\n",
              "      <td>-0.033610</td>\n",
              "      <td>0.155815</td>\n",
              "      <td>0.540453</td>\n",
              "      <td>0.354896</td>\n",
              "      <td>0.457088</td>\n",
              "      <td>0.609271</td>\n",
              "      <td>-0.107670</td>\n",
              "      <td>0.391434</td>\n",
              "      <td>0.767631</td>\n",
              "      <td>-0.016601</td>\n",
              "      <td>0.384023</td>\n",
              "      <td>0.770720</td>\n",
              "      <td>-0.019424</td>\n",
              "      <td>0.423652</td>\n",
              "      <td>0.620989</td>\n",
              "      <td>-0.075051</td>\n",
              "      <td>0.383313</td>\n",
              "      <td>0.614421</td>\n",
              "      <td>-0.040141</td>\n",
              "      <td>0.419421</td>\n",
              "      <td>0.613941</td>\n",
              "      <td>-0.075122</td>\n",
              "      <td>0.485513</td>\n",
              "      <td>0.282853</td>\n",
              "      <td>-0.061855</td>\n",
              "      <td>8109</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82192</th>\n",
              "      <td>0.502264</td>\n",
              "      <td>0.729350</td>\n",
              "      <td>-0.124422</td>\n",
              "      <td>0.353559</td>\n",
              "      <td>0.518789</td>\n",
              "      <td>-0.008207</td>\n",
              "      <td>0.299983</td>\n",
              "      <td>0.548277</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.384109</td>\n",
              "      <td>0.596862</td>\n",
              "      <td>-0.066926</td>\n",
              "      <td>0.197949</td>\n",
              "      <td>0.172215</td>\n",
              "      <td>0.135437</td>\n",
              "      <td>0.221080</td>\n",
              "      <td>0.213989</td>\n",
              "      <td>0.075058</td>\n",
              "      <td>0.245644</td>\n",
              "      <td>0.259729</td>\n",
              "      <td>0.025760</td>\n",
              "      <td>0.388712</td>\n",
              "      <td>0.889448</td>\n",
              "      <td>-0.026940</td>\n",
              "      <td>0.400085</td>\n",
              "      <td>0.264012</td>\n",
              "      <td>-0.029056</td>\n",
              "      <td>0.386800</td>\n",
              "      <td>0.188472</td>\n",
              "      <td>0.004013</td>\n",
              "      <td>0.371691</td>\n",
              "      <td>0.113295</td>\n",
              "      <td>0.040266</td>\n",
              "      <td>0.483361</td>\n",
              "      <td>0.104873</td>\n",
              "      <td>0.029170</td>\n",
              "      <td>0.243689</td>\n",
              "      <td>0.441992</td>\n",
              "      <td>0.059953</td>\n",
              "      <td>0.167194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.853994</td>\n",
              "      <td>-0.052045</td>\n",
              "      <td>0.490505</td>\n",
              "      <td>0.315880</td>\n",
              "      <td>-0.035176</td>\n",
              "      <td>0.415937</td>\n",
              "      <td>0.865337</td>\n",
              "      <td>-0.056379</td>\n",
              "      <td>0.405030</td>\n",
              "      <td>0.876524</td>\n",
              "      <td>-0.050644</td>\n",
              "      <td>0.370499</td>\n",
              "      <td>0.730605</td>\n",
              "      <td>-0.058097</td>\n",
              "      <td>0.122130</td>\n",
              "      <td>0.606974</td>\n",
              "      <td>0.366040</td>\n",
              "      <td>0.496748</td>\n",
              "      <td>0.616874</td>\n",
              "      <td>-0.129017</td>\n",
              "      <td>0.426585</td>\n",
              "      <td>0.837972</td>\n",
              "      <td>-0.038020</td>\n",
              "      <td>0.416297</td>\n",
              "      <td>0.842298</td>\n",
              "      <td>-0.041534</td>\n",
              "      <td>0.456737</td>\n",
              "      <td>0.637862</td>\n",
              "      <td>-0.096383</td>\n",
              "      <td>0.407725</td>\n",
              "      <td>0.638451</td>\n",
              "      <td>-0.058235</td>\n",
              "      <td>0.451836</td>\n",
              "      <td>0.630307</td>\n",
              "      <td>-0.095277</td>\n",
              "      <td>0.488749</td>\n",
              "      <td>0.268140</td>\n",
              "      <td>-0.034920</td>\n",
              "      <td>8110</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82193</th>\n",
              "      <td>0.539922</td>\n",
              "      <td>0.733853</td>\n",
              "      <td>-0.116346</td>\n",
              "      <td>0.380210</td>\n",
              "      <td>0.537888</td>\n",
              "      <td>-0.013831</td>\n",
              "      <td>0.331110</td>\n",
              "      <td>0.571202</td>\n",
              "      <td>-0.009661</td>\n",
              "      <td>0.418051</td>\n",
              "      <td>0.611137</td>\n",
              "      <td>-0.068765</td>\n",
              "      <td>0.201532</td>\n",
              "      <td>0.205195</td>\n",
              "      <td>0.119228</td>\n",
              "      <td>0.231375</td>\n",
              "      <td>0.248058</td>\n",
              "      <td>0.064256</td>\n",
              "      <td>0.261900</td>\n",
              "      <td>0.295296</td>\n",
              "      <td>0.018163</td>\n",
              "      <td>0.436721</td>\n",
              "      <td>0.889712</td>\n",
              "      <td>-0.043109</td>\n",
              "      <td>0.406493</td>\n",
              "      <td>0.281161</td>\n",
              "      <td>-0.023479</td>\n",
              "      <td>0.385404</td>\n",
              "      <td>0.203386</td>\n",
              "      <td>0.009993</td>\n",
              "      <td>0.363763</td>\n",
              "      <td>0.128143</td>\n",
              "      <td>0.044477</td>\n",
              "      <td>0.465458</td>\n",
              "      <td>0.110772</td>\n",
              "      <td>0.041087</td>\n",
              "      <td>0.268633</td>\n",
              "      <td>0.474940</td>\n",
              "      <td>0.045058</td>\n",
              "      <td>0.198463</td>\n",
              "      <td>...</td>\n",
              "      <td>0.846883</td>\n",
              "      <td>-0.060381</td>\n",
              "      <td>0.491801</td>\n",
              "      <td>0.323840</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>0.459720</td>\n",
              "      <td>0.858662</td>\n",
              "      <td>-0.065920</td>\n",
              "      <td>0.451042</td>\n",
              "      <td>0.870935</td>\n",
              "      <td>-0.061647</td>\n",
              "      <td>0.410078</td>\n",
              "      <td>0.744366</td>\n",
              "      <td>-0.063432</td>\n",
              "      <td>0.153611</td>\n",
              "      <td>0.658093</td>\n",
              "      <td>0.310812</td>\n",
              "      <td>0.527831</td>\n",
              "      <td>0.621047</td>\n",
              "      <td>-0.120687</td>\n",
              "      <td>0.462297</td>\n",
              "      <td>0.835062</td>\n",
              "      <td>-0.046397</td>\n",
              "      <td>0.453998</td>\n",
              "      <td>0.839352</td>\n",
              "      <td>-0.049860</td>\n",
              "      <td>0.490025</td>\n",
              "      <td>0.645820</td>\n",
              "      <td>-0.094305</td>\n",
              "      <td>0.442047</td>\n",
              "      <td>0.650435</td>\n",
              "      <td>-0.060938</td>\n",
              "      <td>0.485070</td>\n",
              "      <td>0.638753</td>\n",
              "      <td>-0.092646</td>\n",
              "      <td>0.486013</td>\n",
              "      <td>0.276309</td>\n",
              "      <td>-0.023272</td>\n",
              "      <td>8111</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82194</th>\n",
              "      <td>0.508446</td>\n",
              "      <td>0.805487</td>\n",
              "      <td>-0.070251</td>\n",
              "      <td>0.343381</td>\n",
              "      <td>0.533864</td>\n",
              "      <td>-0.043372</td>\n",
              "      <td>0.283602</td>\n",
              "      <td>0.561997</td>\n",
              "      <td>-0.029687</td>\n",
              "      <td>0.375949</td>\n",
              "      <td>0.647894</td>\n",
              "      <td>-0.074198</td>\n",
              "      <td>0.141720</td>\n",
              "      <td>0.084118</td>\n",
              "      <td>-0.054084</td>\n",
              "      <td>0.171338</td>\n",
              "      <td>0.152992</td>\n",
              "      <td>-0.087664</td>\n",
              "      <td>0.201176</td>\n",
              "      <td>0.220687</td>\n",
              "      <td>-0.112131</td>\n",
              "      <td>0.369933</td>\n",
              "      <td>0.922145</td>\n",
              "      <td>0.066492</td>\n",
              "      <td>0.380160</td>\n",
              "      <td>0.243267</td>\n",
              "      <td>-0.154192</td>\n",
              "      <td>0.365684</td>\n",
              "      <td>0.144395</td>\n",
              "      <td>-0.161555</td>\n",
              "      <td>0.345529</td>\n",
              "      <td>0.036684</td>\n",
              "      <td>-0.159371</td>\n",
              "      <td>0.475918</td>\n",
              "      <td>0.029879</td>\n",
              "      <td>-0.164239</td>\n",
              "      <td>0.215297</td>\n",
              "      <td>0.428085</td>\n",
              "      <td>-0.015826</td>\n",
              "      <td>0.131820</td>\n",
              "      <td>...</td>\n",
              "      <td>0.896993</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.490204</td>\n",
              "      <td>0.304551</td>\n",
              "      <td>-0.133139</td>\n",
              "      <td>0.392669</td>\n",
              "      <td>0.909338</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>0.382303</td>\n",
              "      <td>0.919254</td>\n",
              "      <td>0.033797</td>\n",
              "      <td>0.352291</td>\n",
              "      <td>0.776859</td>\n",
              "      <td>-0.013166</td>\n",
              "      <td>0.083205</td>\n",
              "      <td>0.521482</td>\n",
              "      <td>0.361173</td>\n",
              "      <td>0.508394</td>\n",
              "      <td>0.700883</td>\n",
              "      <td>-0.116656</td>\n",
              "      <td>0.397826</td>\n",
              "      <td>0.875864</td>\n",
              "      <td>0.039519</td>\n",
              "      <td>0.386713</td>\n",
              "      <td>0.880695</td>\n",
              "      <td>0.038403</td>\n",
              "      <td>0.457618</td>\n",
              "      <td>0.709737</td>\n",
              "      <td>-0.077995</td>\n",
              "      <td>0.397727</td>\n",
              "      <td>0.694100</td>\n",
              "      <td>-0.047890</td>\n",
              "      <td>0.451877</td>\n",
              "      <td>0.701441</td>\n",
              "      <td>-0.081601</td>\n",
              "      <td>0.487336</td>\n",
              "      <td>0.245632</td>\n",
              "      <td>-0.153780</td>\n",
              "      <td>8112</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>82195 rows × 1406 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_z  Unnamed: 0  correct\n",
              "0      0.470799  0.682127 -0.080930  ... -0.045183           0        0\n",
              "1      0.469567  0.799870 -0.017644  ... -0.206077           3        0\n",
              "2      0.711111  0.717369 -0.049849  ... -0.146556           4        0\n",
              "3      0.483527  0.809326 -0.011760  ... -0.195280           5        0\n",
              "4      0.061688  0.692855 -0.050553  ... -0.058228           6        0\n",
              "...         ...       ...       ...  ...       ...         ...      ...\n",
              "82190  0.436367  0.787778 -0.124726  ... -0.051585        8108        6\n",
              "82191  0.456891  0.704518 -0.093469  ... -0.061855        8109        6\n",
              "82192  0.502264  0.729350 -0.124422  ... -0.034920        8110        6\n",
              "82193  0.539922  0.733853 -0.116346  ... -0.023272        8111        6\n",
              "82194  0.508446  0.805487 -0.070251  ... -0.153780        8112        6\n",
              "\n",
              "[82195 rows x 1406 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "ZvzA8Gy1EUhN",
        "outputId": "fbb0ab0d-a95d-4fa0-ba19-d251dececea3"
      },
      "source": [
        "df.drop(columns=df.columns[[-2]],axis=1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0_x</th>\n",
              "      <th>0_y</th>\n",
              "      <th>0_z</th>\n",
              "      <th>100_x</th>\n",
              "      <th>100_y</th>\n",
              "      <th>100_z</th>\n",
              "      <th>101_x</th>\n",
              "      <th>101_y</th>\n",
              "      <th>101_z</th>\n",
              "      <th>102_x</th>\n",
              "      <th>102_y</th>\n",
              "      <th>102_z</th>\n",
              "      <th>103_x</th>\n",
              "      <th>103_y</th>\n",
              "      <th>103_z</th>\n",
              "      <th>104_x</th>\n",
              "      <th>104_y</th>\n",
              "      <th>104_z</th>\n",
              "      <th>105_x</th>\n",
              "      <th>105_y</th>\n",
              "      <th>105_z</th>\n",
              "      <th>106_x</th>\n",
              "      <th>106_y</th>\n",
              "      <th>106_z</th>\n",
              "      <th>107_x</th>\n",
              "      <th>107_y</th>\n",
              "      <th>107_z</th>\n",
              "      <th>108_x</th>\n",
              "      <th>108_y</th>\n",
              "      <th>108_z</th>\n",
              "      <th>109_x</th>\n",
              "      <th>109_y</th>\n",
              "      <th>109_z</th>\n",
              "      <th>10_x</th>\n",
              "      <th>10_y</th>\n",
              "      <th>10_z</th>\n",
              "      <th>110_x</th>\n",
              "      <th>110_y</th>\n",
              "      <th>110_z</th>\n",
              "      <th>111_x</th>\n",
              "      <th>...</th>\n",
              "      <th>89_x</th>\n",
              "      <th>89_y</th>\n",
              "      <th>89_z</th>\n",
              "      <th>8_x</th>\n",
              "      <th>8_y</th>\n",
              "      <th>8_z</th>\n",
              "      <th>90_x</th>\n",
              "      <th>90_y</th>\n",
              "      <th>90_z</th>\n",
              "      <th>91_x</th>\n",
              "      <th>91_y</th>\n",
              "      <th>91_z</th>\n",
              "      <th>92_x</th>\n",
              "      <th>92_y</th>\n",
              "      <th>92_z</th>\n",
              "      <th>93_x</th>\n",
              "      <th>93_y</th>\n",
              "      <th>93_z</th>\n",
              "      <th>94_x</th>\n",
              "      <th>94_y</th>\n",
              "      <th>94_z</th>\n",
              "      <th>95_x</th>\n",
              "      <th>95_y</th>\n",
              "      <th>95_z</th>\n",
              "      <th>96_x</th>\n",
              "      <th>96_y</th>\n",
              "      <th>96_z</th>\n",
              "      <th>97_x</th>\n",
              "      <th>97_y</th>\n",
              "      <th>97_z</th>\n",
              "      <th>98_x</th>\n",
              "      <th>98_y</th>\n",
              "      <th>98_z</th>\n",
              "      <th>99_x</th>\n",
              "      <th>99_y</th>\n",
              "      <th>99_z</th>\n",
              "      <th>9_x</th>\n",
              "      <th>9_y</th>\n",
              "      <th>9_z</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.470799</td>\n",
              "      <td>0.682127</td>\n",
              "      <td>-0.080930</td>\n",
              "      <td>0.339680</td>\n",
              "      <td>0.500239</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.296106</td>\n",
              "      <td>0.526307</td>\n",
              "      <td>0.013302</td>\n",
              "      <td>0.361964</td>\n",
              "      <td>0.573378</td>\n",
              "      <td>-0.044539</td>\n",
              "      <td>0.191371</td>\n",
              "      <td>0.194533</td>\n",
              "      <td>0.097439</td>\n",
              "      <td>0.212881</td>\n",
              "      <td>0.238277</td>\n",
              "      <td>0.052467</td>\n",
              "      <td>0.235472</td>\n",
              "      <td>0.285108</td>\n",
              "      <td>0.014390</td>\n",
              "      <td>0.385465</td>\n",
              "      <td>0.781734</td>\n",
              "      <td>-0.007592</td>\n",
              "      <td>0.357043</td>\n",
              "      <td>0.274428</td>\n",
              "      <td>-0.037246</td>\n",
              "      <td>0.340158</td>\n",
              "      <td>0.202853</td>\n",
              "      <td>-0.015436</td>\n",
              "      <td>0.321931</td>\n",
              "      <td>0.130519</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>0.412151</td>\n",
              "      <td>0.113719</td>\n",
              "      <td>-0.008327</td>\n",
              "      <td>0.253681</td>\n",
              "      <td>0.442930</td>\n",
              "      <td>0.056188</td>\n",
              "      <td>0.192371</td>\n",
              "      <td>...</td>\n",
              "      <td>0.405580</td>\n",
              "      <td>0.737290</td>\n",
              "      <td>-0.025478</td>\n",
              "      <td>0.434136</td>\n",
              "      <td>0.314108</td>\n",
              "      <td>-0.042840</td>\n",
              "      <td>0.398939</td>\n",
              "      <td>0.746655</td>\n",
              "      <td>-0.030840</td>\n",
              "      <td>0.393578</td>\n",
              "      <td>0.757806</td>\n",
              "      <td>-0.026364</td>\n",
              "      <td>0.351389</td>\n",
              "      <td>0.677704</td>\n",
              "      <td>-0.021752</td>\n",
              "      <td>0.173446</td>\n",
              "      <td>0.579080</td>\n",
              "      <td>0.323007</td>\n",
              "      <td>0.459225</td>\n",
              "      <td>0.599125</td>\n",
              "      <td>-0.099350</td>\n",
              "      <td>0.399182</td>\n",
              "      <td>0.731124</td>\n",
              "      <td>-0.006322</td>\n",
              "      <td>0.391899</td>\n",
              "      <td>0.733549</td>\n",
              "      <td>-0.009079</td>\n",
              "      <td>0.424361</td>\n",
              "      <td>0.611631</td>\n",
              "      <td>-0.070455</td>\n",
              "      <td>0.380985</td>\n",
              "      <td>0.606789</td>\n",
              "      <td>-0.037339</td>\n",
              "      <td>0.419343</td>\n",
              "      <td>0.605716</td>\n",
              "      <td>-0.069694</td>\n",
              "      <td>0.428503</td>\n",
              "      <td>0.269363</td>\n",
              "      <td>-0.045183</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.469567</td>\n",
              "      <td>0.799870</td>\n",
              "      <td>-0.017644</td>\n",
              "      <td>0.348688</td>\n",
              "      <td>0.513593</td>\n",
              "      <td>-0.045884</td>\n",
              "      <td>0.290292</td>\n",
              "      <td>0.530457</td>\n",
              "      <td>-0.030681</td>\n",
              "      <td>0.352485</td>\n",
              "      <td>0.633756</td>\n",
              "      <td>-0.053042</td>\n",
              "      <td>0.209036</td>\n",
              "      <td>0.094351</td>\n",
              "      <td>-0.139540</td>\n",
              "      <td>0.239477</td>\n",
              "      <td>0.182606</td>\n",
              "      <td>-0.157115</td>\n",
              "      <td>0.265887</td>\n",
              "      <td>0.273181</td>\n",
              "      <td>-0.170042</td>\n",
              "      <td>0.327009</td>\n",
              "      <td>0.827892</td>\n",
              "      <td>0.100199</td>\n",
              "      <td>0.425728</td>\n",
              "      <td>0.307515</td>\n",
              "      <td>-0.205852</td>\n",
              "      <td>0.416682</td>\n",
              "      <td>0.194925</td>\n",
              "      <td>-0.227324</td>\n",
              "      <td>0.406638</td>\n",
              "      <td>0.083548</td>\n",
              "      <td>-0.242068</td>\n",
              "      <td>0.529422</td>\n",
              "      <td>0.090397</td>\n",
              "      <td>-0.250223</td>\n",
              "      <td>0.250477</td>\n",
              "      <td>0.403172</td>\n",
              "      <td>-0.039577</td>\n",
              "      <td>0.157762</td>\n",
              "      <td>...</td>\n",
              "      <td>0.362563</td>\n",
              "      <td>0.798798</td>\n",
              "      <td>0.062630</td>\n",
              "      <td>0.508284</td>\n",
              "      <td>0.366295</td>\n",
              "      <td>-0.177562</td>\n",
              "      <td>0.354303</td>\n",
              "      <td>0.805612</td>\n",
              "      <td>0.062696</td>\n",
              "      <td>0.345936</td>\n",
              "      <td>0.813267</td>\n",
              "      <td>0.070353</td>\n",
              "      <td>0.316791</td>\n",
              "      <td>0.738285</td>\n",
              "      <td>0.024206</td>\n",
              "      <td>0.072487</td>\n",
              "      <td>0.447892</td>\n",
              "      <td>0.333080</td>\n",
              "      <td>0.477093</td>\n",
              "      <td>0.723211</td>\n",
              "      <td>-0.088660</td>\n",
              "      <td>0.350487</td>\n",
              "      <td>0.789399</td>\n",
              "      <td>0.078221</td>\n",
              "      <td>0.343767</td>\n",
              "      <td>0.790928</td>\n",
              "      <td>0.078075</td>\n",
              "      <td>0.422094</td>\n",
              "      <td>0.713270</td>\n",
              "      <td>-0.049938</td>\n",
              "      <td>0.363159</td>\n",
              "      <td>0.677821</td>\n",
              "      <td>-0.022846</td>\n",
              "      <td>0.415244</td>\n",
              "      <td>0.704734</td>\n",
              "      <td>-0.056130</td>\n",
              "      <td>0.513777</td>\n",
              "      <td>0.319253</td>\n",
              "      <td>-0.206077</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.717369</td>\n",
              "      <td>-0.049849</td>\n",
              "      <td>0.492522</td>\n",
              "      <td>0.549239</td>\n",
              "      <td>-0.050151</td>\n",
              "      <td>0.450243</td>\n",
              "      <td>0.587217</td>\n",
              "      <td>-0.045329</td>\n",
              "      <td>0.539467</td>\n",
              "      <td>0.635224</td>\n",
              "      <td>-0.070256</td>\n",
              "      <td>0.223377</td>\n",
              "      <td>0.272958</td>\n",
              "      <td>-0.083552</td>\n",
              "      <td>0.283708</td>\n",
              "      <td>0.326970</td>\n",
              "      <td>-0.109453</td>\n",
              "      <td>0.340850</td>\n",
              "      <td>0.387407</td>\n",
              "      <td>-0.131149</td>\n",
              "      <td>0.603942</td>\n",
              "      <td>0.857271</td>\n",
              "      <td>0.056888</td>\n",
              "      <td>0.490310</td>\n",
              "      <td>0.351051</td>\n",
              "      <td>-0.153234</td>\n",
              "      <td>0.437256</td>\n",
              "      <td>0.259501</td>\n",
              "      <td>-0.155545</td>\n",
              "      <td>0.385495</td>\n",
              "      <td>0.177831</td>\n",
              "      <td>-0.154630</td>\n",
              "      <td>0.488445</td>\n",
              "      <td>0.137941</td>\n",
              "      <td>-0.152786</td>\n",
              "      <td>0.367887</td>\n",
              "      <td>0.503332</td>\n",
              "      <td>-0.038149</td>\n",
              "      <td>0.294357</td>\n",
              "      <td>...</td>\n",
              "      <td>0.629462</td>\n",
              "      <td>0.822683</td>\n",
              "      <td>0.029002</td>\n",
              "      <td>0.578697</td>\n",
              "      <td>0.367118</td>\n",
              "      <td>-0.129393</td>\n",
              "      <td>0.622796</td>\n",
              "      <td>0.834589</td>\n",
              "      <td>0.028758</td>\n",
              "      <td>0.616266</td>\n",
              "      <td>0.845286</td>\n",
              "      <td>0.035142</td>\n",
              "      <td>0.550873</td>\n",
              "      <td>0.747057</td>\n",
              "      <td>-0.012852</td>\n",
              "      <td>0.228344</td>\n",
              "      <td>0.659144</td>\n",
              "      <td>0.268810</td>\n",
              "      <td>0.688327</td>\n",
              "      <td>0.643454</td>\n",
              "      <td>-0.101639</td>\n",
              "      <td>0.612709</td>\n",
              "      <td>0.811374</td>\n",
              "      <td>0.041834</td>\n",
              "      <td>0.604680</td>\n",
              "      <td>0.816566</td>\n",
              "      <td>0.040922</td>\n",
              "      <td>0.633793</td>\n",
              "      <td>0.664174</td>\n",
              "      <td>-0.071061</td>\n",
              "      <td>0.566127</td>\n",
              "      <td>0.666339</td>\n",
              "      <td>-0.046193</td>\n",
              "      <td>0.624728</td>\n",
              "      <td>0.659804</td>\n",
              "      <td>-0.075197</td>\n",
              "      <td>0.565757</td>\n",
              "      <td>0.328598</td>\n",
              "      <td>-0.146556</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.483527</td>\n",
              "      <td>0.809326</td>\n",
              "      <td>-0.011760</td>\n",
              "      <td>0.344644</td>\n",
              "      <td>0.581638</td>\n",
              "      <td>-0.033273</td>\n",
              "      <td>0.295061</td>\n",
              "      <td>0.597420</td>\n",
              "      <td>-0.013657</td>\n",
              "      <td>0.360514</td>\n",
              "      <td>0.680113</td>\n",
              "      <td>-0.041067</td>\n",
              "      <td>0.189817</td>\n",
              "      <td>0.220668</td>\n",
              "      <td>-0.127457</td>\n",
              "      <td>0.215817</td>\n",
              "      <td>0.290792</td>\n",
              "      <td>-0.141683</td>\n",
              "      <td>0.238612</td>\n",
              "      <td>0.363065</td>\n",
              "      <td>-0.150872</td>\n",
              "      <td>0.378712</td>\n",
              "      <td>0.844660</td>\n",
              "      <td>0.111848</td>\n",
              "      <td>0.382644</td>\n",
              "      <td>0.402066</td>\n",
              "      <td>-0.192089</td>\n",
              "      <td>0.370748</td>\n",
              "      <td>0.305048</td>\n",
              "      <td>-0.214974</td>\n",
              "      <td>0.355163</td>\n",
              "      <td>0.208554</td>\n",
              "      <td>-0.232295</td>\n",
              "      <td>0.464029</td>\n",
              "      <td>0.211647</td>\n",
              "      <td>-0.244948</td>\n",
              "      <td>0.247434</td>\n",
              "      <td>0.488584</td>\n",
              "      <td>-0.026676</td>\n",
              "      <td>0.175191</td>\n",
              "      <td>...</td>\n",
              "      <td>0.406199</td>\n",
              "      <td>0.820019</td>\n",
              "      <td>0.071907</td>\n",
              "      <td>0.465131</td>\n",
              "      <td>0.452157</td>\n",
              "      <td>-0.168719</td>\n",
              "      <td>0.398679</td>\n",
              "      <td>0.827414</td>\n",
              "      <td>0.072785</td>\n",
              "      <td>0.392145</td>\n",
              "      <td>0.834791</td>\n",
              "      <td>0.081655</td>\n",
              "      <td>0.346620</td>\n",
              "      <td>0.765851</td>\n",
              "      <td>0.036317</td>\n",
              "      <td>0.139569</td>\n",
              "      <td>0.514248</td>\n",
              "      <td>0.319508</td>\n",
              "      <td>0.475415</td>\n",
              "      <td>0.749521</td>\n",
              "      <td>-0.076082</td>\n",
              "      <td>0.396229</td>\n",
              "      <td>0.811944</td>\n",
              "      <td>0.086949</td>\n",
              "      <td>0.389692</td>\n",
              "      <td>0.813754</td>\n",
              "      <td>0.087736</td>\n",
              "      <td>0.431490</td>\n",
              "      <td>0.742306</td>\n",
              "      <td>-0.039092</td>\n",
              "      <td>0.377746</td>\n",
              "      <td>0.715237</td>\n",
              "      <td>-0.014063</td>\n",
              "      <td>0.424486</td>\n",
              "      <td>0.735751</td>\n",
              "      <td>-0.044687</td>\n",
              "      <td>0.464442</td>\n",
              "      <td>0.411362</td>\n",
              "      <td>-0.195280</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.061688</td>\n",
              "      <td>0.692855</td>\n",
              "      <td>-0.050553</td>\n",
              "      <td>0.088094</td>\n",
              "      <td>0.485313</td>\n",
              "      <td>0.096132</td>\n",
              "      <td>0.066970</td>\n",
              "      <td>0.506532</td>\n",
              "      <td>0.140697</td>\n",
              "      <td>0.043108</td>\n",
              "      <td>0.566985</td>\n",
              "      <td>0.046351</td>\n",
              "      <td>0.085716</td>\n",
              "      <td>0.166476</td>\n",
              "      <td>0.243169</td>\n",
              "      <td>0.064643</td>\n",
              "      <td>0.223619</td>\n",
              "      <td>0.188716</td>\n",
              "      <td>0.040649</td>\n",
              "      <td>0.289942</td>\n",
              "      <td>0.143320</td>\n",
              "      <td>0.082358</td>\n",
              "      <td>0.776436</td>\n",
              "      <td>0.096337</td>\n",
              "      <td>0.084009</td>\n",
              "      <td>0.307915</td>\n",
              "      <td>0.002832</td>\n",
              "      <td>0.094099</td>\n",
              "      <td>0.206533</td>\n",
              "      <td>0.022408</td>\n",
              "      <td>0.109919</td>\n",
              "      <td>0.122637</td>\n",
              "      <td>0.042296</td>\n",
              "      <td>0.163256</td>\n",
              "      <td>0.115645</td>\n",
              "      <td>-0.043075</td>\n",
              "      <td>0.080999</td>\n",
              "      <td>0.401563</td>\n",
              "      <td>0.211497</td>\n",
              "      <td>0.079665</td>\n",
              "      <td>...</td>\n",
              "      <td>0.077165</td>\n",
              "      <td>0.740983</td>\n",
              "      <td>0.055285</td>\n",
              "      <td>0.131059</td>\n",
              "      <td>0.346348</td>\n",
              "      <td>-0.051665</td>\n",
              "      <td>0.071792</td>\n",
              "      <td>0.746798</td>\n",
              "      <td>0.055567</td>\n",
              "      <td>0.069141</td>\n",
              "      <td>0.755517</td>\n",
              "      <td>0.065676</td>\n",
              "      <td>0.047076</td>\n",
              "      <td>0.671389</td>\n",
              "      <td>0.082513</td>\n",
              "      <td>0.229300</td>\n",
              "      <td>0.530411</td>\n",
              "      <td>0.530587</td>\n",
              "      <td>0.052978</td>\n",
              "      <td>0.619554</td>\n",
              "      <td>-0.066292</td>\n",
              "      <td>0.085567</td>\n",
              "      <td>0.738580</td>\n",
              "      <td>0.082343</td>\n",
              "      <td>0.080121</td>\n",
              "      <td>0.739309</td>\n",
              "      <td>0.081224</td>\n",
              "      <td>0.058816</td>\n",
              "      <td>0.624059</td>\n",
              "      <td>-0.017864</td>\n",
              "      <td>0.052244</td>\n",
              "      <td>0.608127</td>\n",
              "      <td>0.034917</td>\n",
              "      <td>0.055178</td>\n",
              "      <td>0.616471</td>\n",
              "      <td>-0.019056</td>\n",
              "      <td>0.126254</td>\n",
              "      <td>0.306380</td>\n",
              "      <td>-0.058228</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82190</th>\n",
              "      <td>0.436367</td>\n",
              "      <td>0.787778</td>\n",
              "      <td>-0.124726</td>\n",
              "      <td>0.343724</td>\n",
              "      <td>0.529838</td>\n",
              "      <td>0.029624</td>\n",
              "      <td>0.286873</td>\n",
              "      <td>0.550876</td>\n",
              "      <td>0.056680</td>\n",
              "      <td>0.348470</td>\n",
              "      <td>0.626252</td>\n",
              "      <td>-0.042911</td>\n",
              "      <td>0.254712</td>\n",
              "      <td>0.095861</td>\n",
              "      <td>0.198664</td>\n",
              "      <td>0.255640</td>\n",
              "      <td>0.142529</td>\n",
              "      <td>0.130252</td>\n",
              "      <td>0.261927</td>\n",
              "      <td>0.191391</td>\n",
              "      <td>0.074780</td>\n",
              "      <td>0.339070</td>\n",
              "      <td>0.897009</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.408799</td>\n",
              "      <td>0.218067</td>\n",
              "      <td>-0.025457</td>\n",
              "      <td>0.410505</td>\n",
              "      <td>0.142572</td>\n",
              "      <td>0.004624</td>\n",
              "      <td>0.414198</td>\n",
              "      <td>0.054057</td>\n",
              "      <td>0.037996</td>\n",
              "      <td>0.532120</td>\n",
              "      <td>0.060675</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.257418</td>\n",
              "      <td>0.427196</td>\n",
              "      <td>0.127661</td>\n",
              "      <td>0.195598</td>\n",
              "      <td>...</td>\n",
              "      <td>0.366158</td>\n",
              "      <td>0.861018</td>\n",
              "      <td>-0.032097</td>\n",
              "      <td>0.503545</td>\n",
              "      <td>0.298082</td>\n",
              "      <td>-0.048313</td>\n",
              "      <td>0.353288</td>\n",
              "      <td>0.871084</td>\n",
              "      <td>-0.038138</td>\n",
              "      <td>0.344398</td>\n",
              "      <td>0.881407</td>\n",
              "      <td>-0.030064</td>\n",
              "      <td>0.322134</td>\n",
              "      <td>0.758520</td>\n",
              "      <td>-0.018979</td>\n",
              "      <td>0.228365</td>\n",
              "      <td>0.563227</td>\n",
              "      <td>0.524367</td>\n",
              "      <td>0.437460</td>\n",
              "      <td>0.668506</td>\n",
              "      <td>-0.135968</td>\n",
              "      <td>0.368955</td>\n",
              "      <td>0.853075</td>\n",
              "      <td>-0.006710</td>\n",
              "      <td>0.358095</td>\n",
              "      <td>0.856069</td>\n",
              "      <td>-0.010641</td>\n",
              "      <td>0.405361</td>\n",
              "      <td>0.682727</td>\n",
              "      <td>-0.090688</td>\n",
              "      <td>0.365983</td>\n",
              "      <td>0.674993</td>\n",
              "      <td>-0.039105</td>\n",
              "      <td>0.401111</td>\n",
              "      <td>0.674640</td>\n",
              "      <td>-0.088754</td>\n",
              "      <td>0.507884</td>\n",
              "      <td>0.236569</td>\n",
              "      <td>-0.051585</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82191</th>\n",
              "      <td>0.456891</td>\n",
              "      <td>0.704518</td>\n",
              "      <td>-0.093469</td>\n",
              "      <td>0.350804</td>\n",
              "      <td>0.496862</td>\n",
              "      <td>-0.003796</td>\n",
              "      <td>0.301899</td>\n",
              "      <td>0.518511</td>\n",
              "      <td>0.008652</td>\n",
              "      <td>0.366491</td>\n",
              "      <td>0.575456</td>\n",
              "      <td>-0.050161</td>\n",
              "      <td>0.239688</td>\n",
              "      <td>0.157823</td>\n",
              "      <td>0.090260</td>\n",
              "      <td>0.254965</td>\n",
              "      <td>0.204339</td>\n",
              "      <td>0.041235</td>\n",
              "      <td>0.271476</td>\n",
              "      <td>0.255241</td>\n",
              "      <td>0.002160</td>\n",
              "      <td>0.358471</td>\n",
              "      <td>0.805676</td>\n",
              "      <td>-0.005432</td>\n",
              "      <td>0.407089</td>\n",
              "      <td>0.273282</td>\n",
              "      <td>-0.053026</td>\n",
              "      <td>0.402834</td>\n",
              "      <td>0.196553</td>\n",
              "      <td>-0.033259</td>\n",
              "      <td>0.397798</td>\n",
              "      <td>0.120587</td>\n",
              "      <td>-0.008418</td>\n",
              "      <td>0.499167</td>\n",
              "      <td>0.121729</td>\n",
              "      <td>-0.022568</td>\n",
              "      <td>0.264070</td>\n",
              "      <td>0.416111</td>\n",
              "      <td>0.052676</td>\n",
              "      <td>0.193656</td>\n",
              "      <td>...</td>\n",
              "      <td>0.392929</td>\n",
              "      <td>0.775459</td>\n",
              "      <td>-0.030029</td>\n",
              "      <td>0.482762</td>\n",
              "      <td>0.327106</td>\n",
              "      <td>-0.056223</td>\n",
              "      <td>0.382838</td>\n",
              "      <td>0.783833</td>\n",
              "      <td>-0.034135</td>\n",
              "      <td>0.374178</td>\n",
              "      <td>0.792643</td>\n",
              "      <td>-0.028977</td>\n",
              "      <td>0.349132</td>\n",
              "      <td>0.689029</td>\n",
              "      <td>-0.033610</td>\n",
              "      <td>0.155815</td>\n",
              "      <td>0.540453</td>\n",
              "      <td>0.354896</td>\n",
              "      <td>0.457088</td>\n",
              "      <td>0.609271</td>\n",
              "      <td>-0.107670</td>\n",
              "      <td>0.391434</td>\n",
              "      <td>0.767631</td>\n",
              "      <td>-0.016601</td>\n",
              "      <td>0.384023</td>\n",
              "      <td>0.770720</td>\n",
              "      <td>-0.019424</td>\n",
              "      <td>0.423652</td>\n",
              "      <td>0.620989</td>\n",
              "      <td>-0.075051</td>\n",
              "      <td>0.383313</td>\n",
              "      <td>0.614421</td>\n",
              "      <td>-0.040141</td>\n",
              "      <td>0.419421</td>\n",
              "      <td>0.613941</td>\n",
              "      <td>-0.075122</td>\n",
              "      <td>0.485513</td>\n",
              "      <td>0.282853</td>\n",
              "      <td>-0.061855</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82192</th>\n",
              "      <td>0.502264</td>\n",
              "      <td>0.729350</td>\n",
              "      <td>-0.124422</td>\n",
              "      <td>0.353559</td>\n",
              "      <td>0.518789</td>\n",
              "      <td>-0.008207</td>\n",
              "      <td>0.299983</td>\n",
              "      <td>0.548277</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.384109</td>\n",
              "      <td>0.596862</td>\n",
              "      <td>-0.066926</td>\n",
              "      <td>0.197949</td>\n",
              "      <td>0.172215</td>\n",
              "      <td>0.135437</td>\n",
              "      <td>0.221080</td>\n",
              "      <td>0.213989</td>\n",
              "      <td>0.075058</td>\n",
              "      <td>0.245644</td>\n",
              "      <td>0.259729</td>\n",
              "      <td>0.025760</td>\n",
              "      <td>0.388712</td>\n",
              "      <td>0.889448</td>\n",
              "      <td>-0.026940</td>\n",
              "      <td>0.400085</td>\n",
              "      <td>0.264012</td>\n",
              "      <td>-0.029056</td>\n",
              "      <td>0.386800</td>\n",
              "      <td>0.188472</td>\n",
              "      <td>0.004013</td>\n",
              "      <td>0.371691</td>\n",
              "      <td>0.113295</td>\n",
              "      <td>0.040266</td>\n",
              "      <td>0.483361</td>\n",
              "      <td>0.104873</td>\n",
              "      <td>0.029170</td>\n",
              "      <td>0.243689</td>\n",
              "      <td>0.441992</td>\n",
              "      <td>0.059953</td>\n",
              "      <td>0.167194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.429142</td>\n",
              "      <td>0.853994</td>\n",
              "      <td>-0.052045</td>\n",
              "      <td>0.490505</td>\n",
              "      <td>0.315880</td>\n",
              "      <td>-0.035176</td>\n",
              "      <td>0.415937</td>\n",
              "      <td>0.865337</td>\n",
              "      <td>-0.056379</td>\n",
              "      <td>0.405030</td>\n",
              "      <td>0.876524</td>\n",
              "      <td>-0.050644</td>\n",
              "      <td>0.370499</td>\n",
              "      <td>0.730605</td>\n",
              "      <td>-0.058097</td>\n",
              "      <td>0.122130</td>\n",
              "      <td>0.606974</td>\n",
              "      <td>0.366040</td>\n",
              "      <td>0.496748</td>\n",
              "      <td>0.616874</td>\n",
              "      <td>-0.129017</td>\n",
              "      <td>0.426585</td>\n",
              "      <td>0.837972</td>\n",
              "      <td>-0.038020</td>\n",
              "      <td>0.416297</td>\n",
              "      <td>0.842298</td>\n",
              "      <td>-0.041534</td>\n",
              "      <td>0.456737</td>\n",
              "      <td>0.637862</td>\n",
              "      <td>-0.096383</td>\n",
              "      <td>0.407725</td>\n",
              "      <td>0.638451</td>\n",
              "      <td>-0.058235</td>\n",
              "      <td>0.451836</td>\n",
              "      <td>0.630307</td>\n",
              "      <td>-0.095277</td>\n",
              "      <td>0.488749</td>\n",
              "      <td>0.268140</td>\n",
              "      <td>-0.034920</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82193</th>\n",
              "      <td>0.539922</td>\n",
              "      <td>0.733853</td>\n",
              "      <td>-0.116346</td>\n",
              "      <td>0.380210</td>\n",
              "      <td>0.537888</td>\n",
              "      <td>-0.013831</td>\n",
              "      <td>0.331110</td>\n",
              "      <td>0.571202</td>\n",
              "      <td>-0.009661</td>\n",
              "      <td>0.418051</td>\n",
              "      <td>0.611137</td>\n",
              "      <td>-0.068765</td>\n",
              "      <td>0.201532</td>\n",
              "      <td>0.205195</td>\n",
              "      <td>0.119228</td>\n",
              "      <td>0.231375</td>\n",
              "      <td>0.248058</td>\n",
              "      <td>0.064256</td>\n",
              "      <td>0.261900</td>\n",
              "      <td>0.295296</td>\n",
              "      <td>0.018163</td>\n",
              "      <td>0.436721</td>\n",
              "      <td>0.889712</td>\n",
              "      <td>-0.043109</td>\n",
              "      <td>0.406493</td>\n",
              "      <td>0.281161</td>\n",
              "      <td>-0.023479</td>\n",
              "      <td>0.385404</td>\n",
              "      <td>0.203386</td>\n",
              "      <td>0.009993</td>\n",
              "      <td>0.363763</td>\n",
              "      <td>0.128143</td>\n",
              "      <td>0.044477</td>\n",
              "      <td>0.465458</td>\n",
              "      <td>0.110772</td>\n",
              "      <td>0.041087</td>\n",
              "      <td>0.268633</td>\n",
              "      <td>0.474940</td>\n",
              "      <td>0.045058</td>\n",
              "      <td>0.198463</td>\n",
              "      <td>...</td>\n",
              "      <td>0.469884</td>\n",
              "      <td>0.846883</td>\n",
              "      <td>-0.060381</td>\n",
              "      <td>0.491801</td>\n",
              "      <td>0.323840</td>\n",
              "      <td>-0.025473</td>\n",
              "      <td>0.459720</td>\n",
              "      <td>0.858662</td>\n",
              "      <td>-0.065920</td>\n",
              "      <td>0.451042</td>\n",
              "      <td>0.870935</td>\n",
              "      <td>-0.061647</td>\n",
              "      <td>0.410078</td>\n",
              "      <td>0.744366</td>\n",
              "      <td>-0.063432</td>\n",
              "      <td>0.153611</td>\n",
              "      <td>0.658093</td>\n",
              "      <td>0.310812</td>\n",
              "      <td>0.527831</td>\n",
              "      <td>0.621047</td>\n",
              "      <td>-0.120687</td>\n",
              "      <td>0.462297</td>\n",
              "      <td>0.835062</td>\n",
              "      <td>-0.046397</td>\n",
              "      <td>0.453998</td>\n",
              "      <td>0.839352</td>\n",
              "      <td>-0.049860</td>\n",
              "      <td>0.490025</td>\n",
              "      <td>0.645820</td>\n",
              "      <td>-0.094305</td>\n",
              "      <td>0.442047</td>\n",
              "      <td>0.650435</td>\n",
              "      <td>-0.060938</td>\n",
              "      <td>0.485070</td>\n",
              "      <td>0.638753</td>\n",
              "      <td>-0.092646</td>\n",
              "      <td>0.486013</td>\n",
              "      <td>0.276309</td>\n",
              "      <td>-0.023272</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82194</th>\n",
              "      <td>0.508446</td>\n",
              "      <td>0.805487</td>\n",
              "      <td>-0.070251</td>\n",
              "      <td>0.343381</td>\n",
              "      <td>0.533864</td>\n",
              "      <td>-0.043372</td>\n",
              "      <td>0.283602</td>\n",
              "      <td>0.561997</td>\n",
              "      <td>-0.029687</td>\n",
              "      <td>0.375949</td>\n",
              "      <td>0.647894</td>\n",
              "      <td>-0.074198</td>\n",
              "      <td>0.141720</td>\n",
              "      <td>0.084118</td>\n",
              "      <td>-0.054084</td>\n",
              "      <td>0.171338</td>\n",
              "      <td>0.152992</td>\n",
              "      <td>-0.087664</td>\n",
              "      <td>0.201176</td>\n",
              "      <td>0.220687</td>\n",
              "      <td>-0.112131</td>\n",
              "      <td>0.369933</td>\n",
              "      <td>0.922145</td>\n",
              "      <td>0.066492</td>\n",
              "      <td>0.380160</td>\n",
              "      <td>0.243267</td>\n",
              "      <td>-0.154192</td>\n",
              "      <td>0.365684</td>\n",
              "      <td>0.144395</td>\n",
              "      <td>-0.161555</td>\n",
              "      <td>0.345529</td>\n",
              "      <td>0.036684</td>\n",
              "      <td>-0.159371</td>\n",
              "      <td>0.475918</td>\n",
              "      <td>0.029879</td>\n",
              "      <td>-0.164239</td>\n",
              "      <td>0.215297</td>\n",
              "      <td>0.428085</td>\n",
              "      <td>-0.015826</td>\n",
              "      <td>0.131820</td>\n",
              "      <td>...</td>\n",
              "      <td>0.405460</td>\n",
              "      <td>0.896993</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.490204</td>\n",
              "      <td>0.304551</td>\n",
              "      <td>-0.133139</td>\n",
              "      <td>0.392669</td>\n",
              "      <td>0.909338</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>0.382303</td>\n",
              "      <td>0.919254</td>\n",
              "      <td>0.033797</td>\n",
              "      <td>0.352291</td>\n",
              "      <td>0.776859</td>\n",
              "      <td>-0.013166</td>\n",
              "      <td>0.083205</td>\n",
              "      <td>0.521482</td>\n",
              "      <td>0.361173</td>\n",
              "      <td>0.508394</td>\n",
              "      <td>0.700883</td>\n",
              "      <td>-0.116656</td>\n",
              "      <td>0.397826</td>\n",
              "      <td>0.875864</td>\n",
              "      <td>0.039519</td>\n",
              "      <td>0.386713</td>\n",
              "      <td>0.880695</td>\n",
              "      <td>0.038403</td>\n",
              "      <td>0.457618</td>\n",
              "      <td>0.709737</td>\n",
              "      <td>-0.077995</td>\n",
              "      <td>0.397727</td>\n",
              "      <td>0.694100</td>\n",
              "      <td>-0.047890</td>\n",
              "      <td>0.451877</td>\n",
              "      <td>0.701441</td>\n",
              "      <td>-0.081601</td>\n",
              "      <td>0.487336</td>\n",
              "      <td>0.245632</td>\n",
              "      <td>-0.153780</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>82195 rows × 1405 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0_x       0_y       0_z  ...       9_y       9_z  correct\n",
              "0      0.470799  0.682127 -0.080930  ...  0.269363 -0.045183        0\n",
              "1      0.469567  0.799870 -0.017644  ...  0.319253 -0.206077        0\n",
              "2      0.711111  0.717369 -0.049849  ...  0.328598 -0.146556        0\n",
              "3      0.483527  0.809326 -0.011760  ...  0.411362 -0.195280        0\n",
              "4      0.061688  0.692855 -0.050553  ...  0.306380 -0.058228        0\n",
              "...         ...       ...       ...  ...       ...       ...      ...\n",
              "82190  0.436367  0.787778 -0.124726  ...  0.236569 -0.051585        6\n",
              "82191  0.456891  0.704518 -0.093469  ...  0.282853 -0.061855        6\n",
              "82192  0.502264  0.729350 -0.124422  ...  0.268140 -0.034920        6\n",
              "82193  0.539922  0.733853 -0.116346  ...  0.276309 -0.023272        6\n",
              "82194  0.508446  0.805487 -0.070251  ...  0.245632 -0.153780        6\n",
              "\n",
              "[82195 rows x 1405 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xf5l4ylWS_V"
      },
      "source": [
        "x=DataFrame(df.drop(\"correct\",axis=1))\n",
        "y=DataFrame(df[\"correct\"])\n",
        "#説明変数・目的変数をそれぞれ訓練データ・テストデータに分割\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.05)\n",
        "#データの整形\n",
        "x_train = x_train.astype(np.float)\n",
        "x_test = x_test.astype(np.float)\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train,7)\n",
        "y_test = np_utils.to_categorical(y_test,7)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sL9ng-uKFK5"
      },
      "source": [
        "#ニューラルネットワークの実装①\n",
        "model = Sequential()\n",
        "model.add(Dense(50, activation='relu', input_shape=(1405,)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(50, activation='relu', input_shape=(1405,)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(50, activation='relu', input_shape=(1405,)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(7, activation='softmax'))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4rTm8KqW0jK",
        "outputId": "0c792fc0-3533-40ea-c0bf-affb5f3b09bc"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 50)                70300     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 50)                2550      \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 50)                2550      \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 7)                 357       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 75,757\n",
            "Trainable params: 75,757\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC4QpTujW_-r"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "adam = Adam(learning_rate=1e-4)\n",
        "model.compile(optimizer=adam,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "704KOTdfW_7y",
        "outputId": "12e6a6d1-ff5e-41c5-9581-fe2d361b0469"
      },
      "source": [
        "history = model.fit(x_train, y_train,batch_size=200,epochs=1000,verbose=1,validation_data=(x_test, y_test))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "391/391 [==============================] - 4s 8ms/step - loss: 82.3857 - accuracy: 0.2456 - val_loss: 4.1832 - val_accuracy: 0.3258\n",
            "Epoch 2/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 16.9791 - accuracy: 0.2735 - val_loss: 1.9223 - val_accuracy: 0.3134\n",
            "Epoch 3/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 6.6713 - accuracy: 0.2731 - val_loss: 1.4251 - val_accuracy: 0.3107\n",
            "Epoch 4/1000\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 3.3640 - accuracy: 0.2778 - val_loss: 1.4516 - val_accuracy: 0.3273\n",
            "Epoch 5/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 2.1197 - accuracy: 0.2837 - val_loss: 1.4579 - val_accuracy: 0.3175\n",
            "Epoch 6/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.7067 - accuracy: 0.2978 - val_loss: 1.4302 - val_accuracy: 0.3251\n",
            "Epoch 7/1000\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.5579 - accuracy: 0.3027 - val_loss: 1.4106 - val_accuracy: 0.3255\n",
            "Epoch 8/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.4869 - accuracy: 0.3129 - val_loss: 1.4049 - val_accuracy: 0.3217\n",
            "Epoch 9/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.4514 - accuracy: 0.3162 - val_loss: 1.3879 - val_accuracy: 0.3251\n",
            "Epoch 10/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.4303 - accuracy: 0.3177 - val_loss: 1.3798 - val_accuracy: 0.3260\n",
            "Epoch 11/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.4119 - accuracy: 0.3227 - val_loss: 1.3734 - val_accuracy: 0.3253\n",
            "Epoch 12/1000\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.3993 - accuracy: 0.3248 - val_loss: 1.3616 - val_accuracy: 0.3333\n",
            "Epoch 13/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3893 - accuracy: 0.3273 - val_loss: 1.3530 - val_accuracy: 0.3314\n",
            "Epoch 14/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3784 - accuracy: 0.3324 - val_loss: 1.3427 - val_accuracy: 0.3372\n",
            "Epoch 15/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3686 - accuracy: 0.3364 - val_loss: 1.3362 - val_accuracy: 0.3399\n",
            "Epoch 16/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3622 - accuracy: 0.3389 - val_loss: 1.3295 - val_accuracy: 0.3453\n",
            "Epoch 17/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3567 - accuracy: 0.3396 - val_loss: 1.3242 - val_accuracy: 0.3467\n",
            "Epoch 18/1000\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.3493 - accuracy: 0.3422 - val_loss: 1.3243 - val_accuracy: 0.3465\n",
            "Epoch 19/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3460 - accuracy: 0.3434 - val_loss: 1.3180 - val_accuracy: 0.3499\n",
            "Epoch 20/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3423 - accuracy: 0.3429 - val_loss: 1.3150 - val_accuracy: 0.3555\n",
            "Epoch 21/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3369 - accuracy: 0.3445 - val_loss: 1.3118 - val_accuracy: 0.3616\n",
            "Epoch 22/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3326 - accuracy: 0.3477 - val_loss: 1.3136 - val_accuracy: 0.3518\n",
            "Epoch 23/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3298 - accuracy: 0.3464 - val_loss: 1.3060 - val_accuracy: 0.3533\n",
            "Epoch 24/1000\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.3268 - accuracy: 0.3474 - val_loss: 1.2995 - val_accuracy: 0.3577\n",
            "Epoch 25/1000\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.3229 - accuracy: 0.3489 - val_loss: 1.3011 - val_accuracy: 0.3672\n",
            "Epoch 26/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3183 - accuracy: 0.3499 - val_loss: 1.2978 - val_accuracy: 0.3582\n",
            "Epoch 27/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3163 - accuracy: 0.3544 - val_loss: 1.2949 - val_accuracy: 0.3589\n",
            "Epoch 28/1000\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.3119 - accuracy: 0.3563 - val_loss: 1.2861 - val_accuracy: 0.3662\n",
            "Epoch 29/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3097 - accuracy: 0.3562 - val_loss: 1.2860 - val_accuracy: 0.3684\n",
            "Epoch 30/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3075 - accuracy: 0.3575 - val_loss: 1.2842 - val_accuracy: 0.3740\n",
            "Epoch 31/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3033 - accuracy: 0.3603 - val_loss: 1.2852 - val_accuracy: 0.3674\n",
            "Epoch 32/1000\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.3026 - accuracy: 0.3612 - val_loss: 1.2805 - val_accuracy: 0.3788\n",
            "Epoch 33/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3009 - accuracy: 0.3639 - val_loss: 1.2804 - val_accuracy: 0.3810\n",
            "Epoch 34/1000\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.2983 - accuracy: 0.3649 - val_loss: 1.2686 - val_accuracy: 0.3837\n",
            "Epoch 35/1000\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.2951 - accuracy: 0.3688 - val_loss: 1.2714 - val_accuracy: 0.3876\n",
            "Epoch 36/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2924 - accuracy: 0.3718 - val_loss: 1.2791 - val_accuracy: 0.3925\n",
            "Epoch 37/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2895 - accuracy: 0.3723 - val_loss: 1.2704 - val_accuracy: 0.3983\n",
            "Epoch 38/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2877 - accuracy: 0.3764 - val_loss: 1.2647 - val_accuracy: 0.4029\n",
            "Epoch 39/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2859 - accuracy: 0.3786 - val_loss: 1.2677 - val_accuracy: 0.4022\n",
            "Epoch 40/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2849 - accuracy: 0.3772 - val_loss: 1.2614 - val_accuracy: 0.4092\n",
            "Epoch 41/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2818 - accuracy: 0.3805 - val_loss: 1.2538 - val_accuracy: 0.4012\n",
            "Epoch 42/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2794 - accuracy: 0.3800 - val_loss: 1.2616 - val_accuracy: 0.3993\n",
            "Epoch 43/1000\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.2782 - accuracy: 0.3797 - val_loss: 1.2545 - val_accuracy: 0.4083\n",
            "Epoch 44/1000\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.2746 - accuracy: 0.3829 - val_loss: 1.2434 - val_accuracy: 0.4105\n",
            "Epoch 45/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2715 - accuracy: 0.3849 - val_loss: 1.2454 - val_accuracy: 0.4071\n",
            "Epoch 46/1000\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.2713 - accuracy: 0.3855 - val_loss: 1.2598 - val_accuracy: 0.4012\n",
            "Epoch 47/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2672 - accuracy: 0.3878 - val_loss: 1.2443 - val_accuracy: 0.4134\n",
            "Epoch 48/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2626 - accuracy: 0.3916 - val_loss: 1.2324 - val_accuracy: 0.4241\n",
            "Epoch 49/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2588 - accuracy: 0.3930 - val_loss: 1.2567 - val_accuracy: 0.4263\n",
            "Epoch 50/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2533 - accuracy: 0.3941 - val_loss: 1.2310 - val_accuracy: 0.4324\n",
            "Epoch 51/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2485 - accuracy: 0.3957 - val_loss: 1.2101 - val_accuracy: 0.4268\n",
            "Epoch 52/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2419 - accuracy: 0.4010 - val_loss: 1.2136 - val_accuracy: 0.4331\n",
            "Epoch 53/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2362 - accuracy: 0.4056 - val_loss: 1.2128 - val_accuracy: 0.4136\n",
            "Epoch 54/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2310 - accuracy: 0.4090 - val_loss: 1.1845 - val_accuracy: 0.4428\n",
            "Epoch 55/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2303 - accuracy: 0.4084 - val_loss: 1.1958 - val_accuracy: 0.4348\n",
            "Epoch 56/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2199 - accuracy: 0.4139 - val_loss: 1.1804 - val_accuracy: 0.4453\n",
            "Epoch 57/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2199 - accuracy: 0.4147 - val_loss: 1.1800 - val_accuracy: 0.4416\n",
            "Epoch 58/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2144 - accuracy: 0.4207 - val_loss: 1.1688 - val_accuracy: 0.4540\n",
            "Epoch 59/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2145 - accuracy: 0.4227 - val_loss: 1.1526 - val_accuracy: 0.4698\n",
            "Epoch 60/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2092 - accuracy: 0.4252 - val_loss: 1.1499 - val_accuracy: 0.4567\n",
            "Epoch 61/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2061 - accuracy: 0.4258 - val_loss: 1.1737 - val_accuracy: 0.4698\n",
            "Epoch 62/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2045 - accuracy: 0.4345 - val_loss: 1.1471 - val_accuracy: 0.4881\n",
            "Epoch 63/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.2004 - accuracy: 0.4368 - val_loss: 1.1617 - val_accuracy: 0.5078\n",
            "Epoch 64/1000\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.1980 - accuracy: 0.4374 - val_loss: 1.1409 - val_accuracy: 0.4650\n",
            "Epoch 65/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1957 - accuracy: 0.4423 - val_loss: 1.1655 - val_accuracy: 0.4800\n",
            "Epoch 66/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1971 - accuracy: 0.4395 - val_loss: 1.1238 - val_accuracy: 0.4910\n",
            "Epoch 67/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1894 - accuracy: 0.4532 - val_loss: 1.1351 - val_accuracy: 0.5165\n",
            "Epoch 68/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1893 - accuracy: 0.4540 - val_loss: 1.1585 - val_accuracy: 0.4650\n",
            "Epoch 69/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1864 - accuracy: 0.4586 - val_loss: 1.1169 - val_accuracy: 0.5285\n",
            "Epoch 70/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1802 - accuracy: 0.4674 - val_loss: 1.1355 - val_accuracy: 0.5263\n",
            "Epoch 71/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1837 - accuracy: 0.4628 - val_loss: 1.1006 - val_accuracy: 0.5740\n",
            "Epoch 72/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1721 - accuracy: 0.4776 - val_loss: 1.1167 - val_accuracy: 0.5625\n",
            "Epoch 73/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1747 - accuracy: 0.4765 - val_loss: 1.1108 - val_accuracy: 0.5640\n",
            "Epoch 74/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1678 - accuracy: 0.4814 - val_loss: 1.0869 - val_accuracy: 0.5951\n",
            "Epoch 75/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1607 - accuracy: 0.4916 - val_loss: 1.0985 - val_accuracy: 0.5842\n",
            "Epoch 76/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1618 - accuracy: 0.4902 - val_loss: 1.0836 - val_accuracy: 0.5837\n",
            "Epoch 77/1000\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.1607 - accuracy: 0.4923 - val_loss: 1.0902 - val_accuracy: 0.5501\n",
            "Epoch 78/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1532 - accuracy: 0.4976 - val_loss: 1.1233 - val_accuracy: 0.5360\n",
            "Epoch 79/1000\n",
            "391/391 [==============================] - 3s 6ms/step - loss: 1.1561 - accuracy: 0.4968 - val_loss: 1.0900 - val_accuracy: 0.5740\n",
            "Epoch 80/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1508 - accuracy: 0.4993 - val_loss: 1.0668 - val_accuracy: 0.5971\n",
            "Epoch 81/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1505 - accuracy: 0.4967 - val_loss: 1.0846 - val_accuracy: 0.5462\n",
            "Epoch 82/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1475 - accuracy: 0.5020 - val_loss: 1.1305 - val_accuracy: 0.5153\n",
            "Epoch 83/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1399 - accuracy: 0.5092 - val_loss: 1.0771 - val_accuracy: 0.5839\n",
            "Epoch 84/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1414 - accuracy: 0.5058 - val_loss: 1.1022 - val_accuracy: 0.5535\n",
            "Epoch 85/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1458 - accuracy: 0.5026 - val_loss: 1.0534 - val_accuracy: 0.5871\n",
            "Epoch 86/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1429 - accuracy: 0.5030 - val_loss: 1.1028 - val_accuracy: 0.5426\n",
            "Epoch 87/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1356 - accuracy: 0.5104 - val_loss: 1.0599 - val_accuracy: 0.5978\n",
            "Epoch 88/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1333 - accuracy: 0.5120 - val_loss: 1.0552 - val_accuracy: 0.6010\n",
            "Epoch 89/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1268 - accuracy: 0.5185 - val_loss: 1.0953 - val_accuracy: 0.5526\n",
            "Epoch 90/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1433 - accuracy: 0.4993 - val_loss: 1.1300 - val_accuracy: 0.5012\n",
            "Epoch 91/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1298 - accuracy: 0.5158 - val_loss: 1.0435 - val_accuracy: 0.5864\n",
            "Epoch 92/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1382 - accuracy: 0.5057 - val_loss: 1.0464 - val_accuracy: 0.5973\n",
            "Epoch 93/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1256 - accuracy: 0.5181 - val_loss: 1.0465 - val_accuracy: 0.5995\n",
            "Epoch 94/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1270 - accuracy: 0.5182 - val_loss: 1.0434 - val_accuracy: 0.6027\n",
            "Epoch 95/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1271 - accuracy: 0.5174 - val_loss: 1.0401 - val_accuracy: 0.6000\n",
            "Epoch 96/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1303 - accuracy: 0.5114 - val_loss: 1.0455 - val_accuracy: 0.5725\n",
            "Epoch 97/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1201 - accuracy: 0.5193 - val_loss: 1.0325 - val_accuracy: 0.5976\n",
            "Epoch 98/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1268 - accuracy: 0.5119 - val_loss: 1.0718 - val_accuracy: 0.5759\n",
            "Epoch 99/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1199 - accuracy: 0.5216 - val_loss: 1.0467 - val_accuracy: 0.6010\n",
            "Epoch 100/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1313 - accuracy: 0.5084 - val_loss: 1.0592 - val_accuracy: 0.5827\n",
            "Epoch 101/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1178 - accuracy: 0.5228 - val_loss: 1.0572 - val_accuracy: 0.5856\n",
            "Epoch 102/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1208 - accuracy: 0.5190 - val_loss: 1.0505 - val_accuracy: 0.5993\n",
            "Epoch 103/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1162 - accuracy: 0.5194 - val_loss: 1.1368 - val_accuracy: 0.4905\n",
            "Epoch 104/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1169 - accuracy: 0.5233 - val_loss: 1.0622 - val_accuracy: 0.5718\n",
            "Epoch 105/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1167 - accuracy: 0.5207 - val_loss: 1.0183 - val_accuracy: 0.6066\n",
            "Epoch 106/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1319 - accuracy: 0.5044 - val_loss: 1.0954 - val_accuracy: 0.5506\n",
            "Epoch 107/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1089 - accuracy: 0.5257 - val_loss: 1.0309 - val_accuracy: 0.6051\n",
            "Epoch 108/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1178 - accuracy: 0.5194 - val_loss: 1.0889 - val_accuracy: 0.5389\n",
            "Epoch 109/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1180 - accuracy: 0.5168 - val_loss: 1.0410 - val_accuracy: 0.5786\n",
            "Epoch 110/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1097 - accuracy: 0.5268 - val_loss: 1.0312 - val_accuracy: 0.5915\n",
            "Epoch 111/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1168 - accuracy: 0.5209 - val_loss: 1.1023 - val_accuracy: 0.5355\n",
            "Epoch 112/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1143 - accuracy: 0.5217 - val_loss: 1.0326 - val_accuracy: 0.6071\n",
            "Epoch 113/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1084 - accuracy: 0.5256 - val_loss: 1.0310 - val_accuracy: 0.6073\n",
            "Epoch 114/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1081 - accuracy: 0.5232 - val_loss: 1.0332 - val_accuracy: 0.6051\n",
            "Epoch 115/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1058 - accuracy: 0.5272 - val_loss: 1.0396 - val_accuracy: 0.5949\n",
            "Epoch 116/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1078 - accuracy: 0.5245 - val_loss: 1.0205 - val_accuracy: 0.6107\n",
            "Epoch 117/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1049 - accuracy: 0.5256 - val_loss: 1.1876 - val_accuracy: 0.4401\n",
            "Epoch 118/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1072 - accuracy: 0.5244 - val_loss: 1.0391 - val_accuracy: 0.5954\n",
            "Epoch 119/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0986 - accuracy: 0.5317 - val_loss: 1.0443 - val_accuracy: 0.5932\n",
            "Epoch 120/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0999 - accuracy: 0.5276 - val_loss: 1.0752 - val_accuracy: 0.5491\n",
            "Epoch 121/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0994 - accuracy: 0.5306 - val_loss: 1.0666 - val_accuracy: 0.5625\n",
            "Epoch 122/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0978 - accuracy: 0.5288 - val_loss: 1.1357 - val_accuracy: 0.4905\n",
            "Epoch 123/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0948 - accuracy: 0.5334 - val_loss: 1.0186 - val_accuracy: 0.5908\n",
            "Epoch 124/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0944 - accuracy: 0.5319 - val_loss: 1.0256 - val_accuracy: 0.6090\n",
            "Epoch 125/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0937 - accuracy: 0.5335 - val_loss: 1.0230 - val_accuracy: 0.6024\n",
            "Epoch 126/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.1056 - accuracy: 0.5209 - val_loss: 1.0125 - val_accuracy: 0.6105\n",
            "Epoch 127/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0941 - accuracy: 0.5319 - val_loss: 1.0656 - val_accuracy: 0.5684\n",
            "Epoch 128/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0959 - accuracy: 0.5298 - val_loss: 1.1311 - val_accuracy: 0.5083\n",
            "Epoch 129/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0912 - accuracy: 0.5344 - val_loss: 0.9949 - val_accuracy: 0.6136\n",
            "Epoch 130/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0898 - accuracy: 0.5355 - val_loss: 1.0089 - val_accuracy: 0.6085\n",
            "Epoch 131/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0976 - accuracy: 0.5264 - val_loss: 1.0004 - val_accuracy: 0.6122\n",
            "Epoch 132/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0867 - accuracy: 0.5363 - val_loss: 1.0272 - val_accuracy: 0.5883\n",
            "Epoch 133/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0849 - accuracy: 0.5384 - val_loss: 1.0489 - val_accuracy: 0.5922\n",
            "Epoch 134/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0836 - accuracy: 0.5382 - val_loss: 1.0120 - val_accuracy: 0.6056\n",
            "Epoch 135/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0979 - accuracy: 0.5281 - val_loss: 1.0862 - val_accuracy: 0.5460\n",
            "Epoch 136/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0920 - accuracy: 0.5328 - val_loss: 1.0115 - val_accuracy: 0.6051\n",
            "Epoch 137/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0815 - accuracy: 0.5415 - val_loss: 1.0266 - val_accuracy: 0.6063\n",
            "Epoch 138/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0903 - accuracy: 0.5324 - val_loss: 1.0079 - val_accuracy: 0.6156\n",
            "Epoch 139/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0820 - accuracy: 0.5403 - val_loss: 1.0117 - val_accuracy: 0.6112\n",
            "Epoch 140/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0766 - accuracy: 0.5434 - val_loss: 1.0256 - val_accuracy: 0.5749\n",
            "Epoch 141/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0835 - accuracy: 0.5379 - val_loss: 1.0296 - val_accuracy: 0.5922\n",
            "Epoch 142/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0868 - accuracy: 0.5359 - val_loss: 1.0452 - val_accuracy: 0.5754\n",
            "Epoch 143/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0808 - accuracy: 0.5392 - val_loss: 1.0934 - val_accuracy: 0.5270\n",
            "Epoch 144/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0961 - accuracy: 0.5307 - val_loss: 1.0585 - val_accuracy: 0.5849\n",
            "Epoch 145/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0786 - accuracy: 0.5434 - val_loss: 1.0001 - val_accuracy: 0.6066\n",
            "Epoch 146/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0799 - accuracy: 0.5406 - val_loss: 1.0822 - val_accuracy: 0.5421\n",
            "Epoch 147/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0763 - accuracy: 0.5449 - val_loss: 0.9961 - val_accuracy: 0.6165\n",
            "Epoch 148/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0809 - accuracy: 0.5399 - val_loss: 1.0233 - val_accuracy: 0.5983\n",
            "Epoch 149/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0745 - accuracy: 0.5468 - val_loss: 0.9841 - val_accuracy: 0.6156\n",
            "Epoch 150/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0719 - accuracy: 0.5477 - val_loss: 0.9808 - val_accuracy: 0.6204\n",
            "Epoch 151/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0727 - accuracy: 0.5454 - val_loss: 1.0424 - val_accuracy: 0.5723\n",
            "Epoch 152/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0789 - accuracy: 0.5417 - val_loss: 1.0117 - val_accuracy: 0.6078\n",
            "Epoch 153/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0797 - accuracy: 0.5393 - val_loss: 1.0051 - val_accuracy: 0.6092\n",
            "Epoch 154/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0718 - accuracy: 0.5464 - val_loss: 0.9921 - val_accuracy: 0.6114\n",
            "Epoch 155/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0730 - accuracy: 0.5452 - val_loss: 1.0204 - val_accuracy: 0.5842\n",
            "Epoch 156/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0679 - accuracy: 0.5491 - val_loss: 1.0177 - val_accuracy: 0.5903\n",
            "Epoch 157/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0688 - accuracy: 0.5511 - val_loss: 1.0144 - val_accuracy: 0.6007\n",
            "Epoch 158/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0652 - accuracy: 0.5496 - val_loss: 1.0101 - val_accuracy: 0.5983\n",
            "Epoch 159/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0638 - accuracy: 0.5534 - val_loss: 1.0191 - val_accuracy: 0.6134\n",
            "Epoch 160/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0711 - accuracy: 0.5461 - val_loss: 1.0004 - val_accuracy: 0.6051\n",
            "Epoch 161/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0712 - accuracy: 0.5458 - val_loss: 1.1003 - val_accuracy: 0.5582\n",
            "Epoch 162/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0690 - accuracy: 0.5500 - val_loss: 1.0748 - val_accuracy: 0.5382\n",
            "Epoch 163/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0672 - accuracy: 0.5486 - val_loss: 1.0332 - val_accuracy: 0.5866\n",
            "Epoch 164/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0671 - accuracy: 0.5502 - val_loss: 0.9910 - val_accuracy: 0.6178\n",
            "Epoch 165/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0657 - accuracy: 0.5500 - val_loss: 0.9944 - val_accuracy: 0.6078\n",
            "Epoch 166/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0651 - accuracy: 0.5517 - val_loss: 1.0111 - val_accuracy: 0.5706\n",
            "Epoch 167/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0614 - accuracy: 0.5534 - val_loss: 1.0248 - val_accuracy: 0.5849\n",
            "Epoch 168/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0591 - accuracy: 0.5535 - val_loss: 1.0153 - val_accuracy: 0.6044\n",
            "Epoch 169/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0660 - accuracy: 0.5505 - val_loss: 0.9757 - val_accuracy: 0.6226\n",
            "Epoch 170/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0613 - accuracy: 0.5534 - val_loss: 1.0135 - val_accuracy: 0.5798\n",
            "Epoch 171/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0654 - accuracy: 0.5512 - val_loss: 1.0157 - val_accuracy: 0.5983\n",
            "Epoch 172/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0533 - accuracy: 0.5612 - val_loss: 0.9968 - val_accuracy: 0.6058\n",
            "Epoch 173/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0703 - accuracy: 0.5481 - val_loss: 1.0049 - val_accuracy: 0.6066\n",
            "Epoch 174/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0643 - accuracy: 0.5509 - val_loss: 0.9938 - val_accuracy: 0.6263\n",
            "Epoch 175/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0639 - accuracy: 0.5527 - val_loss: 0.9906 - val_accuracy: 0.6095\n",
            "Epoch 176/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0596 - accuracy: 0.5573 - val_loss: 1.0023 - val_accuracy: 0.5993\n",
            "Epoch 177/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0547 - accuracy: 0.5587 - val_loss: 1.0757 - val_accuracy: 0.5392\n",
            "Epoch 178/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0563 - accuracy: 0.5591 - val_loss: 0.9832 - val_accuracy: 0.6204\n",
            "Epoch 179/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0504 - accuracy: 0.5607 - val_loss: 0.9698 - val_accuracy: 0.6243\n",
            "Epoch 180/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0648 - accuracy: 0.5521 - val_loss: 1.0040 - val_accuracy: 0.6044\n",
            "Epoch 181/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0575 - accuracy: 0.5602 - val_loss: 0.9850 - val_accuracy: 0.6197\n",
            "Epoch 182/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0492 - accuracy: 0.5666 - val_loss: 1.0000 - val_accuracy: 0.6083\n",
            "Epoch 183/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0460 - accuracy: 0.5659 - val_loss: 0.9908 - val_accuracy: 0.6134\n",
            "Epoch 184/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0613 - accuracy: 0.5564 - val_loss: 0.9820 - val_accuracy: 0.6219\n",
            "Epoch 185/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0554 - accuracy: 0.5608 - val_loss: 0.9829 - val_accuracy: 0.6190\n",
            "Epoch 186/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0554 - accuracy: 0.5588 - val_loss: 0.9951 - val_accuracy: 0.6146\n",
            "Epoch 187/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0553 - accuracy: 0.5597 - val_loss: 1.0024 - val_accuracy: 0.6039\n",
            "Epoch 188/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0517 - accuracy: 0.5613 - val_loss: 1.0037 - val_accuracy: 0.6056\n",
            "Epoch 189/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0438 - accuracy: 0.5665 - val_loss: 0.9812 - val_accuracy: 0.6185\n",
            "Epoch 190/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0494 - accuracy: 0.5655 - val_loss: 1.0988 - val_accuracy: 0.5358\n",
            "Epoch 191/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0586 - accuracy: 0.5588 - val_loss: 1.0660 - val_accuracy: 0.5655\n",
            "Epoch 192/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0534 - accuracy: 0.5599 - val_loss: 1.0614 - val_accuracy: 0.5487\n",
            "Epoch 193/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0538 - accuracy: 0.5597 - val_loss: 1.0146 - val_accuracy: 0.5929\n",
            "Epoch 194/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0492 - accuracy: 0.5653 - val_loss: 1.0187 - val_accuracy: 0.5859\n",
            "Epoch 195/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0459 - accuracy: 0.5674 - val_loss: 1.0142 - val_accuracy: 0.5951\n",
            "Epoch 196/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0420 - accuracy: 0.5680 - val_loss: 0.9715 - val_accuracy: 0.6182\n",
            "Epoch 197/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0455 - accuracy: 0.5676 - val_loss: 1.0026 - val_accuracy: 0.6046\n",
            "Epoch 198/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0524 - accuracy: 0.5627 - val_loss: 0.9812 - val_accuracy: 0.6263\n",
            "Epoch 199/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0452 - accuracy: 0.5661 - val_loss: 1.0113 - val_accuracy: 0.6015\n",
            "Epoch 200/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0438 - accuracy: 0.5676 - val_loss: 0.9989 - val_accuracy: 0.6112\n",
            "Epoch 201/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0412 - accuracy: 0.5688 - val_loss: 0.9803 - val_accuracy: 0.6127\n",
            "Epoch 202/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0376 - accuracy: 0.5710 - val_loss: 0.9668 - val_accuracy: 0.6255\n",
            "Epoch 203/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0552 - accuracy: 0.5589 - val_loss: 0.9876 - val_accuracy: 0.6112\n",
            "Epoch 204/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0463 - accuracy: 0.5666 - val_loss: 0.9832 - val_accuracy: 0.6200\n",
            "Epoch 205/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0446 - accuracy: 0.5670 - val_loss: 0.9996 - val_accuracy: 0.6044\n",
            "Epoch 206/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0461 - accuracy: 0.5672 - val_loss: 0.9841 - val_accuracy: 0.6088\n",
            "Epoch 207/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0457 - accuracy: 0.5685 - val_loss: 1.0287 - val_accuracy: 0.5835\n",
            "Epoch 208/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0522 - accuracy: 0.5619 - val_loss: 1.0921 - val_accuracy: 0.5251\n",
            "Epoch 209/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0568 - accuracy: 0.5600 - val_loss: 0.9867 - val_accuracy: 0.6236\n",
            "Epoch 210/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0418 - accuracy: 0.5694 - val_loss: 0.9766 - val_accuracy: 0.6187\n",
            "Epoch 211/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0435 - accuracy: 0.5675 - val_loss: 1.0401 - val_accuracy: 0.5866\n",
            "Epoch 212/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0325 - accuracy: 0.5735 - val_loss: 0.9775 - val_accuracy: 0.6151\n",
            "Epoch 213/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0355 - accuracy: 0.5733 - val_loss: 0.9651 - val_accuracy: 0.6331\n",
            "Epoch 214/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0338 - accuracy: 0.5758 - val_loss: 0.9917 - val_accuracy: 0.6163\n",
            "Epoch 215/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0387 - accuracy: 0.5694 - val_loss: 0.9849 - val_accuracy: 0.6102\n",
            "Epoch 216/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0414 - accuracy: 0.5702 - val_loss: 1.0005 - val_accuracy: 0.6170\n",
            "Epoch 217/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0433 - accuracy: 0.5683 - val_loss: 1.0432 - val_accuracy: 0.5431\n",
            "Epoch 218/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0350 - accuracy: 0.5713 - val_loss: 0.9614 - val_accuracy: 0.6307\n",
            "Epoch 219/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0304 - accuracy: 0.5772 - val_loss: 0.9776 - val_accuracy: 0.6287\n",
            "Epoch 220/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0371 - accuracy: 0.5695 - val_loss: 1.0114 - val_accuracy: 0.6007\n",
            "Epoch 221/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0269 - accuracy: 0.5786 - val_loss: 1.0410 - val_accuracy: 0.5703\n",
            "Epoch 222/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0353 - accuracy: 0.5736 - val_loss: 1.0067 - val_accuracy: 0.6063\n",
            "Epoch 223/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0302 - accuracy: 0.5764 - val_loss: 0.9698 - val_accuracy: 0.6158\n",
            "Epoch 224/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0469 - accuracy: 0.5669 - val_loss: 0.9673 - val_accuracy: 0.6238\n",
            "Epoch 225/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0418 - accuracy: 0.5690 - val_loss: 1.0539 - val_accuracy: 0.5577\n",
            "Epoch 226/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0273 - accuracy: 0.5777 - val_loss: 0.9813 - val_accuracy: 0.6246\n",
            "Epoch 227/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0366 - accuracy: 0.5714 - val_loss: 0.9745 - val_accuracy: 0.6212\n",
            "Epoch 228/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0284 - accuracy: 0.5761 - val_loss: 0.9705 - val_accuracy: 0.6168\n",
            "Epoch 229/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0359 - accuracy: 0.5714 - val_loss: 0.9803 - val_accuracy: 0.6217\n",
            "Epoch 230/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0277 - accuracy: 0.5758 - val_loss: 1.0081 - val_accuracy: 0.6068\n",
            "Epoch 231/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0361 - accuracy: 0.5720 - val_loss: 1.0170 - val_accuracy: 0.5981\n",
            "Epoch 232/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0360 - accuracy: 0.5729 - val_loss: 1.0118 - val_accuracy: 0.5820\n",
            "Epoch 233/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0368 - accuracy: 0.5700 - val_loss: 1.0560 - val_accuracy: 0.5689\n",
            "Epoch 234/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0332 - accuracy: 0.5722 - val_loss: 0.9836 - val_accuracy: 0.6044\n",
            "Epoch 235/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0274 - accuracy: 0.5781 - val_loss: 0.9634 - val_accuracy: 0.6297\n",
            "Epoch 236/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0350 - accuracy: 0.5720 - val_loss: 0.9781 - val_accuracy: 0.6156\n",
            "Epoch 237/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0342 - accuracy: 0.5699 - val_loss: 0.9566 - val_accuracy: 0.6282\n",
            "Epoch 238/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0221 - accuracy: 0.5808 - val_loss: 0.9668 - val_accuracy: 0.6248\n",
            "Epoch 239/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0223 - accuracy: 0.5795 - val_loss: 1.0068 - val_accuracy: 0.5976\n",
            "Epoch 240/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0307 - accuracy: 0.5742 - val_loss: 0.9634 - val_accuracy: 0.6226\n",
            "Epoch 241/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0225 - accuracy: 0.5799 - val_loss: 0.9884 - val_accuracy: 0.6005\n",
            "Epoch 242/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0224 - accuracy: 0.5782 - val_loss: 0.9577 - val_accuracy: 0.6290\n",
            "Epoch 243/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0186 - accuracy: 0.5820 - val_loss: 0.9645 - val_accuracy: 0.6248\n",
            "Epoch 244/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0167 - accuracy: 0.5843 - val_loss: 0.9747 - val_accuracy: 0.6182\n",
            "Epoch 245/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0196 - accuracy: 0.5822 - val_loss: 0.9635 - val_accuracy: 0.6287\n",
            "Epoch 246/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0336 - accuracy: 0.5718 - val_loss: 0.9717 - val_accuracy: 0.6243\n",
            "Epoch 247/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0248 - accuracy: 0.5793 - val_loss: 0.9794 - val_accuracy: 0.6148\n",
            "Epoch 248/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0083 - accuracy: 0.5890 - val_loss: 0.9589 - val_accuracy: 0.6170\n",
            "Epoch 249/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0279 - accuracy: 0.5744 - val_loss: 0.9778 - val_accuracy: 0.6217\n",
            "Epoch 250/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0226 - accuracy: 0.5786 - val_loss: 1.0168 - val_accuracy: 0.5830\n",
            "Epoch 251/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0152 - accuracy: 0.5867 - val_loss: 0.9709 - val_accuracy: 0.6246\n",
            "Epoch 252/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0263 - accuracy: 0.5792 - val_loss: 0.9847 - val_accuracy: 0.6139\n",
            "Epoch 253/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0101 - accuracy: 0.5885 - val_loss: 0.9675 - val_accuracy: 0.6238\n",
            "Epoch 254/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0210 - accuracy: 0.5815 - val_loss: 0.9547 - val_accuracy: 0.6341\n",
            "Epoch 255/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0176 - accuracy: 0.5840 - val_loss: 0.9813 - val_accuracy: 0.6127\n",
            "Epoch 256/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0102 - accuracy: 0.5881 - val_loss: 0.9646 - val_accuracy: 0.6168\n",
            "Epoch 257/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0135 - accuracy: 0.5872 - val_loss: 0.9401 - val_accuracy: 0.6392\n",
            "Epoch 258/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0147 - accuracy: 0.5830 - val_loss: 1.0147 - val_accuracy: 0.5730\n",
            "Epoch 259/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0267 - accuracy: 0.5775 - val_loss: 0.9473 - val_accuracy: 0.6382\n",
            "Epoch 260/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0155 - accuracy: 0.5828 - val_loss: 0.9882 - val_accuracy: 0.6114\n",
            "Epoch 261/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0163 - accuracy: 0.5831 - val_loss: 0.9429 - val_accuracy: 0.6397\n",
            "Epoch 262/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0127 - accuracy: 0.5862 - val_loss: 0.9431 - val_accuracy: 0.6358\n",
            "Epoch 263/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0213 - accuracy: 0.5792 - val_loss: 0.9744 - val_accuracy: 0.6268\n",
            "Epoch 264/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0197 - accuracy: 0.5810 - val_loss: 1.0011 - val_accuracy: 0.6041\n",
            "Epoch 265/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0296 - accuracy: 0.5749 - val_loss: 0.9814 - val_accuracy: 0.6170\n",
            "Epoch 266/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0128 - accuracy: 0.5852 - val_loss: 0.9511 - val_accuracy: 0.6367\n",
            "Epoch 267/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0011 - accuracy: 0.5922 - val_loss: 0.9595 - val_accuracy: 0.6297\n",
            "Epoch 268/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0131 - accuracy: 0.5862 - val_loss: 0.9452 - val_accuracy: 0.6389\n",
            "Epoch 269/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0127 - accuracy: 0.5846 - val_loss: 0.9658 - val_accuracy: 0.6246\n",
            "Epoch 270/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0110 - accuracy: 0.5870 - val_loss: 1.0094 - val_accuracy: 0.5966\n",
            "Epoch 271/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0107 - accuracy: 0.5867 - val_loss: 0.9967 - val_accuracy: 0.6027\n",
            "Epoch 272/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0155 - accuracy: 0.5826 - val_loss: 0.9613 - val_accuracy: 0.6287\n",
            "Epoch 273/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0045 - accuracy: 0.5913 - val_loss: 0.9290 - val_accuracy: 0.6384\n",
            "Epoch 274/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0241 - accuracy: 0.5779 - val_loss: 0.9752 - val_accuracy: 0.6224\n",
            "Epoch 275/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0022 - accuracy: 0.5929 - val_loss: 0.9381 - val_accuracy: 0.6406\n",
            "Epoch 276/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0177 - accuracy: 0.5800 - val_loss: 0.9639 - val_accuracy: 0.6265\n",
            "Epoch 277/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0060 - accuracy: 0.5859 - val_loss: 0.9865 - val_accuracy: 0.6131\n",
            "Epoch 278/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0149 - accuracy: 0.5838 - val_loss: 0.9460 - val_accuracy: 0.6341\n",
            "Epoch 279/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0065 - accuracy: 0.5876 - val_loss: 0.9554 - val_accuracy: 0.6287\n",
            "Epoch 280/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0036 - accuracy: 0.5909 - val_loss: 0.9469 - val_accuracy: 0.6333\n",
            "Epoch 281/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0086 - accuracy: 0.5868 - val_loss: 0.9488 - val_accuracy: 0.6370\n",
            "Epoch 282/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0076 - accuracy: 0.5885 - val_loss: 0.9497 - val_accuracy: 0.6365\n",
            "Epoch 283/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0084 - accuracy: 0.5871 - val_loss: 0.9389 - val_accuracy: 0.6394\n",
            "Epoch 284/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0014 - accuracy: 0.5918 - val_loss: 0.9638 - val_accuracy: 0.6282\n",
            "Epoch 285/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0124 - accuracy: 0.5850 - val_loss: 0.9498 - val_accuracy: 0.6299\n",
            "Epoch 286/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0071 - accuracy: 0.5892 - val_loss: 1.0297 - val_accuracy: 0.5652\n",
            "Epoch 287/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0108 - accuracy: 0.5854 - val_loss: 0.9654 - val_accuracy: 0.6263\n",
            "Epoch 288/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0082 - accuracy: 0.5873 - val_loss: 0.9892 - val_accuracy: 0.5912\n",
            "Epoch 289/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0102 - accuracy: 0.5861 - val_loss: 1.0312 - val_accuracy: 0.5698\n",
            "Epoch 290/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0055 - accuracy: 0.5898 - val_loss: 1.0077 - val_accuracy: 0.5708\n",
            "Epoch 291/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9998 - accuracy: 0.5941 - val_loss: 0.9756 - val_accuracy: 0.6105\n",
            "Epoch 292/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0059 - accuracy: 0.5899 - val_loss: 0.9465 - val_accuracy: 0.6365\n",
            "Epoch 293/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0005 - accuracy: 0.5931 - val_loss: 0.9468 - val_accuracy: 0.6287\n",
            "Epoch 294/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0060 - accuracy: 0.5886 - val_loss: 0.9363 - val_accuracy: 0.6394\n",
            "Epoch 295/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9989 - accuracy: 0.5925 - val_loss: 0.9985 - val_accuracy: 0.6024\n",
            "Epoch 296/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0048 - accuracy: 0.5889 - val_loss: 0.9564 - val_accuracy: 0.6273\n",
            "Epoch 297/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0148 - accuracy: 0.5826 - val_loss: 0.9434 - val_accuracy: 0.6299\n",
            "Epoch 298/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0046 - accuracy: 0.5892 - val_loss: 0.9361 - val_accuracy: 0.6358\n",
            "Epoch 299/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0095 - accuracy: 0.5872 - val_loss: 1.0003 - val_accuracy: 0.6010\n",
            "Epoch 300/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0120 - accuracy: 0.5870 - val_loss: 0.9929 - val_accuracy: 0.6080\n",
            "Epoch 301/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0094 - accuracy: 0.5836 - val_loss: 0.9536 - val_accuracy: 0.6307\n",
            "Epoch 302/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0017 - accuracy: 0.5913 - val_loss: 0.9548 - val_accuracy: 0.6418\n",
            "Epoch 303/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9985 - accuracy: 0.5926 - val_loss: 0.9546 - val_accuracy: 0.6314\n",
            "Epoch 304/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9991 - accuracy: 0.5942 - val_loss: 0.9469 - val_accuracy: 0.6392\n",
            "Epoch 305/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9986 - accuracy: 0.5926 - val_loss: 1.0064 - val_accuracy: 0.5961\n",
            "Epoch 306/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9997 - accuracy: 0.5923 - val_loss: 0.9563 - val_accuracy: 0.6309\n",
            "Epoch 307/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0008 - accuracy: 0.5912 - val_loss: 0.9718 - val_accuracy: 0.6253\n",
            "Epoch 308/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9915 - accuracy: 0.5956 - val_loss: 0.9486 - val_accuracy: 0.6392\n",
            "Epoch 309/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0002 - accuracy: 0.5905 - val_loss: 0.9714 - val_accuracy: 0.6234\n",
            "Epoch 310/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0049 - accuracy: 0.5907 - val_loss: 0.9565 - val_accuracy: 0.6345\n",
            "Epoch 311/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9944 - accuracy: 0.5957 - val_loss: 1.0076 - val_accuracy: 0.5813\n",
            "Epoch 312/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0068 - accuracy: 0.5876 - val_loss: 0.9445 - val_accuracy: 0.6360\n",
            "Epoch 313/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9969 - accuracy: 0.5932 - val_loss: 0.9401 - val_accuracy: 0.6397\n",
            "Epoch 314/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0035 - accuracy: 0.5892 - val_loss: 0.9636 - val_accuracy: 0.6229\n",
            "Epoch 315/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0112 - accuracy: 0.5853 - val_loss: 0.9637 - val_accuracy: 0.6263\n",
            "Epoch 316/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9953 - accuracy: 0.5941 - val_loss: 0.9690 - val_accuracy: 0.6292\n",
            "Epoch 317/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9969 - accuracy: 0.5935 - val_loss: 0.9832 - val_accuracy: 0.5908\n",
            "Epoch 318/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0074 - accuracy: 0.5870 - val_loss: 0.9966 - val_accuracy: 0.6056\n",
            "Epoch 319/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9939 - accuracy: 0.5942 - val_loss: 0.9624 - val_accuracy: 0.6336\n",
            "Epoch 320/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9933 - accuracy: 0.5957 - val_loss: 0.9642 - val_accuracy: 0.6290\n",
            "Epoch 321/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0009 - accuracy: 0.5929 - val_loss: 0.9555 - val_accuracy: 0.6321\n",
            "Epoch 322/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9978 - accuracy: 0.5922 - val_loss: 0.9337 - val_accuracy: 0.6470\n",
            "Epoch 323/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0068 - accuracy: 0.5887 - val_loss: 0.9486 - val_accuracy: 0.6365\n",
            "Epoch 324/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9986 - accuracy: 0.5925 - val_loss: 0.9333 - val_accuracy: 0.6433\n",
            "Epoch 325/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9906 - accuracy: 0.5973 - val_loss: 0.9498 - val_accuracy: 0.6195\n",
            "Epoch 326/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0015 - accuracy: 0.5900 - val_loss: 0.9240 - val_accuracy: 0.6387\n",
            "Epoch 327/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9918 - accuracy: 0.5959 - val_loss: 0.9549 - val_accuracy: 0.6255\n",
            "Epoch 328/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0028 - accuracy: 0.5894 - val_loss: 0.9648 - val_accuracy: 0.6273\n",
            "Epoch 329/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9871 - accuracy: 0.5986 - val_loss: 0.9761 - val_accuracy: 0.6151\n",
            "Epoch 330/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9902 - accuracy: 0.5966 - val_loss: 0.9519 - val_accuracy: 0.6367\n",
            "Epoch 331/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9946 - accuracy: 0.5942 - val_loss: 0.9568 - val_accuracy: 0.6307\n",
            "Epoch 332/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9940 - accuracy: 0.5950 - val_loss: 0.9362 - val_accuracy: 0.6421\n",
            "Epoch 333/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9922 - accuracy: 0.5956 - val_loss: 0.9383 - val_accuracy: 0.6421\n",
            "Epoch 334/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9994 - accuracy: 0.5892 - val_loss: 0.9640 - val_accuracy: 0.6238\n",
            "Epoch 335/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9886 - accuracy: 0.5995 - val_loss: 0.9334 - val_accuracy: 0.6448\n",
            "Epoch 336/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9947 - accuracy: 0.5938 - val_loss: 0.9767 - val_accuracy: 0.6134\n",
            "Epoch 337/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9853 - accuracy: 0.6001 - val_loss: 0.9220 - val_accuracy: 0.6421\n",
            "Epoch 338/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9957 - accuracy: 0.5943 - val_loss: 0.9565 - val_accuracy: 0.6221\n",
            "Epoch 339/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9953 - accuracy: 0.5919 - val_loss: 0.9510 - val_accuracy: 0.6144\n",
            "Epoch 340/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9892 - accuracy: 0.5965 - val_loss: 0.9660 - val_accuracy: 0.6243\n",
            "Epoch 341/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9921 - accuracy: 0.5941 - val_loss: 0.9799 - val_accuracy: 0.6075\n",
            "Epoch 342/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9928 - accuracy: 0.5968 - val_loss: 0.9434 - val_accuracy: 0.6341\n",
            "Epoch 343/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0096 - accuracy: 0.5847 - val_loss: 0.9443 - val_accuracy: 0.6401\n",
            "Epoch 344/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9967 - accuracy: 0.5936 - val_loss: 0.9509 - val_accuracy: 0.6197\n",
            "Epoch 345/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9943 - accuracy: 0.5942 - val_loss: 1.0175 - val_accuracy: 0.5791\n",
            "Epoch 346/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9896 - accuracy: 0.5969 - val_loss: 0.9385 - val_accuracy: 0.6341\n",
            "Epoch 347/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9954 - accuracy: 0.5926 - val_loss: 0.9664 - val_accuracy: 0.6212\n",
            "Epoch 348/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9905 - accuracy: 0.5971 - val_loss: 0.9496 - val_accuracy: 0.6406\n",
            "Epoch 349/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9895 - accuracy: 0.5975 - val_loss: 0.9873 - val_accuracy: 0.6083\n",
            "Epoch 350/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9910 - accuracy: 0.5963 - val_loss: 0.9360 - val_accuracy: 0.6372\n",
            "Epoch 351/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9935 - accuracy: 0.5946 - val_loss: 0.9239 - val_accuracy: 0.6472\n",
            "Epoch 352/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9848 - accuracy: 0.6019 - val_loss: 0.9332 - val_accuracy: 0.6389\n",
            "Epoch 353/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9922 - accuracy: 0.5954 - val_loss: 0.9362 - val_accuracy: 0.6392\n",
            "Epoch 354/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9955 - accuracy: 0.5932 - val_loss: 0.9422 - val_accuracy: 0.6319\n",
            "Epoch 355/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9916 - accuracy: 0.5953 - val_loss: 0.9394 - val_accuracy: 0.6418\n",
            "Epoch 356/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9927 - accuracy: 0.5939 - val_loss: 0.9532 - val_accuracy: 0.6229\n",
            "Epoch 357/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9929 - accuracy: 0.5935 - val_loss: 0.9797 - val_accuracy: 0.6056\n",
            "Epoch 358/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9968 - accuracy: 0.5929 - val_loss: 0.9445 - val_accuracy: 0.6290\n",
            "Epoch 359/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9846 - accuracy: 0.5991 - val_loss: 0.9914 - val_accuracy: 0.6010\n",
            "Epoch 360/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9974 - accuracy: 0.5912 - val_loss: 0.9421 - val_accuracy: 0.6343\n",
            "Epoch 361/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9832 - accuracy: 0.6002 - val_loss: 1.0470 - val_accuracy: 0.5640\n",
            "Epoch 362/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9960 - accuracy: 0.5914 - val_loss: 0.9417 - val_accuracy: 0.6307\n",
            "Epoch 363/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9872 - accuracy: 0.5981 - val_loss: 0.9402 - val_accuracy: 0.6375\n",
            "Epoch 364/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9891 - accuracy: 0.5975 - val_loss: 0.9390 - val_accuracy: 0.6384\n",
            "Epoch 365/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9891 - accuracy: 0.5961 - val_loss: 0.9450 - val_accuracy: 0.6389\n",
            "Epoch 366/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9884 - accuracy: 0.5993 - val_loss: 0.9239 - val_accuracy: 0.6355\n",
            "Epoch 367/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9843 - accuracy: 0.6009 - val_loss: 0.9516 - val_accuracy: 0.6433\n",
            "Epoch 368/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9970 - accuracy: 0.5908 - val_loss: 0.9478 - val_accuracy: 0.6333\n",
            "Epoch 369/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9834 - accuracy: 0.5979 - val_loss: 0.9649 - val_accuracy: 0.6270\n",
            "Epoch 370/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9919 - accuracy: 0.5953 - val_loss: 0.9270 - val_accuracy: 0.6399\n",
            "Epoch 371/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9959 - accuracy: 0.5941 - val_loss: 0.9967 - val_accuracy: 0.6046\n",
            "Epoch 372/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9863 - accuracy: 0.6000 - val_loss: 0.9733 - val_accuracy: 0.6217\n",
            "Epoch 373/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9951 - accuracy: 0.5949 - val_loss: 0.9338 - val_accuracy: 0.6426\n",
            "Epoch 374/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9902 - accuracy: 0.5970 - val_loss: 0.9683 - val_accuracy: 0.6243\n",
            "Epoch 375/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9872 - accuracy: 0.5984 - val_loss: 0.9409 - val_accuracy: 0.6431\n",
            "Epoch 376/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9831 - accuracy: 0.5990 - val_loss: 0.9765 - val_accuracy: 0.6175\n",
            "Epoch 377/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9866 - accuracy: 0.5974 - val_loss: 0.9407 - val_accuracy: 0.6336\n",
            "Epoch 378/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9922 - accuracy: 0.5943 - val_loss: 0.9333 - val_accuracy: 0.6360\n",
            "Epoch 379/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9866 - accuracy: 0.5982 - val_loss: 0.9366 - val_accuracy: 0.6397\n",
            "Epoch 380/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9931 - accuracy: 0.5950 - val_loss: 0.9761 - val_accuracy: 0.6080\n",
            "Epoch 381/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9867 - accuracy: 0.5988 - val_loss: 0.9242 - val_accuracy: 0.6380\n",
            "Epoch 382/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9927 - accuracy: 0.5941 - val_loss: 1.0134 - val_accuracy: 0.5993\n",
            "Epoch 383/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9945 - accuracy: 0.5926 - val_loss: 0.9490 - val_accuracy: 0.6297\n",
            "Epoch 384/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9984 - accuracy: 0.5921 - val_loss: 1.0331 - val_accuracy: 0.5569\n",
            "Epoch 385/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9833 - accuracy: 0.6007 - val_loss: 0.9772 - val_accuracy: 0.6200\n",
            "Epoch 386/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9884 - accuracy: 0.5980 - val_loss: 0.9423 - val_accuracy: 0.6404\n",
            "Epoch 387/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9894 - accuracy: 0.5952 - val_loss: 0.9388 - val_accuracy: 0.6399\n",
            "Epoch 388/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9985 - accuracy: 0.5906 - val_loss: 1.0058 - val_accuracy: 0.5861\n",
            "Epoch 389/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9858 - accuracy: 0.5993 - val_loss: 0.9486 - val_accuracy: 0.6316\n",
            "Epoch 390/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9944 - accuracy: 0.5944 - val_loss: 1.0387 - val_accuracy: 0.5723\n",
            "Epoch 391/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9898 - accuracy: 0.5961 - val_loss: 0.9356 - val_accuracy: 0.6411\n",
            "Epoch 392/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9947 - accuracy: 0.5936 - val_loss: 0.9582 - val_accuracy: 0.6302\n",
            "Epoch 393/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9960 - accuracy: 0.5938 - val_loss: 0.9763 - val_accuracy: 0.6095\n",
            "Epoch 394/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9890 - accuracy: 0.5973 - val_loss: 0.9494 - val_accuracy: 0.6358\n",
            "Epoch 395/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9997 - accuracy: 0.5898 - val_loss: 0.9747 - val_accuracy: 0.6207\n",
            "Epoch 396/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9957 - accuracy: 0.5938 - val_loss: 0.9378 - val_accuracy: 0.6389\n",
            "Epoch 397/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9805 - accuracy: 0.6017 - val_loss: 0.9657 - val_accuracy: 0.6241\n",
            "Epoch 398/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9778 - accuracy: 0.6032 - val_loss: 0.9861 - val_accuracy: 0.5993\n",
            "Epoch 399/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9856 - accuracy: 0.5989 - val_loss: 0.9415 - val_accuracy: 0.6397\n",
            "Epoch 400/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9914 - accuracy: 0.5957 - val_loss: 0.9385 - val_accuracy: 0.6465\n",
            "Epoch 401/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9906 - accuracy: 0.5959 - val_loss: 1.0039 - val_accuracy: 0.5946\n",
            "Epoch 402/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9910 - accuracy: 0.5950 - val_loss: 0.9806 - val_accuracy: 0.6015\n",
            "Epoch 403/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9854 - accuracy: 0.5995 - val_loss: 0.9398 - val_accuracy: 0.6428\n",
            "Epoch 404/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9888 - accuracy: 0.5966 - val_loss: 0.9609 - val_accuracy: 0.6102\n",
            "Epoch 405/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9851 - accuracy: 0.5984 - val_loss: 0.9516 - val_accuracy: 0.6314\n",
            "Epoch 406/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9853 - accuracy: 0.5970 - val_loss: 1.0074 - val_accuracy: 0.6061\n",
            "Epoch 407/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9965 - accuracy: 0.5945 - val_loss: 0.9302 - val_accuracy: 0.6399\n",
            "Epoch 408/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9838 - accuracy: 0.5999 - val_loss: 0.9534 - val_accuracy: 0.6307\n",
            "Epoch 409/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9783 - accuracy: 0.6017 - val_loss: 0.9680 - val_accuracy: 0.6068\n",
            "Epoch 410/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9902 - accuracy: 0.5945 - val_loss: 0.9882 - val_accuracy: 0.6102\n",
            "Epoch 411/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9821 - accuracy: 0.6016 - val_loss: 0.9241 - val_accuracy: 0.6453\n",
            "Epoch 412/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9817 - accuracy: 0.6002 - val_loss: 0.9855 - val_accuracy: 0.5908\n",
            "Epoch 413/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9903 - accuracy: 0.5949 - val_loss: 0.9790 - val_accuracy: 0.6095\n",
            "Epoch 414/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9853 - accuracy: 0.5979 - val_loss: 0.9499 - val_accuracy: 0.6333\n",
            "Epoch 415/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9841 - accuracy: 0.5986 - val_loss: 1.0002 - val_accuracy: 0.6046\n",
            "Epoch 416/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9928 - accuracy: 0.5960 - val_loss: 0.9682 - val_accuracy: 0.6309\n",
            "Epoch 417/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9831 - accuracy: 0.6008 - val_loss: 0.9363 - val_accuracy: 0.6341\n",
            "Epoch 418/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9808 - accuracy: 0.6034 - val_loss: 0.9160 - val_accuracy: 0.6450\n",
            "Epoch 419/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9776 - accuracy: 0.6026 - val_loss: 0.9517 - val_accuracy: 0.6241\n",
            "Epoch 420/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9816 - accuracy: 0.6002 - val_loss: 0.9324 - val_accuracy: 0.6436\n",
            "Epoch 421/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9760 - accuracy: 0.6049 - val_loss: 0.9260 - val_accuracy: 0.6455\n",
            "Epoch 422/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9812 - accuracy: 0.6011 - val_loss: 0.9434 - val_accuracy: 0.6348\n",
            "Epoch 423/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9794 - accuracy: 0.6011 - val_loss: 0.9612 - val_accuracy: 0.6309\n",
            "Epoch 424/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0015 - accuracy: 0.5876 - val_loss: 0.9342 - val_accuracy: 0.6448\n",
            "Epoch 425/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9938 - accuracy: 0.5937 - val_loss: 0.9512 - val_accuracy: 0.6280\n",
            "Epoch 426/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9778 - accuracy: 0.6018 - val_loss: 0.9508 - val_accuracy: 0.6321\n",
            "Epoch 427/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9924 - accuracy: 0.5945 - val_loss: 0.9703 - val_accuracy: 0.6190\n",
            "Epoch 428/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9802 - accuracy: 0.6014 - val_loss: 0.9543 - val_accuracy: 0.6066\n",
            "Epoch 429/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9813 - accuracy: 0.5987 - val_loss: 0.9320 - val_accuracy: 0.6380\n",
            "Epoch 430/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9866 - accuracy: 0.5969 - val_loss: 1.0291 - val_accuracy: 0.5769\n",
            "Epoch 431/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9798 - accuracy: 0.6022 - val_loss: 0.9755 - val_accuracy: 0.6265\n",
            "Epoch 432/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9760 - accuracy: 0.6039 - val_loss: 0.9796 - val_accuracy: 0.6010\n",
            "Epoch 433/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9844 - accuracy: 0.5994 - val_loss: 0.9386 - val_accuracy: 0.6440\n",
            "Epoch 434/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9756 - accuracy: 0.6038 - val_loss: 0.9273 - val_accuracy: 0.6394\n",
            "Epoch 435/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9741 - accuracy: 0.6045 - val_loss: 0.9547 - val_accuracy: 0.6273\n",
            "Epoch 436/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9850 - accuracy: 0.5986 - val_loss: 0.9237 - val_accuracy: 0.6421\n",
            "Epoch 437/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9812 - accuracy: 0.5995 - val_loss: 0.9418 - val_accuracy: 0.6338\n",
            "Epoch 438/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9855 - accuracy: 0.5987 - val_loss: 0.9425 - val_accuracy: 0.6399\n",
            "Epoch 439/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9780 - accuracy: 0.6024 - val_loss: 0.9391 - val_accuracy: 0.6353\n",
            "Epoch 440/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9806 - accuracy: 0.6015 - val_loss: 0.9370 - val_accuracy: 0.6372\n",
            "Epoch 441/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9799 - accuracy: 0.6010 - val_loss: 0.9474 - val_accuracy: 0.6350\n",
            "Epoch 442/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9794 - accuracy: 0.6009 - val_loss: 0.9388 - val_accuracy: 0.6389\n",
            "Epoch 443/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9725 - accuracy: 0.6042 - val_loss: 0.9551 - val_accuracy: 0.6280\n",
            "Epoch 444/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9757 - accuracy: 0.6048 - val_loss: 0.9517 - val_accuracy: 0.6285\n",
            "Epoch 445/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9807 - accuracy: 0.6004 - val_loss: 0.9413 - val_accuracy: 0.6380\n",
            "Epoch 446/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9738 - accuracy: 0.6050 - val_loss: 0.9685 - val_accuracy: 0.6097\n",
            "Epoch 447/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9813 - accuracy: 0.6002 - val_loss: 0.9402 - val_accuracy: 0.6355\n",
            "Epoch 448/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9709 - accuracy: 0.6082 - val_loss: 0.9495 - val_accuracy: 0.6277\n",
            "Epoch 449/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9945 - accuracy: 0.5941 - val_loss: 0.9769 - val_accuracy: 0.6092\n",
            "Epoch 450/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9804 - accuracy: 0.6013 - val_loss: 0.9470 - val_accuracy: 0.6345\n",
            "Epoch 451/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9991 - accuracy: 0.5897 - val_loss: 0.9250 - val_accuracy: 0.6470\n",
            "Epoch 452/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9783 - accuracy: 0.6010 - val_loss: 0.9484 - val_accuracy: 0.6282\n",
            "Epoch 453/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9734 - accuracy: 0.6050 - val_loss: 0.9482 - val_accuracy: 0.6307\n",
            "Epoch 454/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9819 - accuracy: 0.6002 - val_loss: 1.0221 - val_accuracy: 0.5815\n",
            "Epoch 455/1000\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9780 - accuracy: 0.6028 - val_loss: 0.9292 - val_accuracy: 0.6479\n",
            "Epoch 456/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9781 - accuracy: 0.6037 - val_loss: 0.9291 - val_accuracy: 0.6474\n",
            "Epoch 457/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9856 - accuracy: 0.5998 - val_loss: 0.9202 - val_accuracy: 0.6533\n",
            "Epoch 458/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9793 - accuracy: 0.6009 - val_loss: 0.9284 - val_accuracy: 0.6453\n",
            "Epoch 459/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9798 - accuracy: 0.6018 - val_loss: 0.9484 - val_accuracy: 0.6377\n",
            "Epoch 460/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9764 - accuracy: 0.6033 - val_loss: 0.9462 - val_accuracy: 0.6282\n",
            "Epoch 461/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9821 - accuracy: 0.5992 - val_loss: 0.9652 - val_accuracy: 0.6175\n",
            "Epoch 462/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9769 - accuracy: 0.6022 - val_loss: 0.9507 - val_accuracy: 0.6311\n",
            "Epoch 463/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9779 - accuracy: 0.6023 - val_loss: 0.9397 - val_accuracy: 0.6418\n",
            "Epoch 464/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9768 - accuracy: 0.6024 - val_loss: 0.9492 - val_accuracy: 0.6299\n",
            "Epoch 465/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9769 - accuracy: 0.6041 - val_loss: 0.9476 - val_accuracy: 0.6397\n",
            "Epoch 466/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9873 - accuracy: 0.5965 - val_loss: 0.9251 - val_accuracy: 0.6455\n",
            "Epoch 467/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9750 - accuracy: 0.6044 - val_loss: 0.9641 - val_accuracy: 0.6207\n",
            "Epoch 468/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9770 - accuracy: 0.6029 - val_loss: 0.9457 - val_accuracy: 0.6314\n",
            "Epoch 469/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9724 - accuracy: 0.6053 - val_loss: 1.0278 - val_accuracy: 0.5708\n",
            "Epoch 470/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9790 - accuracy: 0.6009 - val_loss: 0.9363 - val_accuracy: 0.6474\n",
            "Epoch 471/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9792 - accuracy: 0.6027 - val_loss: 0.9691 - val_accuracy: 0.6226\n",
            "Epoch 472/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9854 - accuracy: 0.5976 - val_loss: 1.0228 - val_accuracy: 0.5591\n",
            "Epoch 473/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9782 - accuracy: 0.6026 - val_loss: 0.9264 - val_accuracy: 0.6470\n",
            "Epoch 474/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9862 - accuracy: 0.5975 - val_loss: 0.9535 - val_accuracy: 0.6285\n",
            "Epoch 475/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9796 - accuracy: 0.6020 - val_loss: 0.9552 - val_accuracy: 0.6365\n",
            "Epoch 476/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9757 - accuracy: 0.6034 - val_loss: 0.9263 - val_accuracy: 0.6404\n",
            "Epoch 477/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9748 - accuracy: 0.6033 - val_loss: 0.9268 - val_accuracy: 0.6411\n",
            "Epoch 478/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9665 - accuracy: 0.6100 - val_loss: 0.9288 - val_accuracy: 0.6409\n",
            "Epoch 479/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9716 - accuracy: 0.6055 - val_loss: 0.9480 - val_accuracy: 0.6144\n",
            "Epoch 480/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9767 - accuracy: 0.6035 - val_loss: 0.9604 - val_accuracy: 0.6195\n",
            "Epoch 481/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9797 - accuracy: 0.6014 - val_loss: 0.9156 - val_accuracy: 0.6472\n",
            "Epoch 482/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9679 - accuracy: 0.6072 - val_loss: 0.9397 - val_accuracy: 0.6341\n",
            "Epoch 483/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9771 - accuracy: 0.6032 - val_loss: 0.9200 - val_accuracy: 0.6479\n",
            "Epoch 484/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9845 - accuracy: 0.5989 - val_loss: 0.9405 - val_accuracy: 0.6333\n",
            "Epoch 485/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9777 - accuracy: 0.6021 - val_loss: 0.9552 - val_accuracy: 0.6297\n",
            "Epoch 486/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9775 - accuracy: 0.6029 - val_loss: 0.9442 - val_accuracy: 0.6382\n",
            "Epoch 487/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9787 - accuracy: 0.6039 - val_loss: 0.9205 - val_accuracy: 0.6487\n",
            "Epoch 488/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9819 - accuracy: 0.5998 - val_loss: 0.9487 - val_accuracy: 0.6255\n",
            "Epoch 489/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9762 - accuracy: 0.6028 - val_loss: 0.9165 - val_accuracy: 0.6499\n",
            "Epoch 490/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9753 - accuracy: 0.6020 - val_loss: 0.9168 - val_accuracy: 0.6462\n",
            "Epoch 491/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9786 - accuracy: 0.6035 - val_loss: 0.9422 - val_accuracy: 0.6251\n",
            "Epoch 492/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9778 - accuracy: 0.6041 - val_loss: 0.9530 - val_accuracy: 0.6241\n",
            "Epoch 493/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9791 - accuracy: 0.6004 - val_loss: 0.9286 - val_accuracy: 0.6399\n",
            "Epoch 494/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9746 - accuracy: 0.6043 - val_loss: 0.9804 - val_accuracy: 0.6170\n",
            "Epoch 495/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9718 - accuracy: 0.6059 - val_loss: 0.9706 - val_accuracy: 0.6212\n",
            "Epoch 496/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9782 - accuracy: 0.6015 - val_loss: 0.9759 - val_accuracy: 0.6212\n",
            "Epoch 497/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9827 - accuracy: 0.6001 - val_loss: 0.9736 - val_accuracy: 0.6217\n",
            "Epoch 498/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9739 - accuracy: 0.6058 - val_loss: 0.9175 - val_accuracy: 0.6513\n",
            "Epoch 499/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9745 - accuracy: 0.6042 - val_loss: 0.9300 - val_accuracy: 0.6392\n",
            "Epoch 500/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9823 - accuracy: 0.6004 - val_loss: 0.9475 - val_accuracy: 0.6416\n",
            "Epoch 501/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9776 - accuracy: 0.6018 - val_loss: 0.9741 - val_accuracy: 0.6139\n",
            "Epoch 502/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9715 - accuracy: 0.6062 - val_loss: 0.9331 - val_accuracy: 0.6433\n",
            "Epoch 503/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9744 - accuracy: 0.6031 - val_loss: 1.0219 - val_accuracy: 0.5620\n",
            "Epoch 504/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9745 - accuracy: 0.6037 - val_loss: 0.9649 - val_accuracy: 0.6105\n",
            "Epoch 505/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9785 - accuracy: 0.6026 - val_loss: 0.9200 - val_accuracy: 0.6516\n",
            "Epoch 506/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9687 - accuracy: 0.6070 - val_loss: 0.9814 - val_accuracy: 0.6095\n",
            "Epoch 507/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9834 - accuracy: 0.5978 - val_loss: 0.9333 - val_accuracy: 0.6521\n",
            "Epoch 508/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9704 - accuracy: 0.6070 - val_loss: 0.9337 - val_accuracy: 0.6421\n",
            "Epoch 509/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9763 - accuracy: 0.6039 - val_loss: 0.9543 - val_accuracy: 0.6265\n",
            "Epoch 510/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9698 - accuracy: 0.6067 - val_loss: 0.9317 - val_accuracy: 0.6440\n",
            "Epoch 511/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9724 - accuracy: 0.6046 - val_loss: 0.9310 - val_accuracy: 0.6438\n",
            "Epoch 512/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9692 - accuracy: 0.6064 - val_loss: 0.9650 - val_accuracy: 0.6229\n",
            "Epoch 513/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9719 - accuracy: 0.6053 - val_loss: 0.9400 - val_accuracy: 0.6406\n",
            "Epoch 514/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9652 - accuracy: 0.6104 - val_loss: 0.9437 - val_accuracy: 0.6404\n",
            "Epoch 515/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9752 - accuracy: 0.6045 - val_loss: 0.9463 - val_accuracy: 0.6175\n",
            "Epoch 516/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9718 - accuracy: 0.6050 - val_loss: 0.9174 - val_accuracy: 0.6470\n",
            "Epoch 517/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9716 - accuracy: 0.6050 - val_loss: 0.9361 - val_accuracy: 0.6314\n",
            "Epoch 518/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9808 - accuracy: 0.5998 - val_loss: 0.9433 - val_accuracy: 0.6336\n",
            "Epoch 519/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9746 - accuracy: 0.6033 - val_loss: 1.0340 - val_accuracy: 0.5679\n",
            "Epoch 520/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9770 - accuracy: 0.6021 - val_loss: 0.9556 - val_accuracy: 0.6321\n",
            "Epoch 521/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9828 - accuracy: 0.5991 - val_loss: 0.9368 - val_accuracy: 0.6404\n",
            "Epoch 522/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9732 - accuracy: 0.6055 - val_loss: 0.9268 - val_accuracy: 0.6365\n",
            "Epoch 523/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9821 - accuracy: 0.6008 - val_loss: 0.9455 - val_accuracy: 0.6358\n",
            "Epoch 524/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9667 - accuracy: 0.6085 - val_loss: 0.9151 - val_accuracy: 0.6472\n",
            "Epoch 525/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9742 - accuracy: 0.6049 - val_loss: 0.9367 - val_accuracy: 0.6450\n",
            "Epoch 526/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9775 - accuracy: 0.6029 - val_loss: 0.9257 - val_accuracy: 0.6421\n",
            "Epoch 527/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9743 - accuracy: 0.6052 - val_loss: 0.9332 - val_accuracy: 0.6360\n",
            "Epoch 528/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9802 - accuracy: 0.6002 - val_loss: 0.9589 - val_accuracy: 0.6348\n",
            "Epoch 529/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9796 - accuracy: 0.6012 - val_loss: 1.0500 - val_accuracy: 0.5769\n",
            "Epoch 530/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9701 - accuracy: 0.6086 - val_loss: 0.9283 - val_accuracy: 0.6418\n",
            "Epoch 531/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9650 - accuracy: 0.6082 - val_loss: 0.9614 - val_accuracy: 0.6202\n",
            "Epoch 532/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9707 - accuracy: 0.6058 - val_loss: 0.9309 - val_accuracy: 0.6365\n",
            "Epoch 533/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9695 - accuracy: 0.6069 - val_loss: 0.9479 - val_accuracy: 0.6263\n",
            "Epoch 534/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9677 - accuracy: 0.6081 - val_loss: 0.9377 - val_accuracy: 0.6285\n",
            "Epoch 535/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9694 - accuracy: 0.6074 - val_loss: 0.9484 - val_accuracy: 0.6287\n",
            "Epoch 536/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9690 - accuracy: 0.6071 - val_loss: 0.9579 - val_accuracy: 0.6307\n",
            "Epoch 537/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9658 - accuracy: 0.6094 - val_loss: 0.9360 - val_accuracy: 0.6423\n",
            "Epoch 538/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9687 - accuracy: 0.6074 - val_loss: 0.9358 - val_accuracy: 0.6316\n",
            "Epoch 539/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9721 - accuracy: 0.6063 - val_loss: 0.9510 - val_accuracy: 0.6285\n",
            "Epoch 540/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9825 - accuracy: 0.5978 - val_loss: 0.9382 - val_accuracy: 0.6399\n",
            "Epoch 541/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9694 - accuracy: 0.6075 - val_loss: 1.0048 - val_accuracy: 0.5915\n",
            "Epoch 542/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9716 - accuracy: 0.6042 - val_loss: 0.9386 - val_accuracy: 0.6406\n",
            "Epoch 543/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9730 - accuracy: 0.6055 - val_loss: 0.9744 - val_accuracy: 0.5971\n",
            "Epoch 544/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9684 - accuracy: 0.6069 - val_loss: 0.9266 - val_accuracy: 0.6460\n",
            "Epoch 545/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9692 - accuracy: 0.6064 - val_loss: 0.9482 - val_accuracy: 0.6343\n",
            "Epoch 546/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9749 - accuracy: 0.6034 - val_loss: 0.9487 - val_accuracy: 0.6241\n",
            "Epoch 547/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9697 - accuracy: 0.6066 - val_loss: 0.9891 - val_accuracy: 0.6117\n",
            "Epoch 548/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9734 - accuracy: 0.6029 - val_loss: 0.9237 - val_accuracy: 0.6450\n",
            "Epoch 549/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9699 - accuracy: 0.6064 - val_loss: 0.9268 - val_accuracy: 0.6472\n",
            "Epoch 550/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9663 - accuracy: 0.6080 - val_loss: 0.9528 - val_accuracy: 0.6338\n",
            "Epoch 551/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9682 - accuracy: 0.6068 - val_loss: 0.9106 - val_accuracy: 0.6526\n",
            "Epoch 552/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9703 - accuracy: 0.6080 - val_loss: 0.9820 - val_accuracy: 0.5942\n",
            "Epoch 553/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9788 - accuracy: 0.6003 - val_loss: 0.9266 - val_accuracy: 0.6484\n",
            "Epoch 554/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9732 - accuracy: 0.6038 - val_loss: 0.9348 - val_accuracy: 0.6268\n",
            "Epoch 555/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9711 - accuracy: 0.6046 - val_loss: 0.9271 - val_accuracy: 0.6438\n",
            "Epoch 556/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9701 - accuracy: 0.6058 - val_loss: 0.9183 - val_accuracy: 0.6489\n",
            "Epoch 557/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9668 - accuracy: 0.6095 - val_loss: 0.9253 - val_accuracy: 0.6406\n",
            "Epoch 558/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9667 - accuracy: 0.6081 - val_loss: 0.9479 - val_accuracy: 0.6370\n",
            "Epoch 559/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9682 - accuracy: 0.6078 - val_loss: 0.9422 - val_accuracy: 0.6270\n",
            "Epoch 560/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9742 - accuracy: 0.6032 - val_loss: 0.9694 - val_accuracy: 0.6207\n",
            "Epoch 561/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9775 - accuracy: 0.6018 - val_loss: 0.9273 - val_accuracy: 0.6457\n",
            "Epoch 562/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9720 - accuracy: 0.6049 - val_loss: 0.9401 - val_accuracy: 0.6377\n",
            "Epoch 563/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9690 - accuracy: 0.6066 - val_loss: 0.9322 - val_accuracy: 0.6411\n",
            "Epoch 564/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9713 - accuracy: 0.6058 - val_loss: 0.9685 - val_accuracy: 0.6007\n",
            "Epoch 565/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9667 - accuracy: 0.6066 - val_loss: 0.9236 - val_accuracy: 0.6457\n",
            "Epoch 566/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9696 - accuracy: 0.6069 - val_loss: 0.9699 - val_accuracy: 0.6107\n",
            "Epoch 567/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9696 - accuracy: 0.6063 - val_loss: 0.9573 - val_accuracy: 0.6234\n",
            "Epoch 568/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9736 - accuracy: 0.6039 - val_loss: 0.9220 - val_accuracy: 0.6372\n",
            "Epoch 569/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9710 - accuracy: 0.6056 - val_loss: 0.9626 - val_accuracy: 0.6236\n",
            "Epoch 570/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9715 - accuracy: 0.6051 - val_loss: 0.9393 - val_accuracy: 0.6270\n",
            "Epoch 571/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9649 - accuracy: 0.6079 - val_loss: 0.9251 - val_accuracy: 0.6338\n",
            "Epoch 572/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9712 - accuracy: 0.6049 - val_loss: 0.9414 - val_accuracy: 0.6389\n",
            "Epoch 573/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9627 - accuracy: 0.6108 - val_loss: 0.9493 - val_accuracy: 0.6331\n",
            "Epoch 574/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9680 - accuracy: 0.6062 - val_loss: 0.9466 - val_accuracy: 0.6355\n",
            "Epoch 575/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9716 - accuracy: 0.6054 - val_loss: 0.9315 - val_accuracy: 0.6307\n",
            "Epoch 576/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9664 - accuracy: 0.6077 - val_loss: 0.9631 - val_accuracy: 0.6165\n",
            "Epoch 577/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9667 - accuracy: 0.6101 - val_loss: 0.9322 - val_accuracy: 0.6367\n",
            "Epoch 578/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9659 - accuracy: 0.6077 - val_loss: 0.9317 - val_accuracy: 0.6431\n",
            "Epoch 579/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9650 - accuracy: 0.6090 - val_loss: 0.9268 - val_accuracy: 0.6450\n",
            "Epoch 580/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9661 - accuracy: 0.6096 - val_loss: 0.9907 - val_accuracy: 0.6182\n",
            "Epoch 581/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9746 - accuracy: 0.6034 - val_loss: 0.9392 - val_accuracy: 0.6367\n",
            "Epoch 582/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9663 - accuracy: 0.6078 - val_loss: 0.9437 - val_accuracy: 0.6338\n",
            "Epoch 583/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9714 - accuracy: 0.6050 - val_loss: 0.9735 - val_accuracy: 0.6158\n",
            "Epoch 584/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9714 - accuracy: 0.6035 - val_loss: 0.9755 - val_accuracy: 0.6136\n",
            "Epoch 585/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9681 - accuracy: 0.6073 - val_loss: 0.9409 - val_accuracy: 0.6426\n",
            "Epoch 586/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9772 - accuracy: 0.6015 - val_loss: 0.9285 - val_accuracy: 0.6436\n",
            "Epoch 587/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9678 - accuracy: 0.6076 - val_loss: 0.9286 - val_accuracy: 0.6343\n",
            "Epoch 588/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9703 - accuracy: 0.6049 - val_loss: 0.9300 - val_accuracy: 0.6406\n",
            "Epoch 589/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9746 - accuracy: 0.6043 - val_loss: 0.9378 - val_accuracy: 0.6404\n",
            "Epoch 590/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9619 - accuracy: 0.6111 - val_loss: 0.9738 - val_accuracy: 0.6195\n",
            "Epoch 591/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9650 - accuracy: 0.6093 - val_loss: 0.9442 - val_accuracy: 0.6384\n",
            "Epoch 592/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9798 - accuracy: 0.6013 - val_loss: 1.0021 - val_accuracy: 0.6105\n",
            "Epoch 593/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9676 - accuracy: 0.6086 - val_loss: 0.9249 - val_accuracy: 0.6453\n",
            "Epoch 594/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9662 - accuracy: 0.6078 - val_loss: 0.9579 - val_accuracy: 0.6260\n",
            "Epoch 595/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9669 - accuracy: 0.6090 - val_loss: 0.9259 - val_accuracy: 0.6470\n",
            "Epoch 596/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9707 - accuracy: 0.6056 - val_loss: 0.9896 - val_accuracy: 0.6063\n",
            "Epoch 597/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9696 - accuracy: 0.6064 - val_loss: 0.9682 - val_accuracy: 0.6209\n",
            "Epoch 598/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9639 - accuracy: 0.6083 - val_loss: 0.9552 - val_accuracy: 0.6360\n",
            "Epoch 599/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9750 - accuracy: 0.6033 - val_loss: 0.9681 - val_accuracy: 0.6253\n",
            "Epoch 600/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9672 - accuracy: 0.6082 - val_loss: 0.9249 - val_accuracy: 0.6465\n",
            "Epoch 601/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9698 - accuracy: 0.6064 - val_loss: 0.9223 - val_accuracy: 0.6455\n",
            "Epoch 602/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9681 - accuracy: 0.6071 - val_loss: 0.9673 - val_accuracy: 0.6229\n",
            "Epoch 603/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9719 - accuracy: 0.6063 - val_loss: 0.9388 - val_accuracy: 0.6426\n",
            "Epoch 604/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9630 - accuracy: 0.6095 - val_loss: 0.9273 - val_accuracy: 0.6423\n",
            "Epoch 605/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9643 - accuracy: 0.6075 - val_loss: 0.9244 - val_accuracy: 0.6440\n",
            "Epoch 606/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9621 - accuracy: 0.6105 - val_loss: 0.9355 - val_accuracy: 0.6372\n",
            "Epoch 607/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9586 - accuracy: 0.6124 - val_loss: 0.9525 - val_accuracy: 0.6236\n",
            "Epoch 608/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9683 - accuracy: 0.6088 - val_loss: 0.9706 - val_accuracy: 0.6277\n",
            "Epoch 609/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9885 - accuracy: 0.5963 - val_loss: 0.9324 - val_accuracy: 0.6423\n",
            "Epoch 610/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9696 - accuracy: 0.6061 - val_loss: 0.9200 - val_accuracy: 0.6453\n",
            "Epoch 611/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9752 - accuracy: 0.6019 - val_loss: 1.0024 - val_accuracy: 0.5852\n",
            "Epoch 612/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9687 - accuracy: 0.6066 - val_loss: 0.9613 - val_accuracy: 0.6041\n",
            "Epoch 613/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9659 - accuracy: 0.6091 - val_loss: 0.9146 - val_accuracy: 0.6445\n",
            "Epoch 614/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9617 - accuracy: 0.6109 - val_loss: 0.9495 - val_accuracy: 0.6341\n",
            "Epoch 615/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9645 - accuracy: 0.6086 - val_loss: 0.9351 - val_accuracy: 0.6387\n",
            "Epoch 616/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9723 - accuracy: 0.6046 - val_loss: 0.9590 - val_accuracy: 0.6290\n",
            "Epoch 617/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9671 - accuracy: 0.6081 - val_loss: 0.9189 - val_accuracy: 0.6443\n",
            "Epoch 618/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9707 - accuracy: 0.6037 - val_loss: 0.9489 - val_accuracy: 0.6307\n",
            "Epoch 619/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9626 - accuracy: 0.6096 - val_loss: 0.9504 - val_accuracy: 0.6294\n",
            "Epoch 620/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9629 - accuracy: 0.6108 - val_loss: 0.9642 - val_accuracy: 0.6180\n",
            "Epoch 621/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9669 - accuracy: 0.6068 - val_loss: 0.9766 - val_accuracy: 0.6170\n",
            "Epoch 622/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9730 - accuracy: 0.6038 - val_loss: 0.9682 - val_accuracy: 0.6224\n",
            "Epoch 623/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9679 - accuracy: 0.6090 - val_loss: 0.9463 - val_accuracy: 0.6311\n",
            "Epoch 624/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9724 - accuracy: 0.6034 - val_loss: 0.9797 - val_accuracy: 0.6214\n",
            "Epoch 625/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9664 - accuracy: 0.6077 - val_loss: 0.9418 - val_accuracy: 0.6467\n",
            "Epoch 626/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9667 - accuracy: 0.6077 - val_loss: 0.9180 - val_accuracy: 0.6477\n",
            "Epoch 627/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9666 - accuracy: 0.6061 - val_loss: 0.9537 - val_accuracy: 0.6268\n",
            "Epoch 628/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9678 - accuracy: 0.6068 - val_loss: 0.9650 - val_accuracy: 0.6251\n",
            "Epoch 629/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9605 - accuracy: 0.6128 - val_loss: 0.9719 - val_accuracy: 0.6139\n",
            "Epoch 630/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9672 - accuracy: 0.6060 - val_loss: 0.9589 - val_accuracy: 0.6129\n",
            "Epoch 631/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9644 - accuracy: 0.6104 - val_loss: 0.9719 - val_accuracy: 0.6180\n",
            "Epoch 632/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9701 - accuracy: 0.6050 - val_loss: 0.9603 - val_accuracy: 0.6280\n",
            "Epoch 633/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9656 - accuracy: 0.6075 - val_loss: 0.9344 - val_accuracy: 0.6431\n",
            "Epoch 634/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9676 - accuracy: 0.6076 - val_loss: 0.9399 - val_accuracy: 0.6367\n",
            "Epoch 635/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9669 - accuracy: 0.6072 - val_loss: 0.9207 - val_accuracy: 0.6423\n",
            "Epoch 636/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9700 - accuracy: 0.6054 - val_loss: 0.9288 - val_accuracy: 0.6443\n",
            "Epoch 637/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9634 - accuracy: 0.6111 - val_loss: 0.9367 - val_accuracy: 0.6401\n",
            "Epoch 638/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9623 - accuracy: 0.6110 - val_loss: 1.0068 - val_accuracy: 0.5927\n",
            "Epoch 639/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9727 - accuracy: 0.6034 - val_loss: 1.0142 - val_accuracy: 0.5891\n",
            "Epoch 640/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9635 - accuracy: 0.6091 - val_loss: 0.9299 - val_accuracy: 0.6479\n",
            "Epoch 641/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9587 - accuracy: 0.6119 - val_loss: 0.9344 - val_accuracy: 0.6394\n",
            "Epoch 642/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9583 - accuracy: 0.6115 - val_loss: 0.9386 - val_accuracy: 0.6426\n",
            "Epoch 643/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9750 - accuracy: 0.6023 - val_loss: 0.9308 - val_accuracy: 0.6409\n",
            "Epoch 644/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9659 - accuracy: 0.6100 - val_loss: 0.9198 - val_accuracy: 0.6484\n",
            "Epoch 645/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9629 - accuracy: 0.6074 - val_loss: 0.9784 - val_accuracy: 0.6139\n",
            "Epoch 646/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9668 - accuracy: 0.6086 - val_loss: 0.9747 - val_accuracy: 0.6153\n",
            "Epoch 647/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9665 - accuracy: 0.6069 - val_loss: 0.9286 - val_accuracy: 0.6384\n",
            "Epoch 648/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9633 - accuracy: 0.6090 - val_loss: 0.9361 - val_accuracy: 0.6343\n",
            "Epoch 649/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9595 - accuracy: 0.6117 - val_loss: 0.9439 - val_accuracy: 0.6360\n",
            "Epoch 650/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9716 - accuracy: 0.6024 - val_loss: 0.9426 - val_accuracy: 0.6341\n",
            "Epoch 651/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9557 - accuracy: 0.6135 - val_loss: 0.9429 - val_accuracy: 0.6328\n",
            "Epoch 652/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9596 - accuracy: 0.6133 - val_loss: 0.9575 - val_accuracy: 0.6046\n",
            "Epoch 653/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9645 - accuracy: 0.6089 - val_loss: 0.9243 - val_accuracy: 0.6433\n",
            "Epoch 654/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9654 - accuracy: 0.6082 - val_loss: 0.9042 - val_accuracy: 0.6562\n",
            "Epoch 655/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9701 - accuracy: 0.6052 - val_loss: 0.9264 - val_accuracy: 0.6467\n",
            "Epoch 656/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9625 - accuracy: 0.6083 - val_loss: 0.9297 - val_accuracy: 0.6416\n",
            "Epoch 657/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9681 - accuracy: 0.6070 - val_loss: 0.9465 - val_accuracy: 0.6311\n",
            "Epoch 658/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9730 - accuracy: 0.6046 - val_loss: 0.9416 - val_accuracy: 0.6409\n",
            "Epoch 659/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9715 - accuracy: 0.6043 - val_loss: 0.9494 - val_accuracy: 0.6146\n",
            "Epoch 660/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9667 - accuracy: 0.6073 - val_loss: 0.9085 - val_accuracy: 0.6533\n",
            "Epoch 661/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9651 - accuracy: 0.6080 - val_loss: 0.9600 - val_accuracy: 0.6304\n",
            "Epoch 662/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9621 - accuracy: 0.6091 - val_loss: 0.9295 - val_accuracy: 0.6487\n",
            "Epoch 663/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9711 - accuracy: 0.6049 - val_loss: 0.9755 - val_accuracy: 0.6251\n",
            "Epoch 664/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9654 - accuracy: 0.6084 - val_loss: 0.9572 - val_accuracy: 0.6294\n",
            "Epoch 665/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9612 - accuracy: 0.6104 - val_loss: 0.9441 - val_accuracy: 0.6384\n",
            "Epoch 666/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9671 - accuracy: 0.6064 - val_loss: 0.9144 - val_accuracy: 0.6477\n",
            "Epoch 667/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9625 - accuracy: 0.6099 - val_loss: 0.9331 - val_accuracy: 0.6421\n",
            "Epoch 668/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9600 - accuracy: 0.6099 - val_loss: 0.9469 - val_accuracy: 0.6336\n",
            "Epoch 669/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9676 - accuracy: 0.6057 - val_loss: 0.9742 - val_accuracy: 0.6226\n",
            "Epoch 670/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9653 - accuracy: 0.6089 - val_loss: 0.9660 - val_accuracy: 0.5915\n",
            "Epoch 671/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9575 - accuracy: 0.6137 - val_loss: 0.9225 - val_accuracy: 0.6479\n",
            "Epoch 672/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9588 - accuracy: 0.6132 - val_loss: 1.0092 - val_accuracy: 0.5922\n",
            "Epoch 673/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9598 - accuracy: 0.6106 - val_loss: 0.9415 - val_accuracy: 0.6302\n",
            "Epoch 674/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9644 - accuracy: 0.6098 - val_loss: 0.9477 - val_accuracy: 0.6336\n",
            "Epoch 675/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9648 - accuracy: 0.6102 - val_loss: 0.9748 - val_accuracy: 0.6202\n",
            "Epoch 676/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9729 - accuracy: 0.6033 - val_loss: 0.9569 - val_accuracy: 0.6260\n",
            "Epoch 677/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9620 - accuracy: 0.6098 - val_loss: 0.9295 - val_accuracy: 0.6409\n",
            "Epoch 678/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9635 - accuracy: 0.6076 - val_loss: 0.9779 - val_accuracy: 0.5983\n",
            "Epoch 679/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9640 - accuracy: 0.6084 - val_loss: 0.9725 - val_accuracy: 0.6107\n",
            "Epoch 680/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9600 - accuracy: 0.6103 - val_loss: 0.9220 - val_accuracy: 0.6528\n",
            "Epoch 681/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9648 - accuracy: 0.6079 - val_loss: 0.9155 - val_accuracy: 0.6423\n",
            "Epoch 682/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9637 - accuracy: 0.6059 - val_loss: 0.9436 - val_accuracy: 0.6294\n",
            "Epoch 683/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9589 - accuracy: 0.6114 - val_loss: 0.9329 - val_accuracy: 0.6416\n",
            "Epoch 684/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9602 - accuracy: 0.6118 - val_loss: 0.9447 - val_accuracy: 0.6285\n",
            "Epoch 685/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9626 - accuracy: 0.6100 - val_loss: 0.9383 - val_accuracy: 0.6363\n",
            "Epoch 686/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9619 - accuracy: 0.6096 - val_loss: 0.9385 - val_accuracy: 0.6319\n",
            "Epoch 687/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9643 - accuracy: 0.6069 - val_loss: 0.9819 - val_accuracy: 0.6224\n",
            "Epoch 688/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9696 - accuracy: 0.6059 - val_loss: 0.9774 - val_accuracy: 0.6226\n",
            "Epoch 689/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9573 - accuracy: 0.6109 - val_loss: 0.9248 - val_accuracy: 0.6421\n",
            "Epoch 690/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9636 - accuracy: 0.6089 - val_loss: 0.9483 - val_accuracy: 0.6263\n",
            "Epoch 691/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9695 - accuracy: 0.6046 - val_loss: 0.9362 - val_accuracy: 0.6467\n",
            "Epoch 692/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9603 - accuracy: 0.6113 - val_loss: 1.0226 - val_accuracy: 0.5946\n",
            "Epoch 693/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9611 - accuracy: 0.6095 - val_loss: 0.9616 - val_accuracy: 0.6241\n",
            "Epoch 694/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9604 - accuracy: 0.6101 - val_loss: 1.0076 - val_accuracy: 0.6039\n",
            "Epoch 695/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9627 - accuracy: 0.6104 - val_loss: 0.9062 - val_accuracy: 0.6509\n",
            "Epoch 696/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9625 - accuracy: 0.6113 - val_loss: 0.9686 - val_accuracy: 0.6231\n",
            "Epoch 697/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9607 - accuracy: 0.6106 - val_loss: 0.9092 - val_accuracy: 0.6526\n",
            "Epoch 698/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9650 - accuracy: 0.6091 - val_loss: 0.9560 - val_accuracy: 0.6311\n",
            "Epoch 699/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9628 - accuracy: 0.6104 - val_loss: 0.9243 - val_accuracy: 0.6443\n",
            "Epoch 700/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9580 - accuracy: 0.6115 - val_loss: 0.9452 - val_accuracy: 0.6231\n",
            "Epoch 701/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9623 - accuracy: 0.6083 - val_loss: 0.9233 - val_accuracy: 0.6436\n",
            "Epoch 702/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9599 - accuracy: 0.6121 - val_loss: 0.9211 - val_accuracy: 0.6406\n",
            "Epoch 703/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9609 - accuracy: 0.6087 - val_loss: 0.9944 - val_accuracy: 0.5998\n",
            "Epoch 704/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9667 - accuracy: 0.6076 - val_loss: 0.9385 - val_accuracy: 0.6404\n",
            "Epoch 705/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9564 - accuracy: 0.6135 - val_loss: 0.9151 - val_accuracy: 0.6460\n",
            "Epoch 706/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9587 - accuracy: 0.6109 - val_loss: 0.9487 - val_accuracy: 0.6377\n",
            "Epoch 707/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9588 - accuracy: 0.6110 - val_loss: 0.9259 - val_accuracy: 0.6394\n",
            "Epoch 708/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9652 - accuracy: 0.6065 - val_loss: 0.9485 - val_accuracy: 0.6338\n",
            "Epoch 709/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9637 - accuracy: 0.6087 - val_loss: 0.9571 - val_accuracy: 0.6324\n",
            "Epoch 710/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9592 - accuracy: 0.6112 - val_loss: 0.9368 - val_accuracy: 0.6372\n",
            "Epoch 711/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9564 - accuracy: 0.6125 - val_loss: 0.9288 - val_accuracy: 0.6358\n",
            "Epoch 712/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9573 - accuracy: 0.6121 - val_loss: 0.9368 - val_accuracy: 0.6404\n",
            "Epoch 713/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9600 - accuracy: 0.6110 - val_loss: 0.9388 - val_accuracy: 0.6377\n",
            "Epoch 714/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9627 - accuracy: 0.6081 - val_loss: 0.9763 - val_accuracy: 0.6180\n",
            "Epoch 715/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9569 - accuracy: 0.6101 - val_loss: 0.9577 - val_accuracy: 0.6307\n",
            "Epoch 716/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9705 - accuracy: 0.6057 - val_loss: 0.9361 - val_accuracy: 0.6418\n",
            "Epoch 717/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9691 - accuracy: 0.6076 - val_loss: 0.9254 - val_accuracy: 0.6513\n",
            "Epoch 718/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9603 - accuracy: 0.6116 - val_loss: 0.9434 - val_accuracy: 0.6372\n",
            "Epoch 719/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9582 - accuracy: 0.6111 - val_loss: 0.9412 - val_accuracy: 0.6423\n",
            "Epoch 720/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9568 - accuracy: 0.6111 - val_loss: 0.9334 - val_accuracy: 0.6397\n",
            "Epoch 721/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9549 - accuracy: 0.6148 - val_loss: 0.9346 - val_accuracy: 0.6316\n",
            "Epoch 722/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9597 - accuracy: 0.6108 - val_loss: 0.9419 - val_accuracy: 0.6377\n",
            "Epoch 723/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9669 - accuracy: 0.6070 - val_loss: 0.9310 - val_accuracy: 0.6453\n",
            "Epoch 724/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9568 - accuracy: 0.6145 - val_loss: 0.9284 - val_accuracy: 0.6423\n",
            "Epoch 725/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9555 - accuracy: 0.6119 - val_loss: 0.9277 - val_accuracy: 0.6440\n",
            "Epoch 726/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9511 - accuracy: 0.6167 - val_loss: 0.9339 - val_accuracy: 0.6341\n",
            "Epoch 727/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9630 - accuracy: 0.6099 - val_loss: 0.9246 - val_accuracy: 0.6353\n",
            "Epoch 728/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9648 - accuracy: 0.6070 - val_loss: 0.9521 - val_accuracy: 0.6314\n",
            "Epoch 729/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9586 - accuracy: 0.6105 - val_loss: 0.9119 - val_accuracy: 0.6504\n",
            "Epoch 730/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9709 - accuracy: 0.6052 - val_loss: 0.9439 - val_accuracy: 0.6372\n",
            "Epoch 731/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9610 - accuracy: 0.6095 - val_loss: 0.9607 - val_accuracy: 0.6241\n",
            "Epoch 732/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9653 - accuracy: 0.6074 - val_loss: 0.9470 - val_accuracy: 0.6350\n",
            "Epoch 733/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9546 - accuracy: 0.6140 - val_loss: 0.9848 - val_accuracy: 0.6054\n",
            "Epoch 734/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9549 - accuracy: 0.6138 - val_loss: 0.9608 - val_accuracy: 0.6282\n",
            "Epoch 735/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9673 - accuracy: 0.6061 - val_loss: 0.9555 - val_accuracy: 0.6314\n",
            "Epoch 736/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9601 - accuracy: 0.6099 - val_loss: 0.9237 - val_accuracy: 0.6504\n",
            "Epoch 737/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9549 - accuracy: 0.6136 - val_loss: 0.9255 - val_accuracy: 0.6409\n",
            "Epoch 738/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9600 - accuracy: 0.6101 - val_loss: 0.9197 - val_accuracy: 0.6474\n",
            "Epoch 739/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9575 - accuracy: 0.6115 - val_loss: 0.9419 - val_accuracy: 0.6355\n",
            "Epoch 740/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9593 - accuracy: 0.6102 - val_loss: 0.9399 - val_accuracy: 0.6399\n",
            "Epoch 741/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9599 - accuracy: 0.6103 - val_loss: 0.9281 - val_accuracy: 0.6377\n",
            "Epoch 742/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9570 - accuracy: 0.6122 - val_loss: 0.9213 - val_accuracy: 0.6450\n",
            "Epoch 743/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9524 - accuracy: 0.6143 - val_loss: 0.9249 - val_accuracy: 0.6472\n",
            "Epoch 744/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9584 - accuracy: 0.6113 - val_loss: 0.9255 - val_accuracy: 0.6433\n",
            "Epoch 745/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9568 - accuracy: 0.6115 - val_loss: 0.9688 - val_accuracy: 0.6260\n",
            "Epoch 746/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9601 - accuracy: 0.6115 - val_loss: 1.0085 - val_accuracy: 0.5920\n",
            "Epoch 747/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9571 - accuracy: 0.6124 - val_loss: 0.9486 - val_accuracy: 0.6314\n",
            "Epoch 748/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9632 - accuracy: 0.6091 - val_loss: 0.9359 - val_accuracy: 0.6443\n",
            "Epoch 749/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9655 - accuracy: 0.6061 - val_loss: 0.9088 - val_accuracy: 0.6472\n",
            "Epoch 750/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9593 - accuracy: 0.6098 - val_loss: 0.9517 - val_accuracy: 0.6380\n",
            "Epoch 751/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9543 - accuracy: 0.6142 - val_loss: 0.9478 - val_accuracy: 0.6338\n",
            "Epoch 752/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9560 - accuracy: 0.6130 - val_loss: 0.9745 - val_accuracy: 0.6229\n",
            "Epoch 753/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9567 - accuracy: 0.6140 - val_loss: 1.0627 - val_accuracy: 0.5599\n",
            "Epoch 754/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9697 - accuracy: 0.6058 - val_loss: 0.9392 - val_accuracy: 0.6450\n",
            "Epoch 755/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9569 - accuracy: 0.6118 - val_loss: 0.9441 - val_accuracy: 0.6365\n",
            "Epoch 756/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9626 - accuracy: 0.6087 - val_loss: 0.9895 - val_accuracy: 0.6051\n",
            "Epoch 757/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9553 - accuracy: 0.6144 - val_loss: 0.9213 - val_accuracy: 0.6409\n",
            "Epoch 758/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9583 - accuracy: 0.6128 - val_loss: 0.9401 - val_accuracy: 0.6204\n",
            "Epoch 759/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9627 - accuracy: 0.6110 - val_loss: 0.9639 - val_accuracy: 0.6285\n",
            "Epoch 760/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9510 - accuracy: 0.6157 - val_loss: 0.9655 - val_accuracy: 0.6192\n",
            "Epoch 761/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9636 - accuracy: 0.6083 - val_loss: 0.9410 - val_accuracy: 0.6389\n",
            "Epoch 762/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9627 - accuracy: 0.6094 - val_loss: 0.9455 - val_accuracy: 0.6358\n",
            "Epoch 763/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9529 - accuracy: 0.6136 - val_loss: 0.9564 - val_accuracy: 0.6214\n",
            "Epoch 764/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9642 - accuracy: 0.6087 - val_loss: 1.0233 - val_accuracy: 0.6019\n",
            "Epoch 765/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9616 - accuracy: 0.6096 - val_loss: 0.9471 - val_accuracy: 0.6326\n",
            "Epoch 766/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9525 - accuracy: 0.6150 - val_loss: 0.9364 - val_accuracy: 0.6331\n",
            "Epoch 767/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9614 - accuracy: 0.6097 - val_loss: 1.0310 - val_accuracy: 0.5720\n",
            "Epoch 768/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9598 - accuracy: 0.6109 - val_loss: 0.9408 - val_accuracy: 0.6348\n",
            "Epoch 769/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9524 - accuracy: 0.6153 - val_loss: 0.9590 - val_accuracy: 0.6287\n",
            "Epoch 770/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9550 - accuracy: 0.6124 - val_loss: 0.9367 - val_accuracy: 0.6355\n",
            "Epoch 771/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9611 - accuracy: 0.6074 - val_loss: 0.9712 - val_accuracy: 0.6085\n",
            "Epoch 772/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9569 - accuracy: 0.6104 - val_loss: 0.9474 - val_accuracy: 0.6307\n",
            "Epoch 773/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9609 - accuracy: 0.6099 - val_loss: 0.9613 - val_accuracy: 0.6217\n",
            "Epoch 774/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9570 - accuracy: 0.6123 - val_loss: 0.9731 - val_accuracy: 0.6241\n",
            "Epoch 775/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9576 - accuracy: 0.6126 - val_loss: 0.9347 - val_accuracy: 0.6350\n",
            "Epoch 776/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9591 - accuracy: 0.6101 - val_loss: 0.9234 - val_accuracy: 0.6392\n",
            "Epoch 777/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9552 - accuracy: 0.6131 - val_loss: 0.9684 - val_accuracy: 0.6192\n",
            "Epoch 778/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9546 - accuracy: 0.6130 - val_loss: 0.9006 - val_accuracy: 0.6540\n",
            "Epoch 779/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9590 - accuracy: 0.6088 - val_loss: 0.9568 - val_accuracy: 0.6290\n",
            "Epoch 780/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9651 - accuracy: 0.6063 - val_loss: 0.9805 - val_accuracy: 0.6182\n",
            "Epoch 781/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9652 - accuracy: 0.6064 - val_loss: 0.9186 - val_accuracy: 0.6455\n",
            "Epoch 782/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9543 - accuracy: 0.6130 - val_loss: 0.9248 - val_accuracy: 0.6448\n",
            "Epoch 783/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9584 - accuracy: 0.6126 - val_loss: 0.9515 - val_accuracy: 0.6328\n",
            "Epoch 784/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9621 - accuracy: 0.6092 - val_loss: 0.9654 - val_accuracy: 0.6246\n",
            "Epoch 785/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9581 - accuracy: 0.6117 - val_loss: 0.9723 - val_accuracy: 0.5942\n",
            "Epoch 786/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9558 - accuracy: 0.6138 - val_loss: 0.9261 - val_accuracy: 0.6440\n",
            "Epoch 787/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9586 - accuracy: 0.6116 - val_loss: 0.9447 - val_accuracy: 0.6343\n",
            "Epoch 788/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9581 - accuracy: 0.6117 - val_loss: 0.9330 - val_accuracy: 0.6355\n",
            "Epoch 789/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9618 - accuracy: 0.6087 - val_loss: 0.9415 - val_accuracy: 0.6304\n",
            "Epoch 790/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9635 - accuracy: 0.6082 - val_loss: 0.9284 - val_accuracy: 0.6460\n",
            "Epoch 791/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9610 - accuracy: 0.6107 - val_loss: 0.9343 - val_accuracy: 0.6436\n",
            "Epoch 792/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9595 - accuracy: 0.6113 - val_loss: 0.9422 - val_accuracy: 0.6309\n",
            "Epoch 793/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9517 - accuracy: 0.6165 - val_loss: 0.9297 - val_accuracy: 0.6384\n",
            "Epoch 794/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9628 - accuracy: 0.6082 - val_loss: 0.9167 - val_accuracy: 0.6467\n",
            "Epoch 795/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9598 - accuracy: 0.6109 - val_loss: 0.9550 - val_accuracy: 0.6255\n",
            "Epoch 796/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9692 - accuracy: 0.6072 - val_loss: 0.9953 - val_accuracy: 0.5998\n",
            "Epoch 797/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9620 - accuracy: 0.6095 - val_loss: 0.9514 - val_accuracy: 0.6273\n",
            "Epoch 798/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9586 - accuracy: 0.6115 - val_loss: 0.9959 - val_accuracy: 0.6041\n",
            "Epoch 799/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9582 - accuracy: 0.6105 - val_loss: 0.9338 - val_accuracy: 0.6423\n",
            "Epoch 800/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9540 - accuracy: 0.6114 - val_loss: 0.9560 - val_accuracy: 0.6182\n",
            "Epoch 801/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9579 - accuracy: 0.6100 - val_loss: 0.9720 - val_accuracy: 0.6107\n",
            "Epoch 802/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9507 - accuracy: 0.6145 - val_loss: 0.9291 - val_accuracy: 0.6431\n",
            "Epoch 803/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9560 - accuracy: 0.6121 - val_loss: 0.9293 - val_accuracy: 0.6431\n",
            "Epoch 804/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9575 - accuracy: 0.6132 - val_loss: 0.9607 - val_accuracy: 0.6324\n",
            "Epoch 805/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9587 - accuracy: 0.6118 - val_loss: 0.9260 - val_accuracy: 0.6436\n",
            "Epoch 806/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9503 - accuracy: 0.6157 - val_loss: 0.9611 - val_accuracy: 0.6214\n",
            "Epoch 807/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9572 - accuracy: 0.6113 - val_loss: 0.9500 - val_accuracy: 0.6187\n",
            "Epoch 808/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9513 - accuracy: 0.6152 - val_loss: 0.9321 - val_accuracy: 0.6328\n",
            "Epoch 809/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9497 - accuracy: 0.6163 - val_loss: 0.9601 - val_accuracy: 0.6246\n",
            "Epoch 810/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9554 - accuracy: 0.6126 - val_loss: 0.9327 - val_accuracy: 0.6404\n",
            "Epoch 811/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9630 - accuracy: 0.6079 - val_loss: 0.9179 - val_accuracy: 0.6418\n",
            "Epoch 812/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9569 - accuracy: 0.6130 - val_loss: 0.9175 - val_accuracy: 0.6426\n",
            "Epoch 813/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9558 - accuracy: 0.6130 - val_loss: 0.9720 - val_accuracy: 0.6129\n",
            "Epoch 814/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9586 - accuracy: 0.6116 - val_loss: 0.9176 - val_accuracy: 0.6506\n",
            "Epoch 815/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9492 - accuracy: 0.6164 - val_loss: 0.9671 - val_accuracy: 0.6156\n",
            "Epoch 816/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9551 - accuracy: 0.6126 - val_loss: 0.9148 - val_accuracy: 0.6465\n",
            "Epoch 817/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9589 - accuracy: 0.6101 - val_loss: 0.9229 - val_accuracy: 0.6462\n",
            "Epoch 818/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9473 - accuracy: 0.6167 - val_loss: 0.9335 - val_accuracy: 0.6299\n",
            "Epoch 819/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9552 - accuracy: 0.6126 - val_loss: 1.0002 - val_accuracy: 0.5927\n",
            "Epoch 820/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9601 - accuracy: 0.6101 - val_loss: 0.9358 - val_accuracy: 0.6392\n",
            "Epoch 821/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9583 - accuracy: 0.6111 - val_loss: 0.9456 - val_accuracy: 0.6319\n",
            "Epoch 822/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9543 - accuracy: 0.6138 - val_loss: 0.9949 - val_accuracy: 0.6032\n",
            "Epoch 823/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9573 - accuracy: 0.6102 - val_loss: 0.9821 - val_accuracy: 0.6229\n",
            "Epoch 824/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9559 - accuracy: 0.6121 - val_loss: 0.9340 - val_accuracy: 0.6372\n",
            "Epoch 825/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9533 - accuracy: 0.6142 - val_loss: 0.9215 - val_accuracy: 0.6504\n",
            "Epoch 826/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9587 - accuracy: 0.6103 - val_loss: 0.9087 - val_accuracy: 0.6540\n",
            "Epoch 827/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9518 - accuracy: 0.6135 - val_loss: 0.9359 - val_accuracy: 0.6200\n",
            "Epoch 828/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9574 - accuracy: 0.6115 - val_loss: 0.9549 - val_accuracy: 0.6268\n",
            "Epoch 829/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9576 - accuracy: 0.6105 - val_loss: 0.9144 - val_accuracy: 0.6428\n",
            "Epoch 830/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9517 - accuracy: 0.6132 - val_loss: 0.9219 - val_accuracy: 0.6491\n",
            "Epoch 831/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9600 - accuracy: 0.6106 - val_loss: 0.9473 - val_accuracy: 0.6333\n",
            "Epoch 832/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9534 - accuracy: 0.6140 - val_loss: 0.9198 - val_accuracy: 0.6472\n",
            "Epoch 833/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9640 - accuracy: 0.6082 - val_loss: 0.9658 - val_accuracy: 0.6226\n",
            "Epoch 834/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9580 - accuracy: 0.6095 - val_loss: 0.9544 - val_accuracy: 0.6292\n",
            "Epoch 835/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9519 - accuracy: 0.6160 - val_loss: 0.9202 - val_accuracy: 0.6440\n",
            "Epoch 836/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9558 - accuracy: 0.6122 - val_loss: 0.9943 - val_accuracy: 0.5954\n",
            "Epoch 837/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9582 - accuracy: 0.6104 - val_loss: 0.9344 - val_accuracy: 0.6421\n",
            "Epoch 838/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9573 - accuracy: 0.6130 - val_loss: 0.9620 - val_accuracy: 0.6341\n",
            "Epoch 839/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9530 - accuracy: 0.6130 - val_loss: 0.9487 - val_accuracy: 0.6311\n",
            "Epoch 840/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9604 - accuracy: 0.6102 - val_loss: 0.9655 - val_accuracy: 0.6238\n",
            "Epoch 841/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9508 - accuracy: 0.6142 - val_loss: 0.9478 - val_accuracy: 0.6353\n",
            "Epoch 842/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9658 - accuracy: 0.6071 - val_loss: 0.9283 - val_accuracy: 0.6436\n",
            "Epoch 843/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9544 - accuracy: 0.6126 - val_loss: 0.9260 - val_accuracy: 0.6380\n",
            "Epoch 844/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9531 - accuracy: 0.6139 - val_loss: 0.9142 - val_accuracy: 0.6433\n",
            "Epoch 845/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9483 - accuracy: 0.6161 - val_loss: 0.9365 - val_accuracy: 0.6358\n",
            "Epoch 846/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9493 - accuracy: 0.6152 - val_loss: 0.9418 - val_accuracy: 0.6365\n",
            "Epoch 847/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9575 - accuracy: 0.6101 - val_loss: 0.9456 - val_accuracy: 0.6209\n",
            "Epoch 848/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9535 - accuracy: 0.6126 - val_loss: 0.9626 - val_accuracy: 0.6209\n",
            "Epoch 849/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9555 - accuracy: 0.6129 - val_loss: 0.9341 - val_accuracy: 0.6375\n",
            "Epoch 850/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9529 - accuracy: 0.6149 - val_loss: 0.9336 - val_accuracy: 0.6382\n",
            "Epoch 851/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9534 - accuracy: 0.6146 - val_loss: 0.9598 - val_accuracy: 0.6350\n",
            "Epoch 852/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9532 - accuracy: 0.6143 - val_loss: 0.9201 - val_accuracy: 0.6491\n",
            "Epoch 853/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9525 - accuracy: 0.6132 - val_loss: 1.0061 - val_accuracy: 0.5878\n",
            "Epoch 854/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9668 - accuracy: 0.6058 - val_loss: 0.9777 - val_accuracy: 0.6141\n",
            "Epoch 855/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9641 - accuracy: 0.6088 - val_loss: 0.9590 - val_accuracy: 0.6255\n",
            "Epoch 856/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9538 - accuracy: 0.6136 - val_loss: 0.9394 - val_accuracy: 0.6360\n",
            "Epoch 857/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9542 - accuracy: 0.6120 - val_loss: 0.9703 - val_accuracy: 0.6197\n",
            "Epoch 858/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9546 - accuracy: 0.6137 - val_loss: 0.9391 - val_accuracy: 0.6372\n",
            "Epoch 859/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9572 - accuracy: 0.6098 - val_loss: 0.9659 - val_accuracy: 0.6146\n",
            "Epoch 860/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9537 - accuracy: 0.6123 - val_loss: 0.9112 - val_accuracy: 0.6479\n",
            "Epoch 861/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9553 - accuracy: 0.6114 - val_loss: 0.9390 - val_accuracy: 0.6345\n",
            "Epoch 862/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9534 - accuracy: 0.6138 - val_loss: 0.9421 - val_accuracy: 0.6404\n",
            "Epoch 863/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9539 - accuracy: 0.6147 - val_loss: 1.0498 - val_accuracy: 0.5691\n",
            "Epoch 864/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9623 - accuracy: 0.6082 - val_loss: 0.9580 - val_accuracy: 0.6255\n",
            "Epoch 865/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9550 - accuracy: 0.6123 - val_loss: 0.9474 - val_accuracy: 0.6428\n",
            "Epoch 866/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9500 - accuracy: 0.6159 - val_loss: 0.9494 - val_accuracy: 0.6260\n",
            "Epoch 867/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9580 - accuracy: 0.6116 - val_loss: 0.9798 - val_accuracy: 0.6151\n",
            "Epoch 868/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9537 - accuracy: 0.6135 - val_loss: 0.9012 - val_accuracy: 0.6526\n",
            "Epoch 869/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9599 - accuracy: 0.6096 - val_loss: 0.9254 - val_accuracy: 0.6418\n",
            "Epoch 870/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9541 - accuracy: 0.6144 - val_loss: 0.9410 - val_accuracy: 0.6309\n",
            "Epoch 871/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9538 - accuracy: 0.6133 - val_loss: 0.9449 - val_accuracy: 0.6326\n",
            "Epoch 872/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9501 - accuracy: 0.6152 - val_loss: 0.9150 - val_accuracy: 0.6443\n",
            "Epoch 873/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9534 - accuracy: 0.6120 - val_loss: 0.9355 - val_accuracy: 0.6345\n",
            "Epoch 874/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9535 - accuracy: 0.6140 - val_loss: 0.9313 - val_accuracy: 0.6421\n",
            "Epoch 875/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9530 - accuracy: 0.6133 - val_loss: 0.9200 - val_accuracy: 0.6421\n",
            "Epoch 876/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9530 - accuracy: 0.6135 - val_loss: 0.9793 - val_accuracy: 0.6092\n",
            "Epoch 877/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9546 - accuracy: 0.6104 - val_loss: 0.9122 - val_accuracy: 0.6552\n",
            "Epoch 878/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9610 - accuracy: 0.6080 - val_loss: 0.9792 - val_accuracy: 0.6088\n",
            "Epoch 879/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9529 - accuracy: 0.6135 - val_loss: 0.9259 - val_accuracy: 0.6423\n",
            "Epoch 880/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9547 - accuracy: 0.6134 - val_loss: 0.9193 - val_accuracy: 0.6470\n",
            "Epoch 881/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9536 - accuracy: 0.6125 - val_loss: 0.9524 - val_accuracy: 0.6324\n",
            "Epoch 882/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9565 - accuracy: 0.6109 - val_loss: 0.9381 - val_accuracy: 0.6370\n",
            "Epoch 883/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9507 - accuracy: 0.6146 - val_loss: 0.9166 - val_accuracy: 0.6465\n",
            "Epoch 884/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9553 - accuracy: 0.6136 - val_loss: 0.9503 - val_accuracy: 0.6333\n",
            "Epoch 885/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9585 - accuracy: 0.6118 - val_loss: 0.9330 - val_accuracy: 0.6416\n",
            "Epoch 886/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9577 - accuracy: 0.6125 - val_loss: 0.9049 - val_accuracy: 0.6533\n",
            "Epoch 887/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9530 - accuracy: 0.6152 - val_loss: 0.9174 - val_accuracy: 0.6479\n",
            "Epoch 888/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9507 - accuracy: 0.6142 - val_loss: 0.9519 - val_accuracy: 0.6299\n",
            "Epoch 889/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9500 - accuracy: 0.6163 - val_loss: 0.9919 - val_accuracy: 0.6044\n",
            "Epoch 890/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9568 - accuracy: 0.6119 - val_loss: 0.9466 - val_accuracy: 0.6363\n",
            "Epoch 891/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9577 - accuracy: 0.6097 - val_loss: 0.9764 - val_accuracy: 0.6265\n",
            "Epoch 892/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9507 - accuracy: 0.6157 - val_loss: 0.9871 - val_accuracy: 0.6146\n",
            "Epoch 893/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9562 - accuracy: 0.6112 - val_loss: 0.9974 - val_accuracy: 0.6012\n",
            "Epoch 894/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9461 - accuracy: 0.6183 - val_loss: 0.9414 - val_accuracy: 0.6260\n",
            "Epoch 895/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9486 - accuracy: 0.6181 - val_loss: 0.9382 - val_accuracy: 0.6372\n",
            "Epoch 896/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9545 - accuracy: 0.6138 - val_loss: 0.9213 - val_accuracy: 0.6457\n",
            "Epoch 897/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9585 - accuracy: 0.6101 - val_loss: 0.9646 - val_accuracy: 0.6192\n",
            "Epoch 898/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9507 - accuracy: 0.6135 - val_loss: 0.9314 - val_accuracy: 0.6377\n",
            "Epoch 899/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9552 - accuracy: 0.6116 - val_loss: 0.9326 - val_accuracy: 0.6350\n",
            "Epoch 900/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9514 - accuracy: 0.6152 - val_loss: 0.9160 - val_accuracy: 0.6467\n",
            "Epoch 901/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9482 - accuracy: 0.6168 - val_loss: 0.9237 - val_accuracy: 0.6399\n",
            "Epoch 902/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9529 - accuracy: 0.6126 - val_loss: 0.9308 - val_accuracy: 0.6341\n",
            "Epoch 903/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9546 - accuracy: 0.6136 - val_loss: 0.9309 - val_accuracy: 0.6384\n",
            "Epoch 904/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9488 - accuracy: 0.6160 - val_loss: 0.9442 - val_accuracy: 0.6148\n",
            "Epoch 905/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9540 - accuracy: 0.6122 - val_loss: 0.9422 - val_accuracy: 0.6331\n",
            "Epoch 906/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9535 - accuracy: 0.6143 - val_loss: 0.9233 - val_accuracy: 0.6375\n",
            "Epoch 907/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9543 - accuracy: 0.6113 - val_loss: 0.9419 - val_accuracy: 0.6204\n",
            "Epoch 908/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9567 - accuracy: 0.6095 - val_loss: 0.9740 - val_accuracy: 0.6119\n",
            "Epoch 909/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9531 - accuracy: 0.6138 - val_loss: 0.9153 - val_accuracy: 0.6457\n",
            "Epoch 910/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9557 - accuracy: 0.6123 - val_loss: 0.9362 - val_accuracy: 0.6397\n",
            "Epoch 911/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9509 - accuracy: 0.6148 - val_loss: 0.9146 - val_accuracy: 0.6509\n",
            "Epoch 912/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9556 - accuracy: 0.6131 - val_loss: 0.9380 - val_accuracy: 0.6345\n",
            "Epoch 913/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9433 - accuracy: 0.6187 - val_loss: 0.9754 - val_accuracy: 0.6049\n",
            "Epoch 914/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9482 - accuracy: 0.6171 - val_loss: 0.9640 - val_accuracy: 0.6221\n",
            "Epoch 915/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9503 - accuracy: 0.6164 - val_loss: 0.9317 - val_accuracy: 0.6392\n",
            "Epoch 916/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9506 - accuracy: 0.6147 - val_loss: 0.9717 - val_accuracy: 0.6156\n",
            "Epoch 917/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9464 - accuracy: 0.6168 - val_loss: 0.9467 - val_accuracy: 0.6343\n",
            "Epoch 918/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9467 - accuracy: 0.6181 - val_loss: 0.9700 - val_accuracy: 0.6068\n",
            "Epoch 919/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9569 - accuracy: 0.6124 - val_loss: 0.9410 - val_accuracy: 0.6265\n",
            "Epoch 920/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9522 - accuracy: 0.6140 - val_loss: 0.9240 - val_accuracy: 0.6428\n",
            "Epoch 921/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9576 - accuracy: 0.6132 - val_loss: 0.9313 - val_accuracy: 0.6399\n",
            "Epoch 922/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9499 - accuracy: 0.6164 - val_loss: 0.9458 - val_accuracy: 0.6294\n",
            "Epoch 923/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9522 - accuracy: 0.6136 - val_loss: 0.9368 - val_accuracy: 0.6343\n",
            "Epoch 924/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9529 - accuracy: 0.6152 - val_loss: 0.9850 - val_accuracy: 0.6112\n",
            "Epoch 925/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9486 - accuracy: 0.6157 - val_loss: 0.9384 - val_accuracy: 0.6309\n",
            "Epoch 926/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9487 - accuracy: 0.6136 - val_loss: 0.9303 - val_accuracy: 0.6394\n",
            "Epoch 927/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9526 - accuracy: 0.6133 - val_loss: 0.9257 - val_accuracy: 0.6433\n",
            "Epoch 928/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9512 - accuracy: 0.6141 - val_loss: 0.9484 - val_accuracy: 0.6285\n",
            "Epoch 929/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9506 - accuracy: 0.6154 - val_loss: 0.9463 - val_accuracy: 0.6302\n",
            "Epoch 930/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9490 - accuracy: 0.6151 - val_loss: 0.9617 - val_accuracy: 0.6241\n",
            "Epoch 931/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9509 - accuracy: 0.6140 - val_loss: 0.9763 - val_accuracy: 0.6226\n",
            "Epoch 932/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9538 - accuracy: 0.6145 - val_loss: 0.9443 - val_accuracy: 0.6336\n",
            "Epoch 933/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9549 - accuracy: 0.6128 - val_loss: 0.9283 - val_accuracy: 0.6440\n",
            "Epoch 934/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9543 - accuracy: 0.6130 - val_loss: 0.9407 - val_accuracy: 0.6331\n",
            "Epoch 935/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9522 - accuracy: 0.6130 - val_loss: 0.9374 - val_accuracy: 0.6389\n",
            "Epoch 936/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9448 - accuracy: 0.6175 - val_loss: 0.9462 - val_accuracy: 0.6389\n",
            "Epoch 937/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9494 - accuracy: 0.6147 - val_loss: 0.9204 - val_accuracy: 0.6428\n",
            "Epoch 938/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9510 - accuracy: 0.6141 - val_loss: 0.9496 - val_accuracy: 0.6321\n",
            "Epoch 939/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9500 - accuracy: 0.6160 - val_loss: 0.9668 - val_accuracy: 0.6182\n",
            "Epoch 940/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9524 - accuracy: 0.6160 - val_loss: 0.9141 - val_accuracy: 0.6489\n",
            "Epoch 941/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9558 - accuracy: 0.6106 - val_loss: 0.9283 - val_accuracy: 0.6423\n",
            "Epoch 942/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9501 - accuracy: 0.6158 - val_loss: 1.0063 - val_accuracy: 0.5818\n",
            "Epoch 943/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9526 - accuracy: 0.6139 - val_loss: 0.9368 - val_accuracy: 0.6353\n",
            "Epoch 944/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9550 - accuracy: 0.6134 - val_loss: 0.9464 - val_accuracy: 0.6401\n",
            "Epoch 945/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9508 - accuracy: 0.6142 - val_loss: 0.9452 - val_accuracy: 0.6326\n",
            "Epoch 946/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9494 - accuracy: 0.6166 - val_loss: 0.9241 - val_accuracy: 0.6460\n",
            "Epoch 947/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9515 - accuracy: 0.6149 - val_loss: 0.9739 - val_accuracy: 0.6129\n",
            "Epoch 948/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9499 - accuracy: 0.6165 - val_loss: 0.9213 - val_accuracy: 0.6470\n",
            "Epoch 949/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9469 - accuracy: 0.6169 - val_loss: 0.9483 - val_accuracy: 0.6338\n",
            "Epoch 950/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9478 - accuracy: 0.6137 - val_loss: 0.9796 - val_accuracy: 0.6124\n",
            "Epoch 951/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9558 - accuracy: 0.6140 - val_loss: 0.9161 - val_accuracy: 0.6470\n",
            "Epoch 952/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9491 - accuracy: 0.6163 - val_loss: 0.9319 - val_accuracy: 0.6355\n",
            "Epoch 953/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9470 - accuracy: 0.6161 - val_loss: 0.9322 - val_accuracy: 0.6377\n",
            "Epoch 954/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9578 - accuracy: 0.6110 - val_loss: 0.9583 - val_accuracy: 0.6292\n",
            "Epoch 955/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9479 - accuracy: 0.6153 - val_loss: 0.9555 - val_accuracy: 0.6131\n",
            "Epoch 956/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9472 - accuracy: 0.6173 - val_loss: 0.9420 - val_accuracy: 0.6307\n",
            "Epoch 957/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9533 - accuracy: 0.6143 - val_loss: 0.9523 - val_accuracy: 0.6307\n",
            "Epoch 958/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9464 - accuracy: 0.6158 - val_loss: 0.9384 - val_accuracy: 0.6394\n",
            "Epoch 959/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9530 - accuracy: 0.6123 - val_loss: 0.9509 - val_accuracy: 0.6331\n",
            "Epoch 960/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9514 - accuracy: 0.6141 - val_loss: 0.9401 - val_accuracy: 0.6438\n",
            "Epoch 961/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9556 - accuracy: 0.6120 - val_loss: 0.9370 - val_accuracy: 0.6336\n",
            "Epoch 962/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9507 - accuracy: 0.6142 - val_loss: 0.9695 - val_accuracy: 0.6163\n",
            "Epoch 963/1000\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.9501 - accuracy: 0.6147 - val_loss: 0.9649 - val_accuracy: 0.6253\n",
            "Epoch 964/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9530 - accuracy: 0.6121 - val_loss: 0.9567 - val_accuracy: 0.6309\n",
            "Epoch 965/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9529 - accuracy: 0.6130 - val_loss: 0.9419 - val_accuracy: 0.6258\n",
            "Epoch 966/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9590 - accuracy: 0.6106 - val_loss: 0.9552 - val_accuracy: 0.6273\n",
            "Epoch 967/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9521 - accuracy: 0.6151 - val_loss: 0.9693 - val_accuracy: 0.6229\n",
            "Epoch 968/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9546 - accuracy: 0.6125 - val_loss: 0.9646 - val_accuracy: 0.6246\n",
            "Epoch 969/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9548 - accuracy: 0.6117 - val_loss: 0.9536 - val_accuracy: 0.6321\n",
            "Epoch 970/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9526 - accuracy: 0.6120 - val_loss: 1.0016 - val_accuracy: 0.5978\n",
            "Epoch 971/1000\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.9527 - accuracy: 0.6136 - val_loss: 0.9330 - val_accuracy: 0.6365\n",
            "Epoch 972/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9474 - accuracy: 0.6167 - val_loss: 0.9335 - val_accuracy: 0.6358\n",
            "Epoch 973/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9518 - accuracy: 0.6144 - val_loss: 0.9133 - val_accuracy: 0.6533\n",
            "Epoch 974/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9521 - accuracy: 0.6146 - val_loss: 0.9566 - val_accuracy: 0.6204\n",
            "Epoch 975/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9458 - accuracy: 0.6177 - val_loss: 0.9049 - val_accuracy: 0.6472\n",
            "Epoch 976/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9475 - accuracy: 0.6161 - val_loss: 0.9632 - val_accuracy: 0.6251\n",
            "Epoch 977/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9471 - accuracy: 0.6164 - val_loss: 0.9737 - val_accuracy: 0.6158\n",
            "Epoch 978/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9484 - accuracy: 0.6158 - val_loss: 0.9315 - val_accuracy: 0.6285\n",
            "Epoch 979/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9554 - accuracy: 0.6102 - val_loss: 0.9646 - val_accuracy: 0.6311\n",
            "Epoch 980/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9477 - accuracy: 0.6149 - val_loss: 0.9538 - val_accuracy: 0.6321\n",
            "Epoch 981/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9572 - accuracy: 0.6121 - val_loss: 0.9445 - val_accuracy: 0.6285\n",
            "Epoch 982/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9559 - accuracy: 0.6116 - val_loss: 0.9399 - val_accuracy: 0.6343\n",
            "Epoch 983/1000\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.9481 - accuracy: 0.6158 - val_loss: 0.9610 - val_accuracy: 0.6251\n",
            "Epoch 984/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9520 - accuracy: 0.6134 - val_loss: 0.9608 - val_accuracy: 0.6304\n",
            "Epoch 985/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9469 - accuracy: 0.6172 - val_loss: 0.9479 - val_accuracy: 0.6165\n",
            "Epoch 986/1000\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.9450 - accuracy: 0.6169 - val_loss: 0.9593 - val_accuracy: 0.6229\n",
            "Epoch 987/1000\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.9505 - accuracy: 0.6159 - val_loss: 0.9509 - val_accuracy: 0.6336\n",
            "Epoch 988/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9455 - accuracy: 0.6162 - val_loss: 0.9337 - val_accuracy: 0.6404\n",
            "Epoch 989/1000\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.9484 - accuracy: 0.6168 - val_loss: 0.9505 - val_accuracy: 0.6353\n",
            "Epoch 990/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9485 - accuracy: 0.6156 - val_loss: 0.9656 - val_accuracy: 0.6156\n",
            "Epoch 991/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9514 - accuracy: 0.6124 - val_loss: 0.9201 - val_accuracy: 0.6513\n",
            "Epoch 992/1000\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.9488 - accuracy: 0.6150 - val_loss: 0.9225 - val_accuracy: 0.6455\n",
            "Epoch 993/1000\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9533 - accuracy: 0.6140 - val_loss: 0.9419 - val_accuracy: 0.6389\n",
            "Epoch 994/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9485 - accuracy: 0.6152 - val_loss: 0.9387 - val_accuracy: 0.6328\n",
            "Epoch 995/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9524 - accuracy: 0.6132 - val_loss: 0.9655 - val_accuracy: 0.6253\n",
            "Epoch 996/1000\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.9602 - accuracy: 0.6092 - val_loss: 0.9359 - val_accuracy: 0.6375\n",
            "Epoch 997/1000\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.9516 - accuracy: 0.6130 - val_loss: 0.9710 - val_accuracy: 0.6202\n",
            "Epoch 998/1000\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.9531 - accuracy: 0.6130 - val_loss: 0.9266 - val_accuracy: 0.6404\n",
            "Epoch 999/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9564 - accuracy: 0.6117 - val_loss: 0.9278 - val_accuracy: 0.6506\n",
            "Epoch 1000/1000\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.9452 - accuracy: 0.6165 - val_loss: 0.9236 - val_accuracy: 0.6433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDSyKZrgXdaP",
        "outputId": "6568c37f-c4ae-453f-ff74-b4f2e31e166f"
      },
      "source": [
        "score = model.evaluate(x_test,y_test,verbose=1)\n",
        "print(\"\\n\")\n",
        "print(\"Test loss:\",score[0])\n",
        "print(\"Test accuracy:\",score[1])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "129/129 [==============================] - 0s 2ms/step - loss: 0.9236 - accuracy: 0.6433\n",
            "\n",
            "\n",
            "Test loss: 0.9235625863075256\n",
            "Test accuracy: 0.6433089971542358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhG1k3kVXkO9"
      },
      "source": [
        "def plot_history(history):\n",
        "    # print(history.history.keys())\n",
        "\n",
        "    # 精度の履歴をプロット\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend(['accuracy', 'val_accuracy'], loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "    # 損失の履歴をプロット\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['loss', 'val_loss'], loc='lower right')\n",
        "    plt.show()\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "8gyxnkxyXlr5",
        "outputId": "327330d2-64ec-4ec7-f191-393701956df6"
      },
      "source": [
        "# 学習履歴をプロット\n",
        "plot_history(history)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}